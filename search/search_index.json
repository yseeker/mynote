{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Test \u00b6 test1 \u00b6 test2 \u00b6","title":"plotly"},{"location":"#test","text":"","title":"Test"},{"location":"#test1","text":"","title":"test1"},{"location":"#test2","text":"","title":"test2"},{"location":"albumentations/","text":"Image Augumentations \u00b6 \u30b5\u30f3\u30d7\u30eb\u5199\u771f\u306e\u8868\u793a \u00b6 \u30e9\u30a4\u30d6\u30e9\u30ea\u306eimport \u00b6 1 2 3 4 5 import matplotlib.pyplot as plt import albumentations as A import numpy as np import cv2 from PIL import Image Pillow \u3068OpenCV\u305d\u308c\u305e\u308c\u3067\u753b\u50cf\u3092\u8868\u793a \u00b6 \u753b\u50cf\u30c7\u30fc\u30bf\u306fKaggle\u306e Flowers Recognition \u304b\u3089\u53d6\u5f97\u3002Pillow\u3092\u4f7f\u3046\u5834\u5408\u306f\u3001\u8aad\u307f\u8fbc\u3093\u3060\u3068\u304d\u306bJpegImageFile\u306a\u306e\u3067openCV\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 image_path = '../input/flowers-recognition/flowers/daisy/10140303196_b88d3d6cec.jpg' Pillow\u306f\u5358\u306a\u308b\u753b\u50cf\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308a\u3001OpenCV\u306f\u300c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u300d\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u78ba\u304b\u306b\u6a5f\u80fd\u304c\u91cd\u8907\u3059\u308b\u90e8\u5206\u306f\u591a\u3044\uff08\u3064\u307e\u308aOpenCV\u306b\u306f\u304b\u306a\u308a\u306e\u753b\u50cf\u51e6\u7406\u6a5f\u80fd\u304c\u542b\u307e\u308c\u3066\u3044\u308b\uff09\u304c\u3001 \u305d\u306e\u6271\u3046\u5185\u5bb9\u306f\u5927\u304d\u304f\u7570\u306a\u308a\u307e\u3059\u3002\u6975\u7aef\u306a\u8a71\u3001\u753b\u50cf\u3092\u30ab\u30c3\u30c8\u3084\u30ea\u30b5\u30a4\u30ba\u3057\u305f\u3044\u6642\u3084\u3001\u5c11\u3057\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u3044\u5834\u5408\u306f Pillow \u3092\u4f7f\u3044\u3001\u7269\u4e8b\u3092\u300c\u898b\u3088\u3046\u300d\u3068\u601d\u3063\u3066\u3044\u308b\u30ed\u30dc\u30c3\u30c8\u3092\u7d44\u307f\u305f\u3044\u6642\u306b\u306f OpenCV \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u5f15\u7528\u5143\uff1a https://teratail.com/questions/71851 Pillow 1 2 3 4 5 6 img = Image . open ( image_path ) # img: JpegImageFile img = np . asarray ( img ) # \u3082\u3068\u306e\u753b\u50cf\u306b\u623b\u3059\u5834\u5408 # im = Image.fromarray(np.uint8(myarray*255)) plt . imshow ( img ) OpenCV 1 2 3 4 5 6 img = cv2 . imread ( image_path ) # img : ndarray (N-dimensional array, np.array\u306b\u3088\u3063\u3066\u751f\u6210) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # \u4e0b\u8a18\u3082RGB\u753b\u50cf\u2192BGR\u753b\u50cf\u3078\u306e\u5909\u63db #img = img[:,:,::-1] plt . imshow ( img ) \u53c2\u8003 - https://note.nkmk.me/python-image-processing-pillow-numpy-opencv/ - https://nixeneko.hatenablog.com/entry/2017/09/01/000000 - https://tomomai.com/python-opencv-pillow/ - https://www.codexa.net/opencv_python_introduction/ (open CV\u306b\u95a2\u3057\u3066) Note \u4e0b\u8a18\u306e\u753b\u50cf\u8868\u793a\u30b3\u30fc\u30c9\u306f\u3001 https://github.com/tkuri/albumentations_test/blob/master/albumentations_test.ipynb \u3000\u3092\u53c2\u8003\u306b\u3057\u305f\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 aug = [] n = 3 param1 = ( 1 , 20 ) param2 = ( 16 , 16 ) aug . append ( A . Compose ([ A . Blur ( p = 1 )])) aug . append ( A . Compose ([ A . MedianBlur ( p = 1 )]) aug . append ( A . Compose ([ A . GaussianBlur ( p = 1 )]) aug_img = [ aug [ i ]( image = img ) for i in range ( n )] fig , ax = plt . subplots ( 1 , 1 + n , figsize = ( 5 + 5 * n , 5 )) plt . subplots_adjust ( wspace = 0 ) plt . rcParams [ \"font.size\" ] = 18 [ ax [ i ] . tick_params ( bottom = False , left = False , right = False , top = False , labelbottom = False , labelleft = False , labelright = False , labeltop = False ) for i in range ( 1 + n )] ax [ 0 ] . set_xlabel ( \"Original\" ) ax [ 1 ] . set_xlabel ( \"Default Augmentation\" ) ax [ 2 ] . set_xlabel ( \"blur_limit= {} \" . format ( param1 )) ax [ 3 ] . set_xlabel ( \"blur_limit= {} \" . format ( param2 )) ax [ 0 ] . imshow ( img ) [ ax [ i + 1 ] . imshow ( aug_img [ i ][ 'image' ]) for i in range ( n )] Albumentations \u00b6 \u53c2\u8003\uff1a https://qiita.com/kurilab/items/b69e1be8d0224ae139ad Flip, Crop, Rotate etc.\uff08\u30d5\u30ea\u30c3\u30d7\u3001\u5207\u308a\u53d6\u308a\u3001\u56de\u8ee2\u306a\u3069\uff09 \u00b6 \u30d5\u30ea\u30c3\u30d7 \u00b6 \u5207\u308a\u53d6\u308a \u00b6 Blur, Noise\uff08\u307c\u304b\u3057\uff09 \u00b6 Blur \u00b6 \u9ad8\u5ea6\u5e7e\u4f55\u5909\u63db\u7cfb (Affine, Distortion) \u00b6","title":"Image Augmentations"},{"location":"albumentations/#image-augumentations","text":"","title":"Image Augumentations"},{"location":"albumentations/#_1","text":"","title":"\u30b5\u30f3\u30d7\u30eb\u5199\u771f\u306e\u8868\u793a"},{"location":"albumentations/#import","text":"1 2 3 4 5 import matplotlib.pyplot as plt import albumentations as A import numpy as np import cv2 from PIL import Image","title":"\u30e9\u30a4\u30d6\u30e9\u30ea\u306eimport"},{"location":"albumentations/#pillow-opencv","text":"\u753b\u50cf\u30c7\u30fc\u30bf\u306fKaggle\u306e Flowers Recognition \u304b\u3089\u53d6\u5f97\u3002Pillow\u3092\u4f7f\u3046\u5834\u5408\u306f\u3001\u8aad\u307f\u8fbc\u3093\u3060\u3068\u304d\u306bJpegImageFile\u306a\u306e\u3067openCV\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 image_path = '../input/flowers-recognition/flowers/daisy/10140303196_b88d3d6cec.jpg' Pillow\u306f\u5358\u306a\u308b\u753b\u50cf\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308a\u3001OpenCV\u306f\u300c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u300d\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u78ba\u304b\u306b\u6a5f\u80fd\u304c\u91cd\u8907\u3059\u308b\u90e8\u5206\u306f\u591a\u3044\uff08\u3064\u307e\u308aOpenCV\u306b\u306f\u304b\u306a\u308a\u306e\u753b\u50cf\u51e6\u7406\u6a5f\u80fd\u304c\u542b\u307e\u308c\u3066\u3044\u308b\uff09\u304c\u3001 \u305d\u306e\u6271\u3046\u5185\u5bb9\u306f\u5927\u304d\u304f\u7570\u306a\u308a\u307e\u3059\u3002\u6975\u7aef\u306a\u8a71\u3001\u753b\u50cf\u3092\u30ab\u30c3\u30c8\u3084\u30ea\u30b5\u30a4\u30ba\u3057\u305f\u3044\u6642\u3084\u3001\u5c11\u3057\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u3044\u5834\u5408\u306f Pillow \u3092\u4f7f\u3044\u3001\u7269\u4e8b\u3092\u300c\u898b\u3088\u3046\u300d\u3068\u601d\u3063\u3066\u3044\u308b\u30ed\u30dc\u30c3\u30c8\u3092\u7d44\u307f\u305f\u3044\u6642\u306b\u306f OpenCV \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u5f15\u7528\u5143\uff1a https://teratail.com/questions/71851 Pillow 1 2 3 4 5 6 img = Image . open ( image_path ) # img: JpegImageFile img = np . asarray ( img ) # \u3082\u3068\u306e\u753b\u50cf\u306b\u623b\u3059\u5834\u5408 # im = Image.fromarray(np.uint8(myarray*255)) plt . imshow ( img ) OpenCV 1 2 3 4 5 6 img = cv2 . imread ( image_path ) # img : ndarray (N-dimensional array, np.array\u306b\u3088\u3063\u3066\u751f\u6210) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # \u4e0b\u8a18\u3082RGB\u753b\u50cf\u2192BGR\u753b\u50cf\u3078\u306e\u5909\u63db #img = img[:,:,::-1] plt . imshow ( img ) \u53c2\u8003 - https://note.nkmk.me/python-image-processing-pillow-numpy-opencv/ - https://nixeneko.hatenablog.com/entry/2017/09/01/000000 - https://tomomai.com/python-opencv-pillow/ - https://www.codexa.net/opencv_python_introduction/ (open CV\u306b\u95a2\u3057\u3066) Note \u4e0b\u8a18\u306e\u753b\u50cf\u8868\u793a\u30b3\u30fc\u30c9\u306f\u3001 https://github.com/tkuri/albumentations_test/blob/master/albumentations_test.ipynb \u3000\u3092\u53c2\u8003\u306b\u3057\u305f\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 aug = [] n = 3 param1 = ( 1 , 20 ) param2 = ( 16 , 16 ) aug . append ( A . Compose ([ A . Blur ( p = 1 )])) aug . append ( A . Compose ([ A . MedianBlur ( p = 1 )]) aug . append ( A . Compose ([ A . GaussianBlur ( p = 1 )]) aug_img = [ aug [ i ]( image = img ) for i in range ( n )] fig , ax = plt . subplots ( 1 , 1 + n , figsize = ( 5 + 5 * n , 5 )) plt . subplots_adjust ( wspace = 0 ) plt . rcParams [ \"font.size\" ] = 18 [ ax [ i ] . tick_params ( bottom = False , left = False , right = False , top = False , labelbottom = False , labelleft = False , labelright = False , labeltop = False ) for i in range ( 1 + n )] ax [ 0 ] . set_xlabel ( \"Original\" ) ax [ 1 ] . set_xlabel ( \"Default Augmentation\" ) ax [ 2 ] . set_xlabel ( \"blur_limit= {} \" . format ( param1 )) ax [ 3 ] . set_xlabel ( \"blur_limit= {} \" . format ( param2 )) ax [ 0 ] . imshow ( img ) [ ax [ i + 1 ] . imshow ( aug_img [ i ][ 'image' ]) for i in range ( n )]","title":"Pillow \u3068OpenCV\u305d\u308c\u305e\u308c\u3067\u753b\u50cf\u3092\u8868\u793a"},{"location":"albumentations/#albumentations","text":"\u53c2\u8003\uff1a https://qiita.com/kurilab/items/b69e1be8d0224ae139ad","title":"Albumentations"},{"location":"albumentations/#flip-crop-rotate-etc","text":"","title":"Flip, Crop, Rotate etc.\uff08\u30d5\u30ea\u30c3\u30d7\u3001\u5207\u308a\u53d6\u308a\u3001\u56de\u8ee2\u306a\u3069\uff09"},{"location":"albumentations/#_2","text":"","title":"\u30d5\u30ea\u30c3\u30d7"},{"location":"albumentations/#_3","text":"","title":"\u5207\u308a\u53d6\u308a"},{"location":"albumentations/#blur-noise","text":"","title":"Blur, Noise\uff08\u307c\u304b\u3057\uff09"},{"location":"albumentations/#blur","text":"","title":"Blur"},{"location":"albumentations/#affine-distortion","text":"","title":"\u9ad8\u5ea6\u5e7e\u4f55\u5909\u63db\u7cfb (Affine, Distortion)"},{"location":"dcgan/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 28 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval betwen image samples\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , z ): img = self . model ( z ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . model = nn . Sequential ( nn . Linear ( int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 256 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 256 , 1 ), nn . Sigmoid (), ) def forward ( self , img ): img_flat = img . view ( img . size ( 0 ), - 1 ) validity = self . model ( img_flat ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . size ( 0 ), 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . size ( 0 ), 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) DCGAN \u00b6 G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084\u8ee2\u79fb\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092tanh\u95a2\u6570\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092leaky relu\u306b\u4ee3\u7528 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) cuda = True if torch . cuda . is_available () else False def weights_init_normal ( m ): classname = m . __class__ . __name__ if classname . find ( \"Conv\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 0.0 , 0.02 ) elif classname . find ( \"BatchNorm2d\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 1.0 , 0.02 ) torch . nn . init . constant_ ( m . bias . data , 0.0 ) class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . init_size = opt . img_size // 4 self . l1 = nn . Sequential ( nn . Linear ( opt . latent_dim , 128 * self . init_size ** 2 )) self . conv_blocks = nn . Sequential ( nn . BatchNorm2d ( 128 ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 64 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 64 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Conv2d ( 64 , opt . channels , 3 , stride = 1 , padding = 1 ), nn . Tanh (), ) def forward ( self , z ): out = self . l1 ( z ) out = out . view ( out . shape [ 0 ], 128 , self . init_size , self . init_size ) img = self . conv_blocks ( out ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () def discriminator_block ( in_filters , out_filters , bn = True ): block = [ nn . Conv2d ( in_filters , out_filters , 3 , 2 , 1 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Dropout2d ( 0.25 )] if bn : block . append ( nn . BatchNorm2d ( out_filters , 0.8 )) return block self . model = nn . Sequential ( * discriminator_block ( opt . channels , 16 , bn = False ), * discriminator_block ( 16 , 32 ), * discriminator_block ( 32 , 64 ), * discriminator_block ( 64 , 128 ), ) # The height and width of downsampled image ds_size = opt . img_size // 2 ** 4 self . adv_layer = nn . Sequential ( nn . Linear ( 128 * ds_size ** 2 , 1 ), nn . Sigmoid ()) def forward ( self , img ): out = self . model ( img ) out = out . view ( out . shape [ 0 ], - 1 ) validity = self . adv_layer ( out ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Initialize weights generator . apply ( weights_init_normal ) discriminator . apply ( weights_init_normal ) # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) Conditional GAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) WGAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) Cycle GAN Star GAN Pix2Pix","title":"Dcgan"},{"location":"dcgan/#dcgan","text":"G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084\u8ee2\u79fb\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092tanh\u95a2\u6570\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092leaky relu\u306b\u4ee3\u7528 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) cuda = True if torch . cuda . is_available () else False def weights_init_normal ( m ): classname = m . __class__ . __name__ if classname . find ( \"Conv\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 0.0 , 0.02 ) elif classname . find ( \"BatchNorm2d\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 1.0 , 0.02 ) torch . nn . init . constant_ ( m . bias . data , 0.0 ) class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . init_size = opt . img_size // 4 self . l1 = nn . Sequential ( nn . Linear ( opt . latent_dim , 128 * self . init_size ** 2 )) self . conv_blocks = nn . Sequential ( nn . BatchNorm2d ( 128 ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 64 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 64 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Conv2d ( 64 , opt . channels , 3 , stride = 1 , padding = 1 ), nn . Tanh (), ) def forward ( self , z ): out = self . l1 ( z ) out = out . view ( out . shape [ 0 ], 128 , self . init_size , self . init_size ) img = self . conv_blocks ( out ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () def discriminator_block ( in_filters , out_filters , bn = True ): block = [ nn . Conv2d ( in_filters , out_filters , 3 , 2 , 1 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Dropout2d ( 0.25 )] if bn : block . append ( nn . BatchNorm2d ( out_filters , 0.8 )) return block self . model = nn . Sequential ( * discriminator_block ( opt . channels , 16 , bn = False ), * discriminator_block ( 16 , 32 ), * discriminator_block ( 32 , 64 ), * discriminator_block ( 64 , 128 ), ) # The height and width of downsampled image ds_size = opt . img_size // 2 ** 4 self . adv_layer = nn . Sequential ( nn . Linear ( 128 * ds_size ** 2 , 1 ), nn . Sigmoid ()) def forward ( self , img ): out = self . model ( img ) out = out . view ( out . shape [ 0 ], - 1 ) validity = self . adv_layer ( out ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Initialize weights generator . apply ( weights_init_normal ) discriminator . apply ( weights_init_normal ) # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) Conditional GAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) WGAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) Cycle GAN Star GAN Pix2Pix","title":"DCGAN"},{"location":"gan/","text":"\u751f\u6210\u30e2\u30c7\u30eb \u00b6 \u751f\u6210\u30e2\u30c7\u30eb\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3069\u306e\u3088\u3046\u306b\u751f\u6210\u3055\u308c\u308b\u304b\u78ba\u7387\u30e2\u30c7\u30eb\u306e\u89b3\u70b9\u304b\u3089\u8a18\u8ff0\u3059\u308b\u3002\u3053\u306e\u30e2\u30c7\u30eb\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3002 \u8b58\u5225\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a$p(y|\\textbf{x})$\uff08\u89b3\u6e2c$\\textbf{x}$\u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u306e\u30e9\u30d9\u30eb$y$\u306e\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\uff09 \u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a$p(\\textbf{x})$\uff08\u89b3\u6e2c$\\textbf{x}$\u304c\u89b3\u6e2c\u3055\u308c\u308b\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\uff09 GAN \u00b6 \u751f\u6210\u5668 \u00b6 \u30ce\u30a4\u30ba\u3092\u5165\u308c\u308b\u3001\u751f\u6210\u5668\u3068\u8b58\u5225\u5668\u306e\u9a19\u3057\u5408\u3044\u3001 GAN\u3067\u306f\u78ba\u7387\u5909\u6570\u304c\u751f\u6210\u3055\u308c\u308b\u30bf\u30fc\u30b2\u30c3\u30c8\uff08\u753b\u50cf\u3067\u306f\u30d4\u30af\u30bb\u30eb\u306e\u30d1\u30bf\u30fc\u30f3\uff09 \u30c7\u30fc\u30bf\u304c\u5f93\u3046\u78ba\u7387\u5206\u5e03pr(x)\u305d\u306e\u3082\u306e\u306f\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u751f\u6210\u5668\u306e\u78ba\u7387pG(xi/z)\u3067\u8fd1\u4f3c\u3059\u308b pG(x/z)\u3092pr(x)\u306b\u8fd1\u3065\u3051\u3066\u3044\u304f\u305f\u3081\u306e\u6307\u6a19\u3068\u3057\u3066KL divergence\u3068JS divergence, GANs\u306e\u640d\u5931\u95a2\u6570\u306fJS divergence\u306e\u6700\u5c0f\u5316\uff08\u8b58\u5225\u304d\u306f\u6700\u5927\u5316\uff09\u304b\u3089\u5c0e\u304b\u308c\u308b\u3002 \u7279\u6b8a\u306a\u76ee\u7684\u95a2\u6570\uff08\u640d\u5931\u95a2\u6570\uff09\u3001\u30a8\u30f3\u30b3\u30fc\u30c9\u3068\u30c7\u30b3\u30fc\u30c9\u306e\u95a2\u4fc2 GAN\u306e\u6b20\u70b9 - \u5b66\u7fd2\u6642\u9593\u306e\u9577\u3055 - \u30e2\u30fc\u30c9\u5d29\u58ca - \u52fe\u914d\u6d88\u5931 - \u751f\u6210\u753b\u50cf\u306b\u7d30\u304b\u306a\u30ce\u30a4\u30ba\u304c\u5165\u308b - \u6bd4\u8f03\u53ef\u80fd\u306a\u578b\u306e\u30c7\u30fc\u30bf\u3067\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044 \u6f5c\u5728\u5909\u6570\u3068\u306f\u751f\u6210\u753b\u50cf\u306e\u5143\u306b\u306a\u308b\u6b21\u5143\u524a\u6e1b\u3055\u308c\u305f\u7279\u5fb4\u91cf VAE\u306f\u6f5c\u5728\u5909\u6570\u3092\u6b63\u898f\u5206\u5e03\u3068\u4eee\u5b9a","title":"Gan"},{"location":"gan/#_1","text":"\u751f\u6210\u30e2\u30c7\u30eb\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3069\u306e\u3088\u3046\u306b\u751f\u6210\u3055\u308c\u308b\u304b\u78ba\u7387\u30e2\u30c7\u30eb\u306e\u89b3\u70b9\u304b\u3089\u8a18\u8ff0\u3059\u308b\u3002\u3053\u306e\u30e2\u30c7\u30eb\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3002 \u8b58\u5225\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a$p(y|\\textbf{x})$\uff08\u89b3\u6e2c$\\textbf{x}$\u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u306e\u30e9\u30d9\u30eb$y$\u306e\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\uff09 \u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a$p(\\textbf{x})$\uff08\u89b3\u6e2c$\\textbf{x}$\u304c\u89b3\u6e2c\u3055\u308c\u308b\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\uff09","title":"\u751f\u6210\u30e2\u30c7\u30eb"},{"location":"gan/#gan","text":"","title":"GAN"},{"location":"gan/#_2","text":"\u30ce\u30a4\u30ba\u3092\u5165\u308c\u308b\u3001\u751f\u6210\u5668\u3068\u8b58\u5225\u5668\u306e\u9a19\u3057\u5408\u3044\u3001 GAN\u3067\u306f\u78ba\u7387\u5909\u6570\u304c\u751f\u6210\u3055\u308c\u308b\u30bf\u30fc\u30b2\u30c3\u30c8\uff08\u753b\u50cf\u3067\u306f\u30d4\u30af\u30bb\u30eb\u306e\u30d1\u30bf\u30fc\u30f3\uff09 \u30c7\u30fc\u30bf\u304c\u5f93\u3046\u78ba\u7387\u5206\u5e03pr(x)\u305d\u306e\u3082\u306e\u306f\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u751f\u6210\u5668\u306e\u78ba\u7387pG(xi/z)\u3067\u8fd1\u4f3c\u3059\u308b pG(x/z)\u3092pr(x)\u306b\u8fd1\u3065\u3051\u3066\u3044\u304f\u305f\u3081\u306e\u6307\u6a19\u3068\u3057\u3066KL divergence\u3068JS divergence, GANs\u306e\u640d\u5931\u95a2\u6570\u306fJS divergence\u306e\u6700\u5c0f\u5316\uff08\u8b58\u5225\u304d\u306f\u6700\u5927\u5316\uff09\u304b\u3089\u5c0e\u304b\u308c\u308b\u3002 \u7279\u6b8a\u306a\u76ee\u7684\u95a2\u6570\uff08\u640d\u5931\u95a2\u6570\uff09\u3001\u30a8\u30f3\u30b3\u30fc\u30c9\u3068\u30c7\u30b3\u30fc\u30c9\u306e\u95a2\u4fc2 GAN\u306e\u6b20\u70b9 - \u5b66\u7fd2\u6642\u9593\u306e\u9577\u3055 - \u30e2\u30fc\u30c9\u5d29\u58ca - \u52fe\u914d\u6d88\u5931 - \u751f\u6210\u753b\u50cf\u306b\u7d30\u304b\u306a\u30ce\u30a4\u30ba\u304c\u5165\u308b - \u6bd4\u8f03\u53ef\u80fd\u306a\u578b\u306e\u30c7\u30fc\u30bf\u3067\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044 \u6f5c\u5728\u5909\u6570\u3068\u306f\u751f\u6210\u753b\u50cf\u306e\u5143\u306b\u306a\u308b\u6b21\u5143\u524a\u6e1b\u3055\u308c\u305f\u7279\u5fb4\u91cf VAE\u306f\u6f5c\u5728\u5909\u6570\u3092\u6b63\u898f\u5206\u5e03\u3068\u4eee\u5b9a","title":"\u751f\u6210\u5668"},{"location":"linux_command/","text":"Linux \u30b3\u30de\u30f3\u30c9 \u00b6 \u57fa\u672c\u64cd\u4f5c ctl + a: atama\u306b\u79fb\u52d5 ctrl + e: end\u306b\u79fb\u52d5 ctrl + w:\u3000 word: \u5358\u8a9e\u5358\u4f4d\u3067\u524a\u9664 \u30ab\u30c3\u30c8\u3000\u30a2\u30f3\u30c9\u3000\u30e4\u30f3\u30af ctrl + u \u884c\u982d\u307e\u3067\u30ab\u30c3\u30c8 ctrl + k:\u884c\u672b\u307e\u3067\u30ab\u30c3\u30c8 ctrl + y (yank) \u30bf\u30d6\u3067\u30aa\u30fc\u30c8\u30b3\u30f3\u30d7\u30ea\u30fc\u30c8 ls cat/less space(\u4e00\u753b\u9762\u4e0b)\u3000b\uff08\u4e00\u753b\u9762\u4e0a\uff09 j\uff08\u4e00\u884c\u305a\u3064\u4e0b\uff09 k\uff08\u4e00\u884c\u305a\u3064\u4e0a\uff09 q \u306f\u3082\u3068\u306e\u753b\u9762\u306b\u623b\u308b\uff08quit\uff09 wget \u30b3\u30de\u30f3\u30c9 unzip cp /etc/crontab file2 cp file1 directory\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30b3\u30d4\u30fc\u53ef\u80fd \u6307\u5b9a\u3057\u305f\u30b3\u30d4\u30fc\u5148\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4e2d\u306b\u306a\u308b \u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u65b0\u3057\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308b\u3002 cp -r dir1 dir2\u3067\u518d\u5e30\u7684\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b3\u30d4\u30fc\u53ef\u80fd mv\u30b3\u30de\u30f3\u30c9\u3067\u540d\u524d\u3092\u5909\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3057\u3001\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5\u3067\u304d\u308b\u3002 \u30cf\u30fc\u30c9\u30ea\u30f3\u30af \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af ln file1 file2 (file1\u306bfile2\u3068\u3044\u3046\u30cf\u30fc\u30c9\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b\u3002file1\u3092\u4f5c\u6210\u3057\u3066\u3082file2\u304c\u6b8b\u308b) ln -s file1 file2\u3067\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b mkdir -p dir1/dir2/dir3/target touch p dir1/dir2/dir3/target/file ln -s dir1/dir2/dir3/target/ target \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(/)\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(~)\u306e\u3061\u3083\u3093\u3068\u3057\u305f\u7406\u89e3 - Qiita \u3082\u306e\u3059\u3054\u3044\u7d30\u304b\u3044\u3053\u3068\u3060\u3051\u3069\u3001\u30d1\u30b9\u306e\u6307\u5b9a\u65b9\u6cd5\u3067\u306e ~ \uff08\u30c1\u30eb\u30c0\uff09\u3068 / \uff08\u30b9\u30e9\u30c3\u30b7\u30e5\uff09\u306e\u7406\u89e3\u304c\u66d6\u6627\u3067\u6c17\u6301\u3061\u60aa\u3044\u601d\u3044\u3092\u3057\u305f\u306e\u3067\u30e1\u30e2\u3002 / : \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~ \uff1a\u4eca\u306e\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~taro : taro\u3068\u3044\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u30b9\u30e9\u30c3\u30b7\u30e5\u306e\u610f\u5473\u5408\u3044 \u00b6 \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e / \u3068\u3001\u5404\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u524d\u306b\u3064\u304f / \u306f\u610f\u5473\u5408\u3044\u304c\u9055\u3063\u3066\u3044\u308b\u6a21\u69d8\u3002 \u524d\u8005\uff1a\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u305d\u306e\u3082\u306e \u5f8c\u8005\uff1a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u533a\u5207\u308b\u3082\u306e \u306a\u306e\u3067\u3001\u4e00\u898b\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u305b\u3044\u3067\u300c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u306f\u672b\u5c3e\u306b\u30b9\u30e9\u30c3\u30b7\u30e5\u304c\u4ed8\u3044\u3066\u3044\u308b\u3082\u306e\u300d\u3068\u3044\u3046\u52d8\u9055\u3044\u3092\uff08\u5c11\u306a\u304f\u3082\u7b46\u8005\u306f\uff09\u3057\u3061\u3083\u3046\u304c\u3001 hogehoge/ \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3067\u306f\u306a\u304f hogehoge \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3060\u3002\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ~/ \u3060\u3068\u601d\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u4eba\u306f\u591a\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f history !393\u3067\u4f7f\u3048\u308b \u30d4\u30ea\u30aa\u30c9\u3067\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea find . -name '*.txt' -print \u3053\u306e\u30a2\u30b9\u30bf\u30ea\u30b9\u30af\u306f\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3067\u30d1\u30b9\u540d\u5c55\u958b\u3068\u306f\u9055\u3046\u3002\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c6\u30b7\u30e7\u30f3\u304b\u3069\u3046\u304b find . -type d \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u691c\u7d22 find . -type d -a -name share locate \u30b3\u30de\u30f3\u30c9\u306ffind\u30b3\u30de\u30f3\u30c9\u3088\u308a\u3082\u9ad8\u901f\uff08\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u691c\u7d22\uff09 sudo updatedb\u3092\u3057\u3066\u304b\u3089 locate bash -A doc \u3000and \u691c\u7d22 locate bash doc grep bin /etc/crontab \u30d5\u30a3\u30eb\u30bf history | head wc:\u6587\u5b57\u6570\u3092\u6570\u3048\u308b wc -l ls / | wc -l \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u500b\u6570\u884c\u6570 \u30bd\u30fc\u30c8\u30b3\u30de\u30f3\u30c9 sort word.txt sort -r word.txt sort -n number.txt \u91cd\u8907\u3092\u53d6\u308a\u51fa\u3059 uniq number.txt sort -n number.txt | uniq sort -n number.txt | uniq -c | sort -nr | head -n 3 \u30d5\u30a1\u30a4\u30eb\u3092\u76e3\u8996\u3059\u308b tail -f log.txt \u30e1\u30e2\u30ea\u304b\u3089\u898b\u305f\u5b9f\u884c\u72b6\u614b\u306b\u3042\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30d7\u30ed\u30bb\u30b9\u3068\u3044\u3046 \u30b8\u30e7\u30d6\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306b\u5165\u529b\u3055\u308c\u305f\u884c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3068\u304d\u306f\u8907\u6570\u306b\u306a\u308b\u3002\uff09 ps\u30b3\u30de\u30f3\u30c9 ps -x ps -u sleep\u30b3\u30de\u30f3\u30c9 jobs\u30b3\u30de\u30f3\u30c9 fg\u30b3\u30de\u30f3\u30c9 bg\u30b3\u30de\u30f3\u30c9","title":"Linux \u30b3\u30de\u30f3\u30c9"},{"location":"linux_command/#linux","text":"\u57fa\u672c\u64cd\u4f5c ctl + a: atama\u306b\u79fb\u52d5 ctrl + e: end\u306b\u79fb\u52d5 ctrl + w:\u3000 word: \u5358\u8a9e\u5358\u4f4d\u3067\u524a\u9664 \u30ab\u30c3\u30c8\u3000\u30a2\u30f3\u30c9\u3000\u30e4\u30f3\u30af ctrl + u \u884c\u982d\u307e\u3067\u30ab\u30c3\u30c8 ctrl + k:\u884c\u672b\u307e\u3067\u30ab\u30c3\u30c8 ctrl + y (yank) \u30bf\u30d6\u3067\u30aa\u30fc\u30c8\u30b3\u30f3\u30d7\u30ea\u30fc\u30c8 ls cat/less space(\u4e00\u753b\u9762\u4e0b)\u3000b\uff08\u4e00\u753b\u9762\u4e0a\uff09 j\uff08\u4e00\u884c\u305a\u3064\u4e0b\uff09 k\uff08\u4e00\u884c\u305a\u3064\u4e0a\uff09 q \u306f\u3082\u3068\u306e\u753b\u9762\u306b\u623b\u308b\uff08quit\uff09 wget \u30b3\u30de\u30f3\u30c9 unzip cp /etc/crontab file2 cp file1 directory\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30b3\u30d4\u30fc\u53ef\u80fd \u6307\u5b9a\u3057\u305f\u30b3\u30d4\u30fc\u5148\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4e2d\u306b\u306a\u308b \u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u65b0\u3057\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308b\u3002 cp -r dir1 dir2\u3067\u518d\u5e30\u7684\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b3\u30d4\u30fc\u53ef\u80fd mv\u30b3\u30de\u30f3\u30c9\u3067\u540d\u524d\u3092\u5909\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3057\u3001\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5\u3067\u304d\u308b\u3002 \u30cf\u30fc\u30c9\u30ea\u30f3\u30af \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af ln file1 file2 (file1\u306bfile2\u3068\u3044\u3046\u30cf\u30fc\u30c9\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b\u3002file1\u3092\u4f5c\u6210\u3057\u3066\u3082file2\u304c\u6b8b\u308b) ln -s file1 file2\u3067\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b mkdir -p dir1/dir2/dir3/target touch p dir1/dir2/dir3/target/file ln -s dir1/dir2/dir3/target/ target \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(/)\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(~)\u306e\u3061\u3083\u3093\u3068\u3057\u305f\u7406\u89e3 - Qiita \u3082\u306e\u3059\u3054\u3044\u7d30\u304b\u3044\u3053\u3068\u3060\u3051\u3069\u3001\u30d1\u30b9\u306e\u6307\u5b9a\u65b9\u6cd5\u3067\u306e ~ \uff08\u30c1\u30eb\u30c0\uff09\u3068 / \uff08\u30b9\u30e9\u30c3\u30b7\u30e5\uff09\u306e\u7406\u89e3\u304c\u66d6\u6627\u3067\u6c17\u6301\u3061\u60aa\u3044\u601d\u3044\u3092\u3057\u305f\u306e\u3067\u30e1\u30e2\u3002 / : \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~ \uff1a\u4eca\u306e\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~taro : taro\u3068\u3044\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea","title":"Linux \u30b3\u30de\u30f3\u30c9"},{"location":"linux_command/#_1","text":"\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e / \u3068\u3001\u5404\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u524d\u306b\u3064\u304f / \u306f\u610f\u5473\u5408\u3044\u304c\u9055\u3063\u3066\u3044\u308b\u6a21\u69d8\u3002 \u524d\u8005\uff1a\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u305d\u306e\u3082\u306e \u5f8c\u8005\uff1a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u533a\u5207\u308b\u3082\u306e \u306a\u306e\u3067\u3001\u4e00\u898b\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u305b\u3044\u3067\u300c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u306f\u672b\u5c3e\u306b\u30b9\u30e9\u30c3\u30b7\u30e5\u304c\u4ed8\u3044\u3066\u3044\u308b\u3082\u306e\u300d\u3068\u3044\u3046\u52d8\u9055\u3044\u3092\uff08\u5c11\u306a\u304f\u3082\u7b46\u8005\u306f\uff09\u3057\u3061\u3083\u3046\u304c\u3001 hogehoge/ \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3067\u306f\u306a\u304f hogehoge \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3060\u3002\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ~/ \u3060\u3068\u601d\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u4eba\u306f\u591a\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f history !393\u3067\u4f7f\u3048\u308b \u30d4\u30ea\u30aa\u30c9\u3067\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea find . -name '*.txt' -print \u3053\u306e\u30a2\u30b9\u30bf\u30ea\u30b9\u30af\u306f\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3067\u30d1\u30b9\u540d\u5c55\u958b\u3068\u306f\u9055\u3046\u3002\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c6\u30b7\u30e7\u30f3\u304b\u3069\u3046\u304b find . -type d \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u691c\u7d22 find . -type d -a -name share locate \u30b3\u30de\u30f3\u30c9\u306ffind\u30b3\u30de\u30f3\u30c9\u3088\u308a\u3082\u9ad8\u901f\uff08\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u691c\u7d22\uff09 sudo updatedb\u3092\u3057\u3066\u304b\u3089 locate bash -A doc \u3000and \u691c\u7d22 locate bash doc grep bin /etc/crontab \u30d5\u30a3\u30eb\u30bf history | head wc:\u6587\u5b57\u6570\u3092\u6570\u3048\u308b wc -l ls / | wc -l \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u500b\u6570\u884c\u6570 \u30bd\u30fc\u30c8\u30b3\u30de\u30f3\u30c9 sort word.txt sort -r word.txt sort -n number.txt \u91cd\u8907\u3092\u53d6\u308a\u51fa\u3059 uniq number.txt sort -n number.txt | uniq sort -n number.txt | uniq -c | sort -nr | head -n 3 \u30d5\u30a1\u30a4\u30eb\u3092\u76e3\u8996\u3059\u308b tail -f log.txt \u30e1\u30e2\u30ea\u304b\u3089\u898b\u305f\u5b9f\u884c\u72b6\u614b\u306b\u3042\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30d7\u30ed\u30bb\u30b9\u3068\u3044\u3046 \u30b8\u30e7\u30d6\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306b\u5165\u529b\u3055\u308c\u305f\u884c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3068\u304d\u306f\u8907\u6570\u306b\u306a\u308b\u3002\uff09 ps\u30b3\u30de\u30f3\u30c9 ps -x ps -u sleep\u30b3\u30de\u30f3\u30c9 jobs\u30b3\u30de\u30f3\u30c9 fg\u30b3\u30de\u30f3\u30c9 bg\u30b3\u30de\u30f3\u30c9","title":"\u30b9\u30e9\u30c3\u30b7\u30e5\u306e\u610f\u5473\u5408\u3044"},{"location":"pytorch_basis/","text":"def func_kwargs(**kwargs): print('kwargs: ', kwargs) print('type: ', type(kwargs)) func_kwargs(key1=1, key2=2, key3=3) kwargs: {'key1': 1, 'key2': 2, 'key3': 3} \u00b6 type: \u00b6 def func_kwargs_positional(arg1, arg2, **kwargs): print('arg1: ', arg1) print('arg2: ', arg2) print('kwargs: ', kwargs) func_kwargs_positional(0, 1, key1=1) arg1: 0 \u00b6 arg2: 1 \u00b6 kwargs: {'key1': 1} \u00b6 Summary\u306e\u51fa\u3057\u65b9 \u00b6 1 2 3 4 5 from torchvision import models from torchsummary import summary vgg = models . vgg16 () summary ( vgg , ( 3 , 224 , 224 )) \u3053\u308c\u306f\u5b9f\u306f\uff0cCrossEntropyLoss\u306fcall\u3067forward\u3092\u547c\u3076\u3088\u3046\u306b\u306a\u3063\u3066\u304a\u308a\uff0c\u3064\u307e\u308a\uff0c loss = criterion(outputs, labels) loss = criterion.forward(outputs, labels) \u3053\u306e\u4e8c\u3064\u306f\u540c\u3058\u3053\u3068\u3092\u3057\u3066\u3044\u307e\u3059\uff0e \u306a\u306e\u3067loss = criterion(outputs, labels)\u304cforward\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0e x = torch.autograd.Variable(torch.Tensor([3,4]), requires_grad=True) requires_grad=True\u3067\uff0c\u3053\u306eVariable\u306f\u5fae\u5206\u3059\u308b\u305e\u3068\u4f1d\u3048\u308b \u00b6 print(\"x.grad : \", x.grad) None \u00b6 \u3053\u306e\u6642\u70b9\u3067\u306f\u307e\u3060\u4f55\u3082\u5165\u3063\u3066\u3044\u306a\u3044\uff0e \u00b6 \u9069\u5f53\u306b\u76ee\u7684\u95a2\u6570\u3092\u4f5c\u308b\uff0e \u00b6 y = x[0] 2 + 5 x[1] + x[0] x[1] x[0]\u306e\u5c0e\u95a2\u6570 : 2*x[0] + x[1] \u00b6 x[0]\u306e\u5fae\u5206\u4fc2\u6570 : 2*3 + 4 = 10 \u00b6 x[1]\u306e\u5c0e\u95a2\u6570 : 5 + x[0] \u00b6 x[1]\u306e\u5fae\u5206\u4fc2\u6570 : 5 + 3 = 8 \u00b6 y.backward() torch.autograd.backward(y)\u3000\u3067\u3082\u826f\u3044\uff0e \u00b6 print(\"x.grad : \", x.grad) 10 \u00b6 8 \u00b6 .zero_grad()\u306e\u4ee3\u308f\u308a \u00b6 x.grad = None for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # zero the parameter gradients optimizer.zero_grad() #\u52fe\u914d\u306e\u521d\u671f\u5316 # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) #\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u5c64\u306e\uff08\u30d0\u30c3\u30c1\u6570, \u6b21\u5143(ex \u30af\u30e9\u30b9\u6570)\uff09\u304c\u51fa\u529b\u3055\u308c\u308b print(f'outputs: {outputs}') _, preds = torch.max(outputs, 1) #\u6700\u5927\u3068\u306a\u308bindex\u3092\u8fd4\u3059 print(f'preds: {preds}, labels: {labels}') loss = criterion(outputs, labels) #\u640d\u5931\u5024\u3092\u51fa\u3059 print(f'loss: {loss}') # backward + optimize only if in training phase if phase == 'train': loss.backward()\u3000#\u5c0e\u95a2\u6570\u306e\u7d50\u679c\u304c\u7d2f\u7a4d optimizer.step()\u3000#parameter\u306e\u66f4\u65b0 date_info = {'year': \"2020\", 'month': \"01\", 'day': \"01\"} filename = \"{year}-{month}-{day}.txt\".format(**date_info) filename '2020-01-01.txt' scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch) for epoch in range(0, 100): #\u3053\u3053\u306f\u4ee5\u4e0b\u7701\u7565 scheduler.step() os.cpu_count() psutill.cpu_count AMP https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587 **data\u306f\u8f9e\u66f8\u3092\u53d7\u3051\u53d6\u308b self\u3067\u81ea\u5206\u81ea\u8eab\u3064\u307e\u308aforward\u304c\u547c\u3070\u308c\u308b callback\u95a2\u6570\u306f\u975e\u540c\u671f\u3063\u307d\u3044\u3082\u306e \u640d\u5931\u95a2\u6570 https://yoshinashigoto-blog.herokuapp.com/detail/27/ from torchvision import models model = models.mnasnet0_5() torch.save(model.to('cpu').state_dict(), 'model.pth') from torchvision import models model = models.mnasnet0_5() model.load_state_dict(torch.load('model.pth')) \u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b \u00b6 epoch = 10 \u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u4fdd\u5b58\u3059\u308b\u3002 \u00b6 torch.save( { \"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), }, \"model.tar\", ) \u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u8aad\u307f\u8fbc\u3080\u3002 \u00b6 checkpoint = torch.load(\"model.tar\") model.load_state_dict(checkpoint[\"model_state_dict\"]) optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) epoch = checkpoint[\"epoch\"] 01.\u3000\u640d\u5931\u95a2\u6570\u3068\u306f \u307e\u305a\u640d\u5931\u95a2\u6570\u3068\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\u304c\u3046\u307e\u304f\u884c\u3063\u305f\u306e\u304b\u3069\u3046\u304b\u5224\u65ad\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3059\u308b\u95a2\u6570\u3067\u3059\u3002 \u3053\u306e\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u4e88\u6e2c\u3068\u7b54\u3048\u306e\u8aa4\u5dee\u3092\u6c42\u3081\u307e\u3059\u3002 \u305d\u306e\u8aa4\u5dee\u304c\u6700\u5c0f\u306b\u306a\u308c\u3070\u4e88\u6e2c\u306f\u3088\u308a\u6b63\u78ba\u306a\u3082\u306e\u3060\u3063\u305f\u3068\u3044\u3046\u8a55\u4fa1\u304c\u306a\u3055\u308c\u307e\u3059\u3002 \u640d\u5931\u95a2\u6570\u306b\u306f\u4e0b\u3067\u89e6\u308c\u308b\u3060\u3051\u306e\u7a2e\u985e\u304c\u3042\u308a\u3001\u76ee\u7684\u306b\u3088\u3063\u3066\u4f7f\u3044\u5206\u3051\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u95a2\u6570\u3092\u7528\u3044\u3066\u6570\u5b66\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3059\u308b\u3053\u3068\u3067\u6a5f\u68b0\u5b66\u7fd2\u306e\u4e88\u6e2c\u306e\u6b63\u78ba\u6027\u3092\u9ad8\u3081\u3066\u3044\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u3067\u306f\u6570\u5b66\u7684\u306a\u8981\u7d20\u306b\u8e0f\u307f\u8fbc\u307f\u3059\u304e\u306a\u3044\u7a0b\u5ea6\u306b\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3067\u306e\u6d3b\u7528\u65b9\u6cd5\u3092\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u3057\u3066\u3044\u304d\u307e\u3059\u3002 \u3061\u306a\u307f\u306b\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3068\u306f\u4e0d\u898f\u5247\u6027\u306e\u7a0b\u5ea6\u3092\u8868\u3059\u91cf\u3092\u3044\u3044\u307e\u3059\u3002 \u305d\u306e\u901a\u308a\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u306d\u3002 _02.\u3000\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u304c2\u30af\u30e9\u30b9\u306e\u5834\u5408\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u3068\u3044\u3046\u306e\u306f\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u304c2\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u4e00\u7a2e\u306e\u8ddd\u96e2\u3092\u8868\u3059\u3088\u3046\u306a\u6307\u6a19\u3067\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u3068\u306e\u9593\u306b\u3069\u306e\u7a0b\u5ea6\u306e\u5dee\u304c\u3042\u308b\u306e\u304b\u3092\u793a\u3059\u5c3a\u5ea6\u3067\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931L(y,t)\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u78ba\u7387yi\u3068\u6b63\u89e3j\u30af\u30e9\u30b9ti\u3092\u8868\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u5024yi\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u5c64\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5024\u3092\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u5909\u63db\u3057\u305f\u78ba\u7387\u5024\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u51fa\u529b\u5c64\u304b\u3089\u306e\u51fa\u529b\u5024\u3092\u30ed\u30b8\u30c3\u30c8\u3068\u3044\u3044\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u3068\u306f\u3042\u308b\u3042\u308b\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3068\u305d\u3046\u3067\u306a\u3044\u78ba\u73871-pn\u306e\u6bd4\u306b\u5bfe\u6570\u3092\u3068\u3063\u305f\u5024\u3067\u3059\u3002 \u5148\u306b\u51fa\u3066\u304d\u305f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306f\u30ed\u30b8\u30c3\u30c8\u95a2\u6570\u306e\u9006\u95a2\u6570\u3067\u3059\u3002 \u305d\u306e\u305f\u3081\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u30ed\u30b8\u30c3\u30c8\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u3067\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u8981\u306f\u51fa\u529b\u5024\u30920\u304b\u30891\u306e\u7bc4\u56f2\u306b\u6291\u3048\u3064\u3064\u6271\u3044\u3084\u3059\u3044\u78ba\u7387\u306e\u5f62\u306b\u5909\u63db\u3067\u304d\u308b\u516c\u5f0f\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCELoss()\u3067\u3059\u3002 \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306fnn.Sigmoid()\u3067\u3059\u3002 \u306a\u304a\u3001nn.BCELoss\u306ftorch.float32\u578b\u3092\u30c7\u30fc\u30bf\u578b\u3068\u3057\u3066\u4f7f\u7528\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u305d\u306e\u305f\u3081\u6b63\u89e3\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u578b\u306f\u672c\u6765int\u3067\u3059\u304cfloat\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 import torch from torch import nn m = nn.Sigmoid() y = torch.rand(3) t = torch.empty(3, dtype=torch.float32).random_(2) criterion = nn.BCELoss() loss = criterion(m(y), t) print(\"y: {}\".format(y)) print(\"m(y): {}\".format(m(y))) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss)) \u5b9f\u884c\u7d50\u679c \u00b6 y: tensor([0.2744, 0.9147, 0.3309]) m(y): tensor([0.5682, 0.7140, 0.5820]) t: tensor([0., 1., 0.]) loss: 0.6830 loss\u304c\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 _03.\u3000\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306b\u6700\u521d\u304b\u3089\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u304c\u52a0\u3048\u3089\u308c\u305f\u3082\u306e\u3067\u3059\u3002 \u3059\u306a\u308f\u3061\u51fa\u529b\u5024\u3092\u305d\u306e\u307e\u307e\u4e0e\u3048\u308c\u3070\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u304c\u5f97\u3089\u308c\u307e\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u3001\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u306e\u30af\u30e9\u30b9ti\u3092L(y, t)\u3068\u3057\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCEWithLogitsLoss()\u3067\u3059\u3002 \u9577\u3044\u3067\u3059\u306d\u3002 import torch from torch import nn y = torch.rand(3) t = torch.empty(3, dype=torch.float32).random_(2) criterion = nn.BCEWithLogitsLoss() loss = criterion(y, t) print(\"y: {}\".format(y)) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss)) \u5b9f\u884c\u7d50\u679c \u00b6 y: tensor([0.9709, 0.8976, 0.3228]) t: tensor([0., 1., 0.]) loss: 0.8338 loss\u304c\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 .format()\u3067\u306f\u6307\u5b9a\u3057\u305f\u5909\u6570\u3092{}\u306e\u4e2d\u306b\u4ee3\u5165\u3057\u3066\u305d\u308c\u3092\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002 _04.\u3000\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3082\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u30af\u30e9\u30b9\u304c\u3069\u306e\u304f\u3089\u3044\u96e2\u308c\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u5c3a\u5ea6\u3067\u3059\u3002 \u7279\u306b2\u30af\u30e9\u30b9\u4ee5\u4e0a\u306e\u591a\u30af\u30e9\u30b9\u306b\u5206\u985e\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u306e\u5206\u985e\u3067\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u304c\u30012\u30af\u30e9\u30b9\u4ee5\u4e0a\u3067\u306f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fn\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b9k\u306e\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u30af\u30e9\u30b9ti\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066L(y, t)\u3067\u8868\u3059\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fnn.CrossEntropyLoss\u3067\u3059\u3002 import torch from torch import nn y = torch.rand(3, 5) t = torch.empty(3, dtype=torch.int64).random_(5) criterion = nn.CrossEntropyLoss() loss = criterion(y, t) print(\"y:{}\".format(y)) print(\"t:{}\".format(t)) print(\"loss: {:4f}\".format(loss)) \u5b9f\u884c\u7d50\u679c \u00b6 y: tensor([[0.7775, 0.7587, 0.9474, 0.5149, 0.7741], [0.5059, 0.4802, 0.9846, 0.6292, 0.0167], [0.4339, 0.6873, 0.4253, 0.7067, 0.5678]]) t: tensor([1, 4, 1]) loss: 1.757074 \u30c7\u30fc\u30bf\u6570\u306f3\u3064\u3067\u5404\u30af\u30e9\u30b9\u306b\u51fa\u529b\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u6570\u306f5\u3064\u3067\u3059\u3002 loss\u304c\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 torch.cuda.amp.autocast():\u306e\u6b63\u3057\u3044indent to('cpu').numpy() cpu().detach().numpy() \u9055\u3044 ver1 \u00b6 def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class CFG: project_name = 'sample2' model_name = 'resnet18' note = '2nd' batch_size= 4 n_fold= 4 num_workers =4 image_size =224 epochs = 25 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) from tqdm import tqdm class BasicNN(nn.Module): def init (self, cfg): super(). init () self.cfg = cfg self.model = models.resnet18(pretrained=True) self.model.fc = nn.Linear(self.model.fc.in_features, 2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 self.model = self.model.to('cuda') self.current_epoch = 0 self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler = None self.metrics = None self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, fp16, ): self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.loss() if not self.optimizer: self.optimizer = self.fetch_optimizer() if not self.scheduler: self.scheduler = self.fetch_scheduler() def _init_wandb(self): hyperparams = { 'model_name' : self.cfg.model_name, 'batch_size' : self.cfg.batch_size, 'n_fold' : self.cfg.n_fold, 'num_workers' : self.cfg.num_workers, 'image_size' : self.cfg.image_size, 'epochs' : self.cfg.epochs } wandb.init( config = hyperparams, project= self.cfg.project_name, name=self.cfg.exp_name, ) wandb.watch(self) def loss(self): loss = nn.CrossEntropyLoss() return loss def fetch_optimizer(self): #opt = torch.optim.Adam(self.parameters(), lr=5e-4) opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def fetch_scheduler(self): # sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( # self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 # ) sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def monitor_metrics(self, *args, **kwargs): self.metrics = None return def forward(self, x): return self.model(x) def model_fn(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') return self(inputs) def train_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) loss.backward() self.optimizer.step() return loss, preds, labels def train_one_epoch(self): self.train() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.train_loader: loss, preds, labels = self.train_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) self.scheduler.step() #\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u304bepoch\u5358\u4f4d\u304bbatch\u5358\u4f4d\u304b\u306b\u6ce8\u610f one_epoch_loss = running_loss / dataset_sizes['train'] one_epoch_acc = running_corrects.double() / dataset_sizes['train'] return one_epoch_loss, one_epoch_acc def validate_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) return loss, preds, labels def validate_one_epoch(self): self.eval() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.valid_loader: loss, preds, labels = self.validate_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) one_epoch_loss = running_loss / dataset_sizes['val'] one_epoch_acc = running_corrects.double()/ dataset_sizes['val'] return one_epoch_loss, one_epoch_acc def predict_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds_one_batch = torch.max(outputs, 1) return preds_one_batch def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle=True, num_workers= self.num_workers ) preds_list = [] for inputs, labels in self.test_loader: preds_one_batch = self.predict_one_batch(inputs, labels) preds_list.append(preds_one_batch.to('cpu').numpy()) preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler is not None: sch_state_dict = self.scheduler.state_dict() else: sch_state_dict = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler\"] = sch_state_dict model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 torch.save(model_dict, model_path) def load(self, model_path, device=\"cuda\"): self.device = device if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, train_dataset, valid_dataset= None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, fp16 = fp16 ) self._init_wandb() tk0 = tqdm(range(epochs), position = 0, leave = True) for epoch in enumerate(tk0, 1): train_loss, train_acc = self.train_one_epoch() if valid_dataset: valid_loss, valid_acc = self.validate_one_epoch() #writer.add_scalar(\"Loss/train\", 1.0, epoch) wandb.log({ 'epoch' : epoch, \"train_acc\" : train_acc, \"valid_acc\" : valid_acc, \"loss\": train_loss, }) tk0.set_postfix(train_acc = train_acc.item(), valid_acc = valid_acc.item()) tk0.close() wandb.finish() ver3 \u00b6 class CFG: project_name = 'SETI_test2' model_name = 'efficientnetv2_rw_s' note = '2nd' batch_size= 32 n_fold= 4 num_workers =4 image_size =224 epochs = 5 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count from tqdm import tqdm class BasicNN(nn.Module): def init (self, model_name, pretrained_path): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 self.model = timm.create_model(model_name, pretrained = False, in_chans=3) self.model.load_state_dict(torch.load(pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, 1) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, fp16, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if self.multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 self.train_metric_val = None self.valid_metric_val = None if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'model_name' : cfg.model_name, 'batch_size' : cfg.batch_size, 'n_fold' : cfg.n_fold, 'num_workers' : cfg.num_workers, 'image_size' : cfg.image_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, x, targets = None): x = self.conv1(x) outputs = self.model(x) if targets is not None: loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1)) return outputs, loss return outputs, None def train_one_batch(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): d1 = datetime.datetime.now() preds_one_batch, loss = self.train_one_batch(inputs, labels) running_loss.update(loss.item(), data_loader.batch_size) # wandb.log({ # \"train_loss\": running_loss.avg, # }) d2 = datetime.datetime.now() tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", one_step_time = d2-d1) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_step(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs, loss = self(inputs, labels) return outputs, loss def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, metrics = self.valid_metric_val, stage=\"validation\") wandb.log({ \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) self.valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return self.valid_metric_val, running_loss.avg def predict_one_step(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) with torch.no_grad(): outputs, _ = self(inputs, labels) return outputs def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= self.num_workers ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001 ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, fp16 = fp16 ) # self._init_wandb(cfg) if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) # wandb.log({ # \"epoch\" : epoch, # \"epch_train_loss\" : train_loss, # \"epoch_valid_loss\" : valid_loss, # \"epoch_valid_score\" : valid_score, # }) wandb.finish() ver4 \u00b6 !pip install wandb import os import sys import random from tqdm import tqdm import datetime import psutil import pandas as pd import numpy as np from sklearn import metrics from sklearn.model_selection import StratifiedKFold import torch import torch.nn as nn import torchvision import cv2 from PIL import Image import albumentations as A import wandb import warnings warnings.filterwarnings(\"ignore\") class ClassificationDataset(): def init (self, image_paths, targets, transform = None): self.image_paths = image_paths self.targets = targets self.transform = None 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __len__(self): return len(self.image_paths) def __getitem__(self, item): targets = self.targets[item] #image1 = np.load(self.image_paths[item]).astype(float) image1 = np.load(self.image_paths[item])[::2].astype(np.float32) image = np.vstack(image1).transpose((1, 0)) image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True)) image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True)) image = image.astype(np.float32)[np.newaxis, ] # image = np.load(self.image_paths[item]).astype(np.float32) # image = np.vstack(image).transpose((1, 0)) # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC) # image = image[np.newaxis, :, :] if self.transform: image = self.transform(image=image)[\"image\"] return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count class CFG: project_name = 'SETI_test2' pretrained_model_name = 'efficientnet_b0' pretrained = True prettained_path = '../input/timm_weight/efficientnet_b0_ra-3dd342df.pth' input_channels = 3 out_dim = 1 wandb_note = '' colab_or_kaggle = 'colab' wandb_exp_name = f'{pretrained_model_name} {colab_or_kaggle} {wandb_note}' batch_size= 32 epochs = 5 num_of_fold = 5 seed = 42 patience = 3 delta = 0.002 num_workers = 8 fp16 = True checkpoint_path = '' patience_mode = 'max' patience = 3 delta = 0.002 mixup_alpha = 1.0 train_aug = A.Compose( [ A.Resize(p = 1, height = 512, width = 512), #A.Transpose(p=0.5), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.ShiftScaleRotate(p=0.5, scale_limit=0.02, rotate_limit=10, border_mode = cv2.BORDER_REPLICATE), A.MotionBlur(p=0.5), # Horizontal, Verical, shiftscale rotate, one of (very small Blur, gaussian blur, median blur, motionblur), (\u5225\u67a0gassian noise\uff09, contrast, ] ) df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv') df['img_path'] = df['id'].apply( lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy' ) X = df.img_path.values Y = df.target.values skf = StratifiedKFold(n_splits = CFG.num_of_fold) class BasicNN(nn.Module): def init (self): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 self.model = timm.create_model(CFG.pretrained_model_name, pretrained = CFG.pretrained, in_chans = CFG.input_channels) if not CFG.pretrained: self.model.load_state_dict(torch.load(CFG.pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, CFG.out_dim) #self.fc = ppe.nn.LazyLinear(None, CFG.out_dim) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, num_workers, fp16, multiple_GPU, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if num_workers == -1: num_workers = psutil.cpu_count() self.multiple_GPU = multiple_GPU if multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= num_workers, drop_last = True, pin_memory = True ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = num_workers, drop_last = False, pin_memory = True ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'batch_size' : cfg.batch_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.wandb_exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def mixup_data(self, inputs, targets, alpha=1.0): if alpha > 0: lam = np.random.beta(alpha, alpha) else: lam = 1 batch_size = inputs.size()[0] index = torch.randperm(batch_size) mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :] targets_a, targets_b = targets, targets[index] return mixed_inputs, targets_a, targets_b, lam def mixup_criterion(self, criterion, outputs, targets_a, targets_b, lam): return lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b) def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, image, targets): image, targets_a, targets_b, lam = self.mixup_data(image, targets, alpha= CFG.mixup_alpha) image = self.conv1(image) outputs = self.model(image) if targets is not None: #loss = self.criterion(outputs, targets.view(-1, 1)) loss = self.mixup_criterion(self.criterion, outputs, targets_a.view(-1, 1), targets_b.view(-1, 1), lam) return outputs, loss return outputs, None def train_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, targets) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs, targets) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def validate_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) with torch.no_grad(): outputs, loss = self(inputs, targets) return outputs, loss def predict_one_step(self, inputs, targets): outputs, _ = validate_one_step(inputs, targets) return outputs def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.train_one_step(inputs, targets) running_loss.update(loss.item(), data_loader.batch_size) current_lr = self.optimizer.param_groups[0]['lr'] wandb.log({ \"train_step\" : batch_idx, \"train_loss\": running_loss.avg, \"lr\": current_lr }) tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", lr = current_lr) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, stage=\"validation\") wandb.log({ \"validate_step\" : batch_idx, \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return valid_metric_val, running_loss.avg def predict( self, dataset, batch_size = 16, num_workers = 8, ): self.eval() self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= num_workers, drop_last = False, pin_memory = True ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, num_workers = 8, fp16 = True, multiple_GPU = False, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001, ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, num_workers = num_workers, fp16 = fp16, multiple_GPU = multiple_GPU ) self._init_wandb(cfg) torch.backends.cudnn.benchmark = True if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping and save at the check points. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(CFG.checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) print(f'epoch: {epoch}, epoch_valid_score : {valid_score}') wandb.log({ \"epoch\" : epoch, \"epch_train_loss\" : train_loss, \"epoch_valid_loss\" : valid_loss, \"epoch_valid_score\" : valid_score, }) wandb.finish() for fold_cnt, (train_index, test_index) in enumerate(skf.split(X, Y), 1): train_images, valid_images = X[train_index], X[test_index] train_targets, valid_targets = Y[train_index], Y[test_index] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 train_dataset = ClassificationDataset( image_paths=train_images, targets=train_targets, transform = None ) valid_dataset = ClassificationDataset( image_paths=valid_images, targets=valid_targets, transform = None ) model = BasicNN() model.fit( cfg = CFG, train_dataset = train_dataset, valid_dataset = valid_dataset, valid_targets = valid_targets, epochs = CFG.epochs, train_batchsize = CFG.batch_size, valid_batchsize = CFG.batch_size, num_workers = CFG.num_workers, fp16 = CFG.fp16, checkpoint_save_path = CFG.checkpoint_path, mode = CFG.patience_mode, patience = CFG.patience, delta = CFG.delta )","title":"Pytorch basis"},{"location":"pytorch_basis/#kwargs-key1-1-key2-2-key3-3","text":"","title":"kwargs:  {'key1': 1, 'key2': 2, 'key3': 3}"},{"location":"pytorch_basis/#type","text":"def func_kwargs_positional(arg1, arg2, **kwargs): print('arg1: ', arg1) print('arg2: ', arg2) print('kwargs: ', kwargs) func_kwargs_positional(0, 1, key1=1)","title":"type:  "},{"location":"pytorch_basis/#arg1-0","text":"","title":"arg1:  0"},{"location":"pytorch_basis/#arg2-1","text":"","title":"arg2:  1"},{"location":"pytorch_basis/#kwargs-key1-1","text":"","title":"kwargs:  {'key1': 1}"},{"location":"pytorch_basis/#summary","text":"1 2 3 4 5 from torchvision import models from torchsummary import summary vgg = models . vgg16 () summary ( vgg , ( 3 , 224 , 224 )) \u3053\u308c\u306f\u5b9f\u306f\uff0cCrossEntropyLoss\u306fcall\u3067forward\u3092\u547c\u3076\u3088\u3046\u306b\u306a\u3063\u3066\u304a\u308a\uff0c\u3064\u307e\u308a\uff0c loss = criterion(outputs, labels) loss = criterion.forward(outputs, labels) \u3053\u306e\u4e8c\u3064\u306f\u540c\u3058\u3053\u3068\u3092\u3057\u3066\u3044\u307e\u3059\uff0e \u306a\u306e\u3067loss = criterion(outputs, labels)\u304cforward\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0e x = torch.autograd.Variable(torch.Tensor([3,4]), requires_grad=True)","title":"Summary\u306e\u51fa\u3057\u65b9"},{"location":"pytorch_basis/#requires_gradtruevariable","text":"print(\"x.grad : \", x.grad)","title":"requires_grad=True\u3067\uff0c\u3053\u306eVariable\u306f\u5fae\u5206\u3059\u308b\u305e\u3068\u4f1d\u3048\u308b"},{"location":"pytorch_basis/#none","text":"","title":"None"},{"location":"pytorch_basis/#_1","text":"","title":"\u3053\u306e\u6642\u70b9\u3067\u306f\u307e\u3060\u4f55\u3082\u5165\u3063\u3066\u3044\u306a\u3044\uff0e"},{"location":"pytorch_basis/#_2","text":"y = x[0] 2 + 5 x[1] + x[0] x[1]","title":"\u9069\u5f53\u306b\u76ee\u7684\u95a2\u6570\u3092\u4f5c\u308b\uff0e"},{"location":"pytorch_basis/#x0-2x0-x1","text":"","title":"x[0]\u306e\u5c0e\u95a2\u6570 : 2*x[0] + x[1]"},{"location":"pytorch_basis/#x0-23-4-10","text":"","title":"x[0]\u306e\u5fae\u5206\u4fc2\u6570 : 2*3 + 4 = 10"},{"location":"pytorch_basis/#x1-5-x0","text":"","title":"x[1]\u306e\u5c0e\u95a2\u6570 : 5 + x[0]"},{"location":"pytorch_basis/#x1-5-3-8","text":"y.backward()","title":"x[1]\u306e\u5fae\u5206\u4fc2\u6570 : 5 + 3 = 8"},{"location":"pytorch_basis/#torchautogradbackwardy","text":"print(\"x.grad : \", x.grad)","title":"torch.autograd.backward(y)\u3000\u3067\u3082\u826f\u3044\uff0e"},{"location":"pytorch_basis/#10","text":"","title":"10"},{"location":"pytorch_basis/#8","text":"","title":"8"},{"location":"pytorch_basis/#zero_grad","text":"x.grad = None for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # zero the parameter gradients optimizer.zero_grad() #\u52fe\u914d\u306e\u521d\u671f\u5316 # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) #\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u5c64\u306e\uff08\u30d0\u30c3\u30c1\u6570, \u6b21\u5143(ex \u30af\u30e9\u30b9\u6570)\uff09\u304c\u51fa\u529b\u3055\u308c\u308b print(f'outputs: {outputs}') _, preds = torch.max(outputs, 1) #\u6700\u5927\u3068\u306a\u308bindex\u3092\u8fd4\u3059 print(f'preds: {preds}, labels: {labels}') loss = criterion(outputs, labels) #\u640d\u5931\u5024\u3092\u51fa\u3059 print(f'loss: {loss}') # backward + optimize only if in training phase if phase == 'train': loss.backward()\u3000#\u5c0e\u95a2\u6570\u306e\u7d50\u679c\u304c\u7d2f\u7a4d optimizer.step()\u3000#parameter\u306e\u66f4\u65b0 date_info = {'year': \"2020\", 'month': \"01\", 'day': \"01\"} filename = \"{year}-{month}-{day}.txt\".format(**date_info) filename '2020-01-01.txt' scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch) for epoch in range(0, 100): #\u3053\u3053\u306f\u4ee5\u4e0b\u7701\u7565 scheduler.step() os.cpu_count() psutill.cpu_count AMP https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587 **data\u306f\u8f9e\u66f8\u3092\u53d7\u3051\u53d6\u308b self\u3067\u81ea\u5206\u81ea\u8eab\u3064\u307e\u308aforward\u304c\u547c\u3070\u308c\u308b callback\u95a2\u6570\u306f\u975e\u540c\u671f\u3063\u307d\u3044\u3082\u306e \u640d\u5931\u95a2\u6570 https://yoshinashigoto-blog.herokuapp.com/detail/27/ from torchvision import models model = models.mnasnet0_5() torch.save(model.to('cpu').state_dict(), 'model.pth') from torchvision import models model = models.mnasnet0_5() model.load_state_dict(torch.load('model.pth'))","title":".zero_grad()\u306e\u4ee3\u308f\u308a"},{"location":"pytorch_basis/#_3","text":"epoch = 10","title":"\u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b"},{"location":"pytorch_basis/#_4","text":"torch.save( { \"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), }, \"model.tar\", )","title":"\u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u4fdd\u5b58\u3059\u308b\u3002"},{"location":"pytorch_basis/#_5","text":"checkpoint = torch.load(\"model.tar\") model.load_state_dict(checkpoint[\"model_state_dict\"]) optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) epoch = checkpoint[\"epoch\"] 01.\u3000\u640d\u5931\u95a2\u6570\u3068\u306f \u307e\u305a\u640d\u5931\u95a2\u6570\u3068\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\u304c\u3046\u307e\u304f\u884c\u3063\u305f\u306e\u304b\u3069\u3046\u304b\u5224\u65ad\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3059\u308b\u95a2\u6570\u3067\u3059\u3002 \u3053\u306e\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u4e88\u6e2c\u3068\u7b54\u3048\u306e\u8aa4\u5dee\u3092\u6c42\u3081\u307e\u3059\u3002 \u305d\u306e\u8aa4\u5dee\u304c\u6700\u5c0f\u306b\u306a\u308c\u3070\u4e88\u6e2c\u306f\u3088\u308a\u6b63\u78ba\u306a\u3082\u306e\u3060\u3063\u305f\u3068\u3044\u3046\u8a55\u4fa1\u304c\u306a\u3055\u308c\u307e\u3059\u3002 \u640d\u5931\u95a2\u6570\u306b\u306f\u4e0b\u3067\u89e6\u308c\u308b\u3060\u3051\u306e\u7a2e\u985e\u304c\u3042\u308a\u3001\u76ee\u7684\u306b\u3088\u3063\u3066\u4f7f\u3044\u5206\u3051\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u95a2\u6570\u3092\u7528\u3044\u3066\u6570\u5b66\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3059\u308b\u3053\u3068\u3067\u6a5f\u68b0\u5b66\u7fd2\u306e\u4e88\u6e2c\u306e\u6b63\u78ba\u6027\u3092\u9ad8\u3081\u3066\u3044\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u3067\u306f\u6570\u5b66\u7684\u306a\u8981\u7d20\u306b\u8e0f\u307f\u8fbc\u307f\u3059\u304e\u306a\u3044\u7a0b\u5ea6\u306b\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3067\u306e\u6d3b\u7528\u65b9\u6cd5\u3092\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u3057\u3066\u3044\u304d\u307e\u3059\u3002 \u3061\u306a\u307f\u306b\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3068\u306f\u4e0d\u898f\u5247\u6027\u306e\u7a0b\u5ea6\u3092\u8868\u3059\u91cf\u3092\u3044\u3044\u307e\u3059\u3002 \u305d\u306e\u901a\u308a\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u306d\u3002 _02.\u3000\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u304c2\u30af\u30e9\u30b9\u306e\u5834\u5408\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u3068\u3044\u3046\u306e\u306f\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u304c2\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u4e00\u7a2e\u306e\u8ddd\u96e2\u3092\u8868\u3059\u3088\u3046\u306a\u6307\u6a19\u3067\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u3068\u306e\u9593\u306b\u3069\u306e\u7a0b\u5ea6\u306e\u5dee\u304c\u3042\u308b\u306e\u304b\u3092\u793a\u3059\u5c3a\u5ea6\u3067\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931L(y,t)\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u78ba\u7387yi\u3068\u6b63\u89e3j\u30af\u30e9\u30b9ti\u3092\u8868\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u5024yi\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u5c64\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5024\u3092\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u5909\u63db\u3057\u305f\u78ba\u7387\u5024\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u51fa\u529b\u5c64\u304b\u3089\u306e\u51fa\u529b\u5024\u3092\u30ed\u30b8\u30c3\u30c8\u3068\u3044\u3044\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u3068\u306f\u3042\u308b\u3042\u308b\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3068\u305d\u3046\u3067\u306a\u3044\u78ba\u73871-pn\u306e\u6bd4\u306b\u5bfe\u6570\u3092\u3068\u3063\u305f\u5024\u3067\u3059\u3002 \u5148\u306b\u51fa\u3066\u304d\u305f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306f\u30ed\u30b8\u30c3\u30c8\u95a2\u6570\u306e\u9006\u95a2\u6570\u3067\u3059\u3002 \u305d\u306e\u305f\u3081\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u30ed\u30b8\u30c3\u30c8\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u3067\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u8981\u306f\u51fa\u529b\u5024\u30920\u304b\u30891\u306e\u7bc4\u56f2\u306b\u6291\u3048\u3064\u3064\u6271\u3044\u3084\u3059\u3044\u78ba\u7387\u306e\u5f62\u306b\u5909\u63db\u3067\u304d\u308b\u516c\u5f0f\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCELoss()\u3067\u3059\u3002 \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306fnn.Sigmoid()\u3067\u3059\u3002 \u306a\u304a\u3001nn.BCELoss\u306ftorch.float32\u578b\u3092\u30c7\u30fc\u30bf\u578b\u3068\u3057\u3066\u4f7f\u7528\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u305d\u306e\u305f\u3081\u6b63\u89e3\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u578b\u306f\u672c\u6765int\u3067\u3059\u304cfloat\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 import torch from torch import nn m = nn.Sigmoid() y = torch.rand(3) t = torch.empty(3, dtype=torch.float32).random_(2) criterion = nn.BCELoss() loss = criterion(m(y), t) print(\"y: {}\".format(y)) print(\"m(y): {}\".format(m(y))) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss))","title":"\u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u8aad\u307f\u8fbc\u3080\u3002"},{"location":"pytorch_basis/#_6","text":"y: tensor([0.2744, 0.9147, 0.3309]) m(y): tensor([0.5682, 0.7140, 0.5820]) t: tensor([0., 1., 0.]) loss: 0.6830 loss\u304c\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 _03.\u3000\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306b\u6700\u521d\u304b\u3089\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u304c\u52a0\u3048\u3089\u308c\u305f\u3082\u306e\u3067\u3059\u3002 \u3059\u306a\u308f\u3061\u51fa\u529b\u5024\u3092\u305d\u306e\u307e\u307e\u4e0e\u3048\u308c\u3070\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u304c\u5f97\u3089\u308c\u307e\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u3001\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u306e\u30af\u30e9\u30b9ti\u3092L(y, t)\u3068\u3057\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCEWithLogitsLoss()\u3067\u3059\u3002 \u9577\u3044\u3067\u3059\u306d\u3002 import torch from torch import nn y = torch.rand(3) t = torch.empty(3, dype=torch.float32).random_(2) criterion = nn.BCEWithLogitsLoss() loss = criterion(y, t) print(\"y: {}\".format(y)) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss))","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"pytorch_basis/#_7","text":"y: tensor([0.9709, 0.8976, 0.3228]) t: tensor([0., 1., 0.]) loss: 0.8338 loss\u304c\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 .format()\u3067\u306f\u6307\u5b9a\u3057\u305f\u5909\u6570\u3092{}\u306e\u4e2d\u306b\u4ee3\u5165\u3057\u3066\u305d\u308c\u3092\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002 _04.\u3000\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3082\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u30af\u30e9\u30b9\u304c\u3069\u306e\u304f\u3089\u3044\u96e2\u308c\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u5c3a\u5ea6\u3067\u3059\u3002 \u7279\u306b2\u30af\u30e9\u30b9\u4ee5\u4e0a\u306e\u591a\u30af\u30e9\u30b9\u306b\u5206\u985e\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u306e\u5206\u985e\u3067\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u304c\u30012\u30af\u30e9\u30b9\u4ee5\u4e0a\u3067\u306f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fn\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b9k\u306e\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u30af\u30e9\u30b9ti\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066L(y, t)\u3067\u8868\u3059\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fnn.CrossEntropyLoss\u3067\u3059\u3002 import torch from torch import nn y = torch.rand(3, 5) t = torch.empty(3, dtype=torch.int64).random_(5) criterion = nn.CrossEntropyLoss() loss = criterion(y, t) print(\"y:{}\".format(y)) print(\"t:{}\".format(t)) print(\"loss: {:4f}\".format(loss))","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"pytorch_basis/#_8","text":"y: tensor([[0.7775, 0.7587, 0.9474, 0.5149, 0.7741], [0.5059, 0.4802, 0.9846, 0.6292, 0.0167], [0.4339, 0.6873, 0.4253, 0.7067, 0.5678]]) t: tensor([1, 4, 1]) loss: 1.757074 \u30c7\u30fc\u30bf\u6570\u306f3\u3064\u3067\u5404\u30af\u30e9\u30b9\u306b\u51fa\u529b\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u6570\u306f5\u3064\u3067\u3059\u3002 loss\u304c\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 torch.cuda.amp.autocast():\u306e\u6b63\u3057\u3044indent to('cpu').numpy() cpu().detach().numpy() \u9055\u3044","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"pytorch_basis/#ver1","text":"def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class CFG: project_name = 'sample2' model_name = 'resnet18' note = '2nd' batch_size= 4 n_fold= 4 num_workers =4 image_size =224 epochs = 25 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) from tqdm import tqdm class BasicNN(nn.Module): def init (self, cfg): super(). init () self.cfg = cfg self.model = models.resnet18(pretrained=True) self.model.fc = nn.Linear(self.model.fc.in_features, 2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 self.model = self.model.to('cuda') self.current_epoch = 0 self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler = None self.metrics = None self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, fp16, ): self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.loss() if not self.optimizer: self.optimizer = self.fetch_optimizer() if not self.scheduler: self.scheduler = self.fetch_scheduler() def _init_wandb(self): hyperparams = { 'model_name' : self.cfg.model_name, 'batch_size' : self.cfg.batch_size, 'n_fold' : self.cfg.n_fold, 'num_workers' : self.cfg.num_workers, 'image_size' : self.cfg.image_size, 'epochs' : self.cfg.epochs } wandb.init( config = hyperparams, project= self.cfg.project_name, name=self.cfg.exp_name, ) wandb.watch(self) def loss(self): loss = nn.CrossEntropyLoss() return loss def fetch_optimizer(self): #opt = torch.optim.Adam(self.parameters(), lr=5e-4) opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def fetch_scheduler(self): # sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( # self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 # ) sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def monitor_metrics(self, *args, **kwargs): self.metrics = None return def forward(self, x): return self.model(x) def model_fn(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') return self(inputs) def train_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) loss.backward() self.optimizer.step() return loss, preds, labels def train_one_epoch(self): self.train() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.train_loader: loss, preds, labels = self.train_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) self.scheduler.step() #\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u304bepoch\u5358\u4f4d\u304bbatch\u5358\u4f4d\u304b\u306b\u6ce8\u610f one_epoch_loss = running_loss / dataset_sizes['train'] one_epoch_acc = running_corrects.double() / dataset_sizes['train'] return one_epoch_loss, one_epoch_acc def validate_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) return loss, preds, labels def validate_one_epoch(self): self.eval() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.valid_loader: loss, preds, labels = self.validate_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) one_epoch_loss = running_loss / dataset_sizes['val'] one_epoch_acc = running_corrects.double()/ dataset_sizes['val'] return one_epoch_loss, one_epoch_acc def predict_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds_one_batch = torch.max(outputs, 1) return preds_one_batch def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle=True, num_workers= self.num_workers ) preds_list = [] for inputs, labels in self.test_loader: preds_one_batch = self.predict_one_batch(inputs, labels) preds_list.append(preds_one_batch.to('cpu').numpy()) preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler is not None: sch_state_dict = self.scheduler.state_dict() else: sch_state_dict = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler\"] = sch_state_dict model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 torch.save(model_dict, model_path) def load(self, model_path, device=\"cuda\"): self.device = device if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, train_dataset, valid_dataset= None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, fp16 = fp16 ) self._init_wandb() tk0 = tqdm(range(epochs), position = 0, leave = True) for epoch in enumerate(tk0, 1): train_loss, train_acc = self.train_one_epoch() if valid_dataset: valid_loss, valid_acc = self.validate_one_epoch() #writer.add_scalar(\"Loss/train\", 1.0, epoch) wandb.log({ 'epoch' : epoch, \"train_acc\" : train_acc, \"valid_acc\" : valid_acc, \"loss\": train_loss, }) tk0.set_postfix(train_acc = train_acc.item(), valid_acc = valid_acc.item()) tk0.close() wandb.finish()","title":"ver1"},{"location":"pytorch_basis/#ver3","text":"class CFG: project_name = 'SETI_test2' model_name = 'efficientnetv2_rw_s' note = '2nd' batch_size= 32 n_fold= 4 num_workers =4 image_size =224 epochs = 5 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count from tqdm import tqdm class BasicNN(nn.Module): def init (self, model_name, pretrained_path): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 self.model = timm.create_model(model_name, pretrained = False, in_chans=3) self.model.load_state_dict(torch.load(pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, 1) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, fp16, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if self.multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 self.train_metric_val = None self.valid_metric_val = None if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'model_name' : cfg.model_name, 'batch_size' : cfg.batch_size, 'n_fold' : cfg.n_fold, 'num_workers' : cfg.num_workers, 'image_size' : cfg.image_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, x, targets = None): x = self.conv1(x) outputs = self.model(x) if targets is not None: loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1)) return outputs, loss return outputs, None def train_one_batch(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): d1 = datetime.datetime.now() preds_one_batch, loss = self.train_one_batch(inputs, labels) running_loss.update(loss.item(), data_loader.batch_size) # wandb.log({ # \"train_loss\": running_loss.avg, # }) d2 = datetime.datetime.now() tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", one_step_time = d2-d1) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_step(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs, loss = self(inputs, labels) return outputs, loss def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, metrics = self.valid_metric_val, stage=\"validation\") wandb.log({ \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) self.valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return self.valid_metric_val, running_loss.avg def predict_one_step(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) with torch.no_grad(): outputs, _ = self(inputs, labels) return outputs def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= self.num_workers ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001 ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, fp16 = fp16 ) # self._init_wandb(cfg) if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) # wandb.log({ # \"epoch\" : epoch, # \"epch_train_loss\" : train_loss, # \"epoch_valid_loss\" : valid_loss, # \"epoch_valid_score\" : valid_score, # }) wandb.finish()","title":"ver3"},{"location":"pytorch_basis/#ver4","text":"!pip install wandb import os import sys import random from tqdm import tqdm import datetime import psutil import pandas as pd import numpy as np from sklearn import metrics from sklearn.model_selection import StratifiedKFold import torch import torch.nn as nn import torchvision import cv2 from PIL import Image import albumentations as A import wandb import warnings warnings.filterwarnings(\"ignore\") class ClassificationDataset(): def init (self, image_paths, targets, transform = None): self.image_paths = image_paths self.targets = targets self.transform = None 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __len__(self): return len(self.image_paths) def __getitem__(self, item): targets = self.targets[item] #image1 = np.load(self.image_paths[item]).astype(float) image1 = np.load(self.image_paths[item])[::2].astype(np.float32) image = np.vstack(image1).transpose((1, 0)) image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True)) image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True)) image = image.astype(np.float32)[np.newaxis, ] # image = np.load(self.image_paths[item]).astype(np.float32) # image = np.vstack(image).transpose((1, 0)) # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC) # image = image[np.newaxis, :, :] if self.transform: image = self.transform(image=image)[\"image\"] return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count class CFG: project_name = 'SETI_test2' pretrained_model_name = 'efficientnet_b0' pretrained = True prettained_path = '../input/timm_weight/efficientnet_b0_ra-3dd342df.pth' input_channels = 3 out_dim = 1 wandb_note = '' colab_or_kaggle = 'colab' wandb_exp_name = f'{pretrained_model_name} {colab_or_kaggle} {wandb_note}' batch_size= 32 epochs = 5 num_of_fold = 5 seed = 42 patience = 3 delta = 0.002 num_workers = 8 fp16 = True checkpoint_path = '' patience_mode = 'max' patience = 3 delta = 0.002 mixup_alpha = 1.0 train_aug = A.Compose( [ A.Resize(p = 1, height = 512, width = 512), #A.Transpose(p=0.5), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.ShiftScaleRotate(p=0.5, scale_limit=0.02, rotate_limit=10, border_mode = cv2.BORDER_REPLICATE), A.MotionBlur(p=0.5), # Horizontal, Verical, shiftscale rotate, one of (very small Blur, gaussian blur, median blur, motionblur), (\u5225\u67a0gassian noise\uff09, contrast, ] ) df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv') df['img_path'] = df['id'].apply( lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy' ) X = df.img_path.values Y = df.target.values skf = StratifiedKFold(n_splits = CFG.num_of_fold) class BasicNN(nn.Module): def init (self): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 self.model = timm.create_model(CFG.pretrained_model_name, pretrained = CFG.pretrained, in_chans = CFG.input_channels) if not CFG.pretrained: self.model.load_state_dict(torch.load(CFG.pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, CFG.out_dim) #self.fc = ppe.nn.LazyLinear(None, CFG.out_dim) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, num_workers, fp16, multiple_GPU, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if num_workers == -1: num_workers = psutil.cpu_count() self.multiple_GPU = multiple_GPU if multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= num_workers, drop_last = True, pin_memory = True ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = num_workers, drop_last = False, pin_memory = True ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'batch_size' : cfg.batch_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.wandb_exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def mixup_data(self, inputs, targets, alpha=1.0): if alpha > 0: lam = np.random.beta(alpha, alpha) else: lam = 1 batch_size = inputs.size()[0] index = torch.randperm(batch_size) mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :] targets_a, targets_b = targets, targets[index] return mixed_inputs, targets_a, targets_b, lam def mixup_criterion(self, criterion, outputs, targets_a, targets_b, lam): return lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b) def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, image, targets): image, targets_a, targets_b, lam = self.mixup_data(image, targets, alpha= CFG.mixup_alpha) image = self.conv1(image) outputs = self.model(image) if targets is not None: #loss = self.criterion(outputs, targets.view(-1, 1)) loss = self.mixup_criterion(self.criterion, outputs, targets_a.view(-1, 1), targets_b.view(-1, 1), lam) return outputs, loss return outputs, None def train_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, targets) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs, targets) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def validate_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) with torch.no_grad(): outputs, loss = self(inputs, targets) return outputs, loss def predict_one_step(self, inputs, targets): outputs, _ = validate_one_step(inputs, targets) return outputs def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.train_one_step(inputs, targets) running_loss.update(loss.item(), data_loader.batch_size) current_lr = self.optimizer.param_groups[0]['lr'] wandb.log({ \"train_step\" : batch_idx, \"train_loss\": running_loss.avg, \"lr\": current_lr }) tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", lr = current_lr) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, stage=\"validation\") wandb.log({ \"validate_step\" : batch_idx, \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return valid_metric_val, running_loss.avg def predict( self, dataset, batch_size = 16, num_workers = 8, ): self.eval() self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= num_workers, drop_last = False, pin_memory = True ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, num_workers = 8, fp16 = True, multiple_GPU = False, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001, ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, num_workers = num_workers, fp16 = fp16, multiple_GPU = multiple_GPU ) self._init_wandb(cfg) torch.backends.cudnn.benchmark = True if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping and save at the check points. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(CFG.checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) print(f'epoch: {epoch}, epoch_valid_score : {valid_score}') wandb.log({ \"epoch\" : epoch, \"epch_train_loss\" : train_loss, \"epoch_valid_loss\" : valid_loss, \"epoch_valid_score\" : valid_score, }) wandb.finish() for fold_cnt, (train_index, test_index) in enumerate(skf.split(X, Y), 1): train_images, valid_images = X[train_index], X[test_index] train_targets, valid_targets = Y[train_index], Y[test_index] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 train_dataset = ClassificationDataset( image_paths=train_images, targets=train_targets, transform = None ) valid_dataset = ClassificationDataset( image_paths=valid_images, targets=valid_targets, transform = None ) model = BasicNN() model.fit( cfg = CFG, train_dataset = train_dataset, valid_dataset = valid_dataset, valid_targets = valid_targets, epochs = CFG.epochs, train_batchsize = CFG.batch_size, valid_batchsize = CFG.batch_size, num_workers = CFG.num_workers, fp16 = CFG.fp16, checkpoint_save_path = CFG.checkpoint_path, mode = CFG.patience_mode, patience = CFG.patience, delta = CFG.delta )","title":"ver4"}]}