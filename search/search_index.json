{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"al_ds/","text":"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u30c7\u30fc\u30bf\u69cb\u9020 \u00b6 https://github.com/TheAlgorithms \u5178\u578b 90 \u554f \u3042\u308a\u672c\u3001\u3051\u3093\u3061\u3087\u3093\u672c\u306a\u3069, EPI, \u4e16\u754c\u3067\u6226\u3046, \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u30af\u30a4\u30c3\u30af\u30fb\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9, \u307f\u3093\u306a\u306e\u30c7\u30fc\u30bf\u69cb\u9020 Hashmap \u00b6 two sum \u00b6 1 2 3 4 5 6 7 8 class Solution : def twoSum ( self , nums : List [ int ], target : int ) -> List [ int ]: lookup = {} for i , n in enumerate ( nums ): if target - n in lookup : return [ lookup [ target - n ], i ] else : lookup [ n ] = i 1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public : vector < int > twoSum ( vector < int >& nums , int target ) { std :: unordered_map < int , int > lookup ; for ( int i = 0 ; i < nums . size (); ++ i ) { if ( lookup . count ( target - nums [ i ])) { return { lookup [ target - nums [ i ]], i }; } lookup [ nums [ i ]] = i ; } return { -1 , -1 }; } }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 use std :: collections :: HashMap ; impl Solution { pub fn two_sum ( nums : Vec < i32 > , target : i32 ) -> Vec < i32 > { let mut lookup = HashMap :: with_capacity ( nums . len ()); for ( idx , & n ) in nums . iter (). enumerate () { let y = target - n ; if let Some ( & i ) = lookup . get ( & y ) { return vec! [ i as i32 , idx as i32 ]; } else { lookup . insert ( n , idx ); } } vec! [] } } use std :: collections :: HashMap ; impl Solution { pub fn two_sum ( nums : Vec < i32 > , target : i32 ) -> Vec < i32 > { let mut lookup : HashMap < i32 , i32 > = HashMap :: new (); for i in 0 .. nums . len () { match lookup . get ( & nums [ i ]) { Some ( & x ) => return vec! [ x , i as i32 ], None => lookup . insert ( target - nums [ i ], i as i32 ), }; } return vec! [ - 1 , - 1 ]; } } 1 2 3 4 5 6 7 8 9 10 11 func twoSum ( nums [] int , target int ) [] int { lookup := make ( map [ int ] int ) for i , val := range ( nums ){ if ( lookup [ target - val ] != 0 ){ return [] int { lookup [ target - val ] - 1 , i } } lookup [ val ] = i + 1 ; } return [] int {} } 1 2 3 4 5 6 7 8 9 10 11 var twoSum = function ( nums , target ) { let lookup = new Map (); for ( let i = 0 ; i < nums . length ; i ++ ) { if ( lookup . has ( target - nums [ i ])) { return [ lookup . get ( target - nums [ i ]), i ]; } lookup . set ( nums [ i ], i ); } return []; }; Flood Fill \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public : void dfs ( vector < vector < int >>& image , int i , int j , int val , int newColor ) { if ( i < 0 || i >= image . size () || j < 0 || j >= image [ 0 ]. size () || image [ i ][ j ] == newColor || image [ i ][ j ] != val ) { return ; } image [ i ][ j ] = newColor ; dfs ( image , i -1 , j , val , newColor ); dfs ( image , i + 1 , j , val , newColor ); dfs ( image , i , j -1 , val , newColor ); dfs ( image , i , j + 1 , val , newColor ); } vector < vector < int >> floodFill ( vector < vector < int >>& image , int sr , int sc , int newColor ) { int val = image [ sr ][ sc ]; dfs ( image , sr , sc , val , newColor ); return image ; } }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 impl Solution { pub fn flood_fill ( mut image : Vec < Vec < i32 >> , sr : i32 , sc : i32 , new_color : i32 ) -> Vec < Vec < i32 >> { let old_color = image [ sr as usize ][ sc as usize ]; if old_color != new_color { Self :: fill ( & mut image , sr as usize , sc as usize , old_color , new_color ); } image } fn fill ( image : & mut Vec < Vec < i32 >> , i : usize , j : usize , old_color : i32 , new_color : i32 ) { if image [ i ][ j ] != old_color { return ; } image [ i ][ j ] = new_color ; if i > 0 { Self :: fill ( image , i - 1 , j , old_color , new_color ); } if i < image . len () - 1 { Self :: fill ( image , i + 1 , j , old_color , new_color ); } if j > 0 { Self :: fill ( image , i , j - 1 , old_color , new_color ); } if j < image [ 0 ]. len () - 1 { Self :: fill ( image , i , j + 1 , old_color , new_color ); } } } impl Solution { pub fn flood_fill ( mut image : Vec < Vec < i32 >> , sr : i32 , sc : i32 , new_color : i32 ) -> Vec < Vec < i32 >> { let old_color = image [ sr as usize ][ sc as usize ]; if old_color == new_color { return image ; } let mut stack = vec! [( sr as usize , sc as usize )]; while let Some (( i , j )) = stack . pop () { if image [ i ][ j ] != old_color { continue ; } image [ i ][ j ] = new_color ; if i > 0 { stack . push (( i - 1 , j )); } if i < image . len () - 1 { stack . push (( i + 1 , j )) } if j > 0 { stack . push (( i , j - 1 )); } if j < image [ 0 ]. len () - 1 { stack . push (( i , j + 1 )); } } image } } sliding window \u00b6 prefix sum \u00b6 \u533a\u9593\u548c \u00b6 BFS \u00b6 queue \u00b6 DFS \u00b6 recursion \u00b6 stack \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public : bool isValid ( string s ) { stack < char > st ; for ( auto i : s ) { if ( i == '(' or i == '{' or i == '[' ){ st . push ( i ); } else { if ( st . empty () or ( st . top () == '(' and i != ')' ) or ( st . top () == '{' and i != '}' ) or ( st . top () == '[' and i != ']' )) return false ; st . pop (); } } return st . empty (); } }; 1 2 3 4 5 6 7 8 9 10 11 12 class Solution : def isValid ( self , s : str ) -> bool : brackets = { '{' : '}' , '(' : ')' , '[' : ']' } stack = [] for char in s : if char in brackets : stack . append ( char ) elif not stack or brackets [ stack . pop ()] != char : return False return not stack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 impl Solution { pub fn is_valid ( s : String ) -> bool { let mut stack = Vec :: new (); for i in s . chars () { match i { '{' => stack . push ( '}' ), '(' => stack . push ( ')' ), '[' => stack . push ( ']' ), '}' | ')' | ']' if Some ( i ) != stack . pop () => return false , _ => (), } } return stack . is_empty (); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func isValid ( s string ) bool { if len ( s ) == 0 || len ( s ) % 2 == 1 { return false } pairs := map [ rune ] rune { '(' : ')' , '{' : '}' , '[' : ']' , } stack := [] rune {} for _ , r := range s { if _ , ok := pairs [ r ]; ok { stack = append ( stack , r ) } else if len ( stack ) == 0 || pairs [ stack [ len ( stack ) - 1 ]] != r { return false } else { stack = stack [: len ( stack ) - 1 ] } } return len ( stack ) == 0 } Back tracking \u00b6 recursion \u00b6 stack \u00b6 DP \u00b6 LRU cathe \u00b6 table \u00b6 Greedy \u00b6 Linked List \u00b6 Tree \u00b6 Heap \u00b6 Union Find \u00b6 Topological sort \u00b6 Diijkstra \u00b6 A \u00b6 \u6700\u5c0f\u5168\u57df\u6728 \u00b6 Flow \u00b6","title":"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u30c7\u30fc\u30bf\u69cb\u9020"},{"location":"al_ds/#_1","text":"https://github.com/TheAlgorithms \u5178\u578b 90 \u554f \u3042\u308a\u672c\u3001\u3051\u3093\u3061\u3087\u3093\u672c\u306a\u3069, EPI, \u4e16\u754c\u3067\u6226\u3046, \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u30af\u30a4\u30c3\u30af\u30fb\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9, \u307f\u3093\u306a\u306e\u30c7\u30fc\u30bf\u69cb\u9020","title":"\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3068\u30c7\u30fc\u30bf\u69cb\u9020"},{"location":"al_ds/#hashmap","text":"","title":"Hashmap"},{"location":"al_ds/#two-sum","text":"1 2 3 4 5 6 7 8 class Solution : def twoSum ( self , nums : List [ int ], target : int ) -> List [ int ]: lookup = {} for i , n in enumerate ( nums ): if target - n in lookup : return [ lookup [ target - n ], i ] else : lookup [ n ] = i 1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public : vector < int > twoSum ( vector < int >& nums , int target ) { std :: unordered_map < int , int > lookup ; for ( int i = 0 ; i < nums . size (); ++ i ) { if ( lookup . count ( target - nums [ i ])) { return { lookup [ target - nums [ i ]], i }; } lookup [ nums [ i ]] = i ; } return { -1 , -1 }; } }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 use std :: collections :: HashMap ; impl Solution { pub fn two_sum ( nums : Vec < i32 > , target : i32 ) -> Vec < i32 > { let mut lookup = HashMap :: with_capacity ( nums . len ()); for ( idx , & n ) in nums . iter (). enumerate () { let y = target - n ; if let Some ( & i ) = lookup . get ( & y ) { return vec! [ i as i32 , idx as i32 ]; } else { lookup . insert ( n , idx ); } } vec! [] } } use std :: collections :: HashMap ; impl Solution { pub fn two_sum ( nums : Vec < i32 > , target : i32 ) -> Vec < i32 > { let mut lookup : HashMap < i32 , i32 > = HashMap :: new (); for i in 0 .. nums . len () { match lookup . get ( & nums [ i ]) { Some ( & x ) => return vec! [ x , i as i32 ], None => lookup . insert ( target - nums [ i ], i as i32 ), }; } return vec! [ - 1 , - 1 ]; } } 1 2 3 4 5 6 7 8 9 10 11 func twoSum ( nums [] int , target int ) [] int { lookup := make ( map [ int ] int ) for i , val := range ( nums ){ if ( lookup [ target - val ] != 0 ){ return [] int { lookup [ target - val ] - 1 , i } } lookup [ val ] = i + 1 ; } return [] int {} } 1 2 3 4 5 6 7 8 9 10 11 var twoSum = function ( nums , target ) { let lookup = new Map (); for ( let i = 0 ; i < nums . length ; i ++ ) { if ( lookup . has ( target - nums [ i ])) { return [ lookup . get ( target - nums [ i ]), i ]; } lookup . set ( nums [ i ], i ); } return []; };","title":"two sum"},{"location":"al_ds/#flood-fill","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public : void dfs ( vector < vector < int >>& image , int i , int j , int val , int newColor ) { if ( i < 0 || i >= image . size () || j < 0 || j >= image [ 0 ]. size () || image [ i ][ j ] == newColor || image [ i ][ j ] != val ) { return ; } image [ i ][ j ] = newColor ; dfs ( image , i -1 , j , val , newColor ); dfs ( image , i + 1 , j , val , newColor ); dfs ( image , i , j -1 , val , newColor ); dfs ( image , i , j + 1 , val , newColor ); } vector < vector < int >> floodFill ( vector < vector < int >>& image , int sr , int sc , int newColor ) { int val = image [ sr ][ sc ]; dfs ( image , sr , sc , val , newColor ); return image ; } }; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 impl Solution { pub fn flood_fill ( mut image : Vec < Vec < i32 >> , sr : i32 , sc : i32 , new_color : i32 ) -> Vec < Vec < i32 >> { let old_color = image [ sr as usize ][ sc as usize ]; if old_color != new_color { Self :: fill ( & mut image , sr as usize , sc as usize , old_color , new_color ); } image } fn fill ( image : & mut Vec < Vec < i32 >> , i : usize , j : usize , old_color : i32 , new_color : i32 ) { if image [ i ][ j ] != old_color { return ; } image [ i ][ j ] = new_color ; if i > 0 { Self :: fill ( image , i - 1 , j , old_color , new_color ); } if i < image . len () - 1 { Self :: fill ( image , i + 1 , j , old_color , new_color ); } if j > 0 { Self :: fill ( image , i , j - 1 , old_color , new_color ); } if j < image [ 0 ]. len () - 1 { Self :: fill ( image , i , j + 1 , old_color , new_color ); } } } impl Solution { pub fn flood_fill ( mut image : Vec < Vec < i32 >> , sr : i32 , sc : i32 , new_color : i32 ) -> Vec < Vec < i32 >> { let old_color = image [ sr as usize ][ sc as usize ]; if old_color == new_color { return image ; } let mut stack = vec! [( sr as usize , sc as usize )]; while let Some (( i , j )) = stack . pop () { if image [ i ][ j ] != old_color { continue ; } image [ i ][ j ] = new_color ; if i > 0 { stack . push (( i - 1 , j )); } if i < image . len () - 1 { stack . push (( i + 1 , j )) } if j > 0 { stack . push (( i , j - 1 )); } if j < image [ 0 ]. len () - 1 { stack . push (( i , j + 1 )); } } image } }","title":"Flood Fill"},{"location":"al_ds/#sliding-window","text":"","title":"sliding window"},{"location":"al_ds/#prefix-sum","text":"","title":"prefix sum"},{"location":"al_ds/#_2","text":"","title":"\u533a\u9593\u548c"},{"location":"al_ds/#bfs","text":"","title":"BFS"},{"location":"al_ds/#queue","text":"","title":"queue"},{"location":"al_ds/#dfs","text":"","title":"DFS"},{"location":"al_ds/#recursion","text":"","title":"recursion"},{"location":"al_ds/#stack","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public : bool isValid ( string s ) { stack < char > st ; for ( auto i : s ) { if ( i == '(' or i == '{' or i == '[' ){ st . push ( i ); } else { if ( st . empty () or ( st . top () == '(' and i != ')' ) or ( st . top () == '{' and i != '}' ) or ( st . top () == '[' and i != ']' )) return false ; st . pop (); } } return st . empty (); } }; 1 2 3 4 5 6 7 8 9 10 11 12 class Solution : def isValid ( self , s : str ) -> bool : brackets = { '{' : '}' , '(' : ')' , '[' : ']' } stack = [] for char in s : if char in brackets : stack . append ( char ) elif not stack or brackets [ stack . pop ()] != char : return False return not stack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 impl Solution { pub fn is_valid ( s : String ) -> bool { let mut stack = Vec :: new (); for i in s . chars () { match i { '{' => stack . push ( '}' ), '(' => stack . push ( ')' ), '[' => stack . push ( ']' ), '}' | ')' | ']' if Some ( i ) != stack . pop () => return false , _ => (), } } return stack . is_empty (); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func isValid ( s string ) bool { if len ( s ) == 0 || len ( s ) % 2 == 1 { return false } pairs := map [ rune ] rune { '(' : ')' , '{' : '}' , '[' : ']' , } stack := [] rune {} for _ , r := range s { if _ , ok := pairs [ r ]; ok { stack = append ( stack , r ) } else if len ( stack ) == 0 || pairs [ stack [ len ( stack ) - 1 ]] != r { return false } else { stack = stack [: len ( stack ) - 1 ] } } return len ( stack ) == 0 }","title":"stack"},{"location":"al_ds/#back-tracking","text":"","title":"Back tracking"},{"location":"al_ds/#recursion_1","text":"","title":"recursion"},{"location":"al_ds/#stack_1","text":"","title":"stack"},{"location":"al_ds/#dp","text":"","title":"DP"},{"location":"al_ds/#lru-cathe","text":"","title":"LRU cathe"},{"location":"al_ds/#table","text":"","title":"table"},{"location":"al_ds/#greedy","text":"","title":"Greedy"},{"location":"al_ds/#linked-list","text":"","title":"Linked List"},{"location":"al_ds/#tree","text":"","title":"Tree"},{"location":"al_ds/#heap","text":"","title":"Heap"},{"location":"al_ds/#union-find","text":"","title":"Union Find"},{"location":"al_ds/#topological-sort","text":"","title":"Topological sort"},{"location":"al_ds/#diijkstra","text":"","title":"Diijkstra"},{"location":"al_ds/#a","text":"","title":"A"},{"location":"al_ds/#_3","text":"","title":"\u6700\u5c0f\u5168\u57df\u6728"},{"location":"al_ds/#flow","text":"","title":"Flow"},{"location":"books_note/","text":"\u8aad\u66f8\u30e1\u30e2 \u00b6 \u306a\u305c\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u7d42\u308f\u3089\u306a\u3044\u306e\u304b \u00b6 \u5185\u5bb9 \u00b6 \u6700\u521d\u306e 2 \u5272\u3067\u898b\u7a4d\u3082\u308a\u3092\u884c\u3044\u30018 \u5272\u3092\u7d42\u308f\u3089\u3059 100%\u3088\u308a\u3082 80%\u3092\u76ee\u6307\u3059 \u5168\u4f53\u3092\u628a\u63e1\u3057\u3001\u30b4\u30fc\u30eb\u3092\u660e\u78ba\u306b\u3057\u3001\u89e3\u6c7a\u624b\u6bb5\uff08\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30d7\u30e9\u30f3\uff09\u3092\u8907\u6570\u7528\u610f \u56f0\u96e3\u306f\u5206\u5272\u305b\u3088\u3001\u4ed5\u4e8b\u306e\u30d6\u30ec\u30fc\u30af\u30c0\u30a6\u30f3 \u30e2\u30c3\u30af\u30a2\u30c3\u30d7\u3001\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u3092\u4f5c\u308b\uff08\u6587\u7ae0\u3088\u308a\u3082\u56f3\u3067\u8aac\u660e\uff09","title":"\u8aad\u66f8\u30e1\u30e2"},{"location":"books_note/#_1","text":"","title":"\u8aad\u66f8\u30e1\u30e2"},{"location":"books_note/#_2","text":"","title":"\u306a\u305c\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u7d42\u308f\u3089\u306a\u3044\u306e\u304b"},{"location":"books_note/#_3","text":"\u6700\u521d\u306e 2 \u5272\u3067\u898b\u7a4d\u3082\u308a\u3092\u884c\u3044\u30018 \u5272\u3092\u7d42\u308f\u3089\u3059 100%\u3088\u308a\u3082 80%\u3092\u76ee\u6307\u3059 \u5168\u4f53\u3092\u628a\u63e1\u3057\u3001\u30b4\u30fc\u30eb\u3092\u660e\u78ba\u306b\u3057\u3001\u89e3\u6c7a\u624b\u6bb5\uff08\u30d0\u30c3\u30af\u30a2\u30c3\u30d7\u30d7\u30e9\u30f3\uff09\u3092\u8907\u6570\u7528\u610f \u56f0\u96e3\u306f\u5206\u5272\u305b\u3088\u3001\u4ed5\u4e8b\u306e\u30d6\u30ec\u30fc\u30af\u30c0\u30a6\u30f3 \u30e2\u30c3\u30af\u30a2\u30c3\u30d7\u3001\u30d7\u30ed\u30c8\u30bf\u30a4\u30d7\u3092\u4f5c\u308b\uff08\u6587\u7ae0\u3088\u308a\u3082\u56f3\u3067\u8aac\u660e\uff09","title":"\u5185\u5bb9"},{"location":"colab/","text":"Google colab \u00b6 Timeout \u00b6 1 2 3 4 5 function KeepClicking (){ console . log ( \"Clicking\" ); document . querySelector ( \"colab-connect-button\" ). click () } setInterval ( KeepClicking , 60000 ) Data\u3092content\u76f4\u4e0b\u306b\u79fb\u52d5\u3057\u3066unzip \u00b6 1 2 3 %%capture !unzip \"/content/drive/MyDrive/kaggle/input/seti-breakthrough-listen/seti-train.zip\" -d \"/content\" print ( 'Downlaod done' ) Kaggle\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3046 \u00b6 1 2 3 4 5 6 7 8 import os import json f = open ( \"/content/drive/My Drive/Kaggle/kaggle.json\" , 'r' ) json_data = json . load ( f ) #JSON\u5f62\u5f0f\u3067\u8aad\u307f\u8fbc\u3080 os . environ [ 'KAGGLE_USERNAME' ] = json_data [ 'username' ] os . environ [ 'KAGGLE_KEY' ] = json_data [ 'key' ] ! kaggle competitions submit digit - recognizer - f my_submission . csv - m \"Yeah! I submit my file through the Google Colab!\" Github\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u00b6 1 !pip install git+https://github.com/yseeker/tez_custom Output\u30bb\u30eb\u3092\u975e\u8868\u793a \u00b6 1 %%capture","title":"Google Colab"},{"location":"colab/#google-colab","text":"","title":"Google colab"},{"location":"colab/#timeout","text":"1 2 3 4 5 function KeepClicking (){ console . log ( \"Clicking\" ); document . querySelector ( \"colab-connect-button\" ). click () } setInterval ( KeepClicking , 60000 )","title":"Timeout"},{"location":"colab/#datacontentunzip","text":"1 2 3 %%capture !unzip \"/content/drive/MyDrive/kaggle/input/seti-breakthrough-listen/seti-train.zip\" -d \"/content\" print ( 'Downlaod done' )","title":"Data\u3092content\u76f4\u4e0b\u306b\u79fb\u52d5\u3057\u3066unzip"},{"location":"colab/#kaggle","text":"1 2 3 4 5 6 7 8 import os import json f = open ( \"/content/drive/My Drive/Kaggle/kaggle.json\" , 'r' ) json_data = json . load ( f ) #JSON\u5f62\u5f0f\u3067\u8aad\u307f\u8fbc\u3080 os . environ [ 'KAGGLE_USERNAME' ] = json_data [ 'username' ] os . environ [ 'KAGGLE_KEY' ] = json_data [ 'key' ] ! kaggle competitions submit digit - recognizer - f my_submission . csv - m \"Yeah! I submit my file through the Google Colab!\"","title":"Kaggle\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3046"},{"location":"colab/#github","text":"1 !pip install git+https://github.com/yseeker/tez_custom","title":"Github\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"colab/#output","text":"1 %%capture","title":"Output\u30bb\u30eb\u3092\u975e\u8868\u793a"},{"location":"docker/","text":"Dockerhub \u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Dockerfile \u304b\u3089 DockerImage, DockerImage \u304b\u3089\u30b3\u30f3\u30c6\u30ca https://scrapbox.io/llminatoll/docker_run%E3%81%AE%E3%82%AA%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D https://beyondjapan.com/blog/2016/08/docker-command-reverse-resolutions/ https://smot93516.hatenablog.jp/entry/2018/09/20/001052 https://morizyun.github.io/docker/about-docker-command.html \u793e\u5185\u306e Dockerfile \u306e\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u3092\u516c\u958b\u3057\u307e\u3059 https://qiita.com/zembutsu/items/a96b68277d699f79418d https://www.slideshare.net/zembutsu/explaining-best-practices-for-writing-dockerfiles https://qiita.com/zembutsu/items/24558f9d0d254e33088f https://www.slideshare.net/zembutsu/what-isdockerdoing https://nykergoto.hatenablog.jp/entry/2020/07/25/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AAdockerfile%E3%82%92%E6%9B%B8%E3%81%8F%E3%81%A8%E3%81%8D%E3%81%AB%E6%B0%97%E3%82%92%E3%81%A4%E3%81%91%E3%81%A8%E3%81%8F%E3%81%A8%E8%89%AF%E3%81%84%E3%81%93 https://qiita.com/zembutsu/items/a96b68277d699f79418d DL/RL on server \u306e Docker \u74b0\u5883\u69cb\u7bc9\u3010\u795d 500DL\uff01\u3011(21/08/04 \u66f4\u65b0) Running Jupyter on a remote Docker container using SSH docker login : dockerhub \u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b docker pull <image> dockerhub \u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u3068\u3063\u3066\u304f\u308b docker images dockerimage \u306e\u4e00\u89a7\u3092\u8868\u793a docker run <image> create + start, docker \u30a4\u30e1\u30fc\u30b8\u304b\u3089\u30b3\u30f3\u30c6\u30ca\u3092\u4f5c\u6210\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u304c\u5b9f\u884c\u3055\u308c\u308b\u3002 docker ps -a \u30b3\u30f3\u30c6\u30ca\u30a4\u30e1\u30fc\u30b8\u306e\u4e00\u89a7\u3092\u8868\u793a docker run -it ubuntu bash :docker \u3067 ubuntu \u306e bash \u3092\u8d77\u52d5\uff08bash \u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30de\u30f3\u30c9\u4e0a\u66f8\u304d\uff09, -it \u306f bash \u3092\u8d77\u52d5\u72b6\u614b\uff08up\uff09\u306b\u4fdd\u6301\u3059\u308b\u3002-it \u304c\u306a\u3044\u3068 exit \u72b6\u614b\u306b\u5909\u308f\u308b\u3002-i:\u6a19\u6e96\u5165\u529b\u3092\u958b\u304f\u3001-t:\u51fa\u529b\u304c\u304d\u308c\u3044\u306b\u306a\u308b\u3002 ctri + p + q :detach \u7d42\u4e86 docker attach <container> attach \u3067 up \u72b6\u614b\u306e container \u306b\u5165\u308b\u3002 exit :\u7d42\u4e86 docker restart <container> docker exec -it <container> <command> docker commit <container> <new image> \u30b3\u30f3\u30c6\u30ca\u304b\u3089 new image \u3068\u3057\u3066\u4fdd\u5b58 docker commit <container> ubuntu:updated \u30bb\u30df\u30b3\u30ed\u30f3\u3067 tag \u540d\u306b\u306a\u308b image \u540d\u306f repostitory \u540d\uff0b tag \u540d docker tag <source> <target> docker tag ubuntu:updated <username>/my-repo :\u540d\u524d\u306e\u5909\u66f4 library/ubuntu \u306f, \u6b63\u5f0f\u306b\u306f registry-1.docker.io/library/ubuntu:latest docker push <image> docker pull <image> docker rmi <image> docker image \u3092\u524a\u9664 docker rm <container> \u30b3\u30f3\u30c6\u30ca\u306e\u524a\u9664 docker stop <container> \u30b3\u30f3\u30c6\u30ca\u3092\u6b62\u3081\u308b docker system prune :\u30b3\u30f3\u30c6\u30ca\u5168\u524a\u9664 sudo docker system prune -all : \u30b3\u30f3\u30c6\u30ca\u30a4\u30e1\u30fc\u30b8\u5168\u524a\u9664 docker volume prune docker run --name <name> <image> :\u30b3\u30f3\u30c6\u30ca\u306e\u540d\u524d\u3092\u3064\u3051\u308b\u3002 docker run -d <image> :\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5\u5f8c\u306b detach \u3059\u308b\uff08host \u306b\u623b\u308b\uff09 docker run -rm <image> :\u30b3\u30f3\u30c6\u30ca\u3092 exit \u5f8c\u306b\u524a\u9664\u3059\u308b\u3002 docker images -aq | xargs docker rmi docker ps -aq | xargs docker rm docker stop $(docker ps -q) : \u3059\u3079\u3066\u306e\u30b3\u30f3\u30c6\u30ca\u505c\u6b62 docker rm $(docker ps -q -a) : \u3059\u3079\u3066\u306e\u30b3\u30f3\u30c6\u30ca\u524a\u9664 docker rmi $(docker images -q) : \u3059\u3079\u3066\u306e\u30a4\u30e1\u30fc\u30b8\u524a\u9664 docker-compose logs --tail=100 -f xxxx-server docker system prune -a --filter \"until=168h\" `` docker file \u306e\u4f5c\u6210 \u00b6 1 2 3 4 5 6 7 8 9 10 11 FROM ubuntu:latest ADD copressed.tar / COPY something /new_directory/ ENV key1 value RUN apt update && apt install -y \\ aaa \\ bbb \\ ccc WORKDIR /sample_folder RUN touch something CMD [ \"executable\" , \"param1\" , \"param2\" ] docker build <directory> docker build -t <name> <directory> docker build -f <dockerfilename> <build context> \u540d\u524d\u306f\u30c9\u30c3\u30c8\u3067\u3064\u306a\u304c\u308b\u3053\u3068\u304c\u591a\u3044\u3002Dockerfile \u3068\u3044\u3046\u540d\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u30d3\u30eb\u30c9\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u5165\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3002 FROM :\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30a4\u30e1\u30fc\u30b8\u3092\u6c7a\u5b9a RUN :Linux \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3002RUN \u6bce\u306b Layer \u304c\u4f5c\u3089\u308c\u308b\u3002Layer \u6570\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002&&\u3067\u3064\u306a\u3052\u308b\u3002(\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u3067)\\\u30d0\u30c3\u30af\u30b9\u30e9\u30c3\u30b7\u30e5\u3067\u6539\u884c\u3059\u308b\u3002 \u6700\u521d\u306f Layer \u3092\u7d30\u304b\u304f\u5206\u3051\u3066\u901a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\u6700\u5f8c\u306b Layer \u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 CMD :\u30b3\u30f3\u30c6\u30ca\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u3092\u6307\u5b9a\u3002CMD [\"command\", \"param1\", \"paramn2\"] ex. CMD [/bin/bash], CMD \u306f\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u3089\u306a\u3044\u3002 Docker \u30b3\u30de\u30f3\u30c9\u3067 Docker Daemon \u306b\u547d\u4ee4\u3092\u51fa\u3059 COPY: \u5358\u7d14\u306b\u30d5\u30a1\u30a4\u30eb\u3084\u30d5\u30a9\u30eb\u30c0\u3092\u30b3\u30d4\u30fc\u3059\u308b\u5834\u5408 ADD: tar \u306e\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u3092\u89e3\u7b54\u3059\u308b ENTRYPOINT \u306f\u4e0a\u66f8\u304d\u3067\u304d\u306a\u3044\uff08CMD \u306f\u4e0a\u66f8\u304d\u3067\u304d\u308b\uff09\u3002ENTRYPOINT \u304c\u3042\u308b\u3068\u304d\u306f CMD \u306f params \u306e\u307f\u3092\u66f8\u304f\u3002 ENTRYPOINT \u306f\u30b3\u30f3\u30c6\u30ca\u3092\u30b3\u30de\u30f3\u30c9\u306e\u3088\u3046\u306b\u4f7f\u3044\u305f\u3044\u3068\u304d\u3002 ENV :\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u3002 WORKDIR \u5b9f\u884c\u74b0\u5883\u3092\u5909\u66f4\u3059\u308b\u3002 \u30db\u30b9\u30c8\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u3064\u306a\u3050 \u00b6 docker run -it -v <host>:<container> <image bash> docker run -it -u $(id -u):$(id -g) -v ~/mouted_folder:/new_dir <image> bash -u <uder id>:<group id>: \u30e6\u30fc\u30b6ID\u3068\u30b0\u30eb\u30fc\u30d7ID\u3092\u6307\u5b9a\u3059\u308b -p <host_port>:<container_port> docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash docker run -it --rm --cpus 4 --memory 2g ubuntu bash docker inspect <container> | grep -i cpu \u30ed\u30fc\u30ab\u30eb\u3067\u74b0\u5883\u69cb\u7bc9 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 FROM ubuntu:latest RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ] docker run -p 8888:8888 -v ~/Desktop/ds-pyhton:/work --name my-lab <container> GPU \u74b0\u5883\u4f8b \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip && pip install \\ keras == 2 .3 \\ scipy == 1 .4.1 \\ tensorflow-gpu == 2 .1 WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ] 20211205c \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 FROM nvcr.io/nvidia/pytorch:21.11-py3 ENV DEBIAN_FRONTEND = noninteractive ENV TZ = Asia/Tokyo RUN ln -snf /usr/share/zoneinfo/ $TZ /etc/localtime && echo $TZ > /etc/timezone RUN apt-get update && apt-get upgrade -y \\ && apt-get install -y --no-install-recommends \\ sudo \\ wget \\ curl \\ git \\ vim \\ make \\ cmake \\ nodejs \\ default-jdk \\ default-jre \\ libgl1-mesa-dev \\ && apt-get autoremove -y \\ && apt-get clean \\ && rm -rf \\ /var/lib/apt/lists/* \\ /var/cache/apt/* \\ /usr/local/src/* \\ /tmp/* RUN conda install -y \\ nodejs && \\ conda clean -i -t -y RUN python3 -m pip install --upgrade pip \\ && pip install --no-cache-dir --upgrade \\ black \\ jupyterlab \\ jupyterlab_code_formatter \\ lckr-jupyterlab-variableinspector \\ jupyterlab_widgets \\ ipywidgets \\ import-ipynb \\ && jupyter labextension install jupyterlab-plotly@5.4.0 RUN pip install --no-cache-dir --upgrade \\ numpy \\ pandas == 1 .2.5 \\ scipy \\ scikit-learn \\ matplotlib \\ seaborn \\ plotly \\ Pillow \\ opencv-python \\ opencv-contrib-python \\ albumentations \\ requests \\ tqdm \\ xgboost \\ lightgbm \\ optuna \\ timm \\ omegaconf \\ hydra-core \\ pytorch-pfn-extras \\ wandb \\ mlflow \\ pyyaml \\ mplfinance \\ statsmodels \\ shap \\ numba \\ ccxt \\ TA-Lib == 0 .4.21 \\ ta \\ pandas_ta \\ finta \\ tsfresh \\ && rm -rf ~/.cache/pip minimum \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 FROM python : 3.8 ENV DEBIAN_FRONTEND = noninteractive ENV TZ = Asia / Tokyo RUN ln - snf / usr / share / zoneinfo / $ TZ / etc / localtime && echo $ TZ > / etc / timezone RUN apt - get update && apt - get upgrade - y \\ && apt - get install - y -- no - install - recommends \\ sudo \\ wget \\ make \\ && apt - get autoremove - y \\ && apt - get clean \\ && rm - rf \\ / var / lib / apt / lists /* \\ / var / cache / apt /* \\ / usr / local / src /* \\ / tmp /* RUN pip install -- no - cache - dir \\ pandas \\ scipy \\ scikit - learn \\ matplotlib \\ seaborn \\ japanize - matplotlib \\ tqdm \\ lightgbm \\ pyyaml \\ requests \\ numba \\ shap \\ statsmodels \\ optuna \\ mlflow \\ ccxt \\ ta \\ pandas_ta \\ finta \\ mplfinance \\ tsfresh \\ \"git+https://github.com/richmanbtc/crypto_data_fetcher.git@v0.0.15#egg=crypto_data_fetcher\" \\ && rm - rf ~/. cache / pip RUN cd / tmp \\ && wget http : // prdownloads . sourceforge . net / ta - lib / ta - lib - 0.4.0 - src . tar . gz \\ && tar - zxvf ta - lib - 0.4.0 - src . tar . gz \\ && cd ta - lib \\ && ./ configure -- prefix =/ usr \\ && sudo make \\ && sudo make install \\ && cd ../ \\ && sudo rm - rf ta - lib - 0.4.0 - src . tar . gz \\ && sudo rm - rf ta - lib \\ && python3 - m pip install -- upgrade pip \\ && pip install -- no - cache - dir install TA - Lib \\ && rm - rf / tmp /* CMD [ \"/bin/bash\" ] docker makefile \u00b6 http://www.jsk.t.u-tokyo.ac.jp/~k-okada/makefile/ Docker \u306e Makefile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 NAME = xxx VERSION = xxx build : docker build -t $( NAME ) : $( VERSION ) . restart : stop start start : docker run -itd \\ -p xxx:xxx \\ -v xxx:xxx \\ --name $( NAME ) \\ $( NAME ) : $( VERSION ) bash contener = ` docker ps -a -q ` image = ` docker images | awk '/^<none>/ { print $$3 }' ` clean : @if [ \" $( image ) \" ! = \"\" ] ; then \\ docker rmi $( image ) ; \\ fi @if [ \" $( contener ) \" ! = \"\" ] ; then \\ docker rm $( contener ) ; \\ fi stop : docker rm -f $( NAME ) attach : docker exec -it $( NAME ) /bin/bash logs : docker logs $( NAME ) Laravel \u958b\u767a\u6642\u306b\u4fbf\u5229\u306a make(Makefile)\u30b3\u30de\u30f3\u30c9\u96c6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 up : docker-compose up -d build : docker-compose build create-project : docker-compose up -d --build docker-compose exec app composer create-project --prefer-dist laravel/laravel . docker-compose exec app composer require predis/predis install : docker-compose up -d --build docker-compose exec app composer install docker-compose exec app cp .env.example .env docker-compose exec app php artisan key:generate docker-compose exec app php artisan migrate:fresh --seed reinstall : @make destroy @make install stop : docker-compose stop restart : docker-compose down docker-compose up -d down : docker-compose down destroy : docker-compose down --rmi all --volumes ps : docker-compose ps app : docker-compose exec app bash fresh : docker-compose exec app php artisan migrate:fresh --seed seed : docker-compose exec app php artisan db:seed tinker : docker-compose exec app php artisan tinker dump : docker-compose exec app php artisan dump-server test : docker-compose exec app php ./vendor/bin/phpunit cache : docker-compose exec app composer dump-autoload -o docker-compose exec app php artisan optimize:clear docker-compose exec app php artisan optimize cache-clear : docker-compose exec app php artisan optimize:clear cs : docker-compose exec app ./vendor/bin/phpcs cbf : docker-compose exec app ./vendor/bin/phpcbf db : docker-compose exec db bash sql : docker-compose exec db bash -c 'mysql -u $$MYSQL_USER -p$$MYSQL_PASSWORD $$MYSQL_DATABASE' node : docker-compose exec node ash npm : docker-compose exec node npm install docker-compose exec node npm run dev yarn : docker-compose exec node yarn docker-compose exec node yarn dev 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 DOCKER_REPOSITORY := shibui/ml-system-in-actions ABSOLUTE_PATH := $( shell pwd ) DOCKERFILE := Dockerfile IMAGE_VERSION := 0 .0.1 WEB_SINGLE_PATTERN := web_single_pattern WEB_SINGLE_PATTERN_PORT := 8000 .PHONY : build build : docker build \\ -t $( DOCKER_REPOSITORY ) : $( WEB_SINGLE_PATTERN ) _ $( IMAGE_VERSION ) \\ -f $( DOCKERFILE ) \\ . .PHONY : run run : docker run \\ -it \\ --name $( WEB_SINGLE_PATTERN ) \\ -p $( WEB_SINGLE_PATTERN_PORT ) : $( WEB_SINGLE_PATTERN_PORT ) \\ $( DOCKER_REPOSITORY ) : $( WEB_SINGLE_PATTERN ) _ $( IMAGE_VERSION ) .PHONY : stop stop : docker rm -f $( WEB_SINGLE_PATTERN ) .PHONY : push push : docker push $( DOCKER_REPOSITORY ) : $( WEB_SINGLE_PATTERN ) _ $( IMAGE_VERSION ) .PHONY : build_all build_all : build .PHONY : run_all run_all : run .PHONY : push_all push_all : push","title":"Docker"},{"location":"docker/#docker-file","text":"1 2 3 4 5 6 7 8 9 10 11 FROM ubuntu:latest ADD copressed.tar / COPY something /new_directory/ ENV key1 value RUN apt update && apt install -y \\ aaa \\ bbb \\ ccc WORKDIR /sample_folder RUN touch something CMD [ \"executable\" , \"param1\" , \"param2\" ] docker build <directory> docker build -t <name> <directory> docker build -f <dockerfilename> <build context> \u540d\u524d\u306f\u30c9\u30c3\u30c8\u3067\u3064\u306a\u304c\u308b\u3053\u3068\u304c\u591a\u3044\u3002Dockerfile \u3068\u3044\u3046\u540d\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u30d3\u30eb\u30c9\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u5165\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3002 FROM :\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30a4\u30e1\u30fc\u30b8\u3092\u6c7a\u5b9a RUN :Linux \u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3002RUN \u6bce\u306b Layer \u304c\u4f5c\u3089\u308c\u308b\u3002Layer \u6570\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002&&\u3067\u3064\u306a\u3052\u308b\u3002(\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u3067)\\\u30d0\u30c3\u30af\u30b9\u30e9\u30c3\u30b7\u30e5\u3067\u6539\u884c\u3059\u308b\u3002 \u6700\u521d\u306f Layer \u3092\u7d30\u304b\u304f\u5206\u3051\u3066\u901a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\u6700\u5f8c\u306b Layer \u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 CMD :\u30b3\u30f3\u30c6\u30ca\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u3092\u6307\u5b9a\u3002CMD [\"command\", \"param1\", \"paramn2\"] ex. CMD [/bin/bash], CMD \u306f\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u3089\u306a\u3044\u3002 Docker \u30b3\u30de\u30f3\u30c9\u3067 Docker Daemon \u306b\u547d\u4ee4\u3092\u51fa\u3059 COPY: \u5358\u7d14\u306b\u30d5\u30a1\u30a4\u30eb\u3084\u30d5\u30a9\u30eb\u30c0\u3092\u30b3\u30d4\u30fc\u3059\u308b\u5834\u5408 ADD: tar \u306e\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u3092\u89e3\u7b54\u3059\u308b ENTRYPOINT \u306f\u4e0a\u66f8\u304d\u3067\u304d\u306a\u3044\uff08CMD \u306f\u4e0a\u66f8\u304d\u3067\u304d\u308b\uff09\u3002ENTRYPOINT \u304c\u3042\u308b\u3068\u304d\u306f CMD \u306f params \u306e\u307f\u3092\u66f8\u304f\u3002 ENTRYPOINT \u306f\u30b3\u30f3\u30c6\u30ca\u3092\u30b3\u30de\u30f3\u30c9\u306e\u3088\u3046\u306b\u4f7f\u3044\u305f\u3044\u3068\u304d\u3002 ENV :\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u3002 WORKDIR \u5b9f\u884c\u74b0\u5883\u3092\u5909\u66f4\u3059\u308b\u3002","title":"docker file \u306e\u4f5c\u6210"},{"location":"docker/#_1","text":"docker run -it -v <host>:<container> <image bash> docker run -it -u $(id -u):$(id -g) -v ~/mouted_folder:/new_dir <image> bash -u <uder id>:<group id>: \u30e6\u30fc\u30b6ID\u3068\u30b0\u30eb\u30fc\u30d7ID\u3092\u6307\u5b9a\u3059\u308b -p <host_port>:<container_port> docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash docker run -it --rm --cpus 4 --memory 2g ubuntu bash docker inspect <container> | grep -i cpu","title":"\u30db\u30b9\u30c8\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u3064\u306a\u3050"},{"location":"docker/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 FROM ubuntu:latest RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ] docker run -p 8888:8888 -v ~/Desktop/ds-pyhton:/work --name my-lab <container>","title":"\u30ed\u30fc\u30ab\u30eb\u3067\u74b0\u5883\u69cb\u7bc9"},{"location":"docker/#gpu","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip && pip install \\ keras == 2 .3 \\ scipy == 1 .4.1 \\ tensorflow-gpu == 2 .1 WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ]","title":"GPU \u74b0\u5883\u4f8b"},{"location":"docker/#20211205c","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 FROM nvcr.io/nvidia/pytorch:21.11-py3 ENV DEBIAN_FRONTEND = noninteractive ENV TZ = Asia/Tokyo RUN ln -snf /usr/share/zoneinfo/ $TZ /etc/localtime && echo $TZ > /etc/timezone RUN apt-get update && apt-get upgrade -y \\ && apt-get install -y --no-install-recommends \\ sudo \\ wget \\ curl \\ git \\ vim \\ make \\ cmake \\ nodejs \\ default-jdk \\ default-jre \\ libgl1-mesa-dev \\ && apt-get autoremove -y \\ && apt-get clean \\ && rm -rf \\ /var/lib/apt/lists/* \\ /var/cache/apt/* \\ /usr/local/src/* \\ /tmp/* RUN conda install -y \\ nodejs && \\ conda clean -i -t -y RUN python3 -m pip install --upgrade pip \\ && pip install --no-cache-dir --upgrade \\ black \\ jupyterlab \\ jupyterlab_code_formatter \\ lckr-jupyterlab-variableinspector \\ jupyterlab_widgets \\ ipywidgets \\ import-ipynb \\ && jupyter labextension install jupyterlab-plotly@5.4.0 RUN pip install --no-cache-dir --upgrade \\ numpy \\ pandas == 1 .2.5 \\ scipy \\ scikit-learn \\ matplotlib \\ seaborn \\ plotly \\ Pillow \\ opencv-python \\ opencv-contrib-python \\ albumentations \\ requests \\ tqdm \\ xgboost \\ lightgbm \\ optuna \\ timm \\ omegaconf \\ hydra-core \\ pytorch-pfn-extras \\ wandb \\ mlflow \\ pyyaml \\ mplfinance \\ statsmodels \\ shap \\ numba \\ ccxt \\ TA-Lib == 0 .4.21 \\ ta \\ pandas_ta \\ finta \\ tsfresh \\ && rm -rf ~/.cache/pip","title":"20211205c"},{"location":"docker/#minimum","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 FROM python : 3.8 ENV DEBIAN_FRONTEND = noninteractive ENV TZ = Asia / Tokyo RUN ln - snf / usr / share / zoneinfo / $ TZ / etc / localtime && echo $ TZ > / etc / timezone RUN apt - get update && apt - get upgrade - y \\ && apt - get install - y -- no - install - recommends \\ sudo \\ wget \\ make \\ && apt - get autoremove - y \\ && apt - get clean \\ && rm - rf \\ / var / lib / apt / lists /* \\ / var / cache / apt /* \\ / usr / local / src /* \\ / tmp /* RUN pip install -- no - cache - dir \\ pandas \\ scipy \\ scikit - learn \\ matplotlib \\ seaborn \\ japanize - matplotlib \\ tqdm \\ lightgbm \\ pyyaml \\ requests \\ numba \\ shap \\ statsmodels \\ optuna \\ mlflow \\ ccxt \\ ta \\ pandas_ta \\ finta \\ mplfinance \\ tsfresh \\ \"git+https://github.com/richmanbtc/crypto_data_fetcher.git@v0.0.15#egg=crypto_data_fetcher\" \\ && rm - rf ~/. cache / pip RUN cd / tmp \\ && wget http : // prdownloads . sourceforge . net / ta - lib / ta - lib - 0.4.0 - src . tar . gz \\ && tar - zxvf ta - lib - 0.4.0 - src . tar . gz \\ && cd ta - lib \\ && ./ configure -- prefix =/ usr \\ && sudo make \\ && sudo make install \\ && cd ../ \\ && sudo rm - rf ta - lib - 0.4.0 - src . tar . gz \\ && sudo rm - rf ta - lib \\ && python3 - m pip install -- upgrade pip \\ && pip install -- no - cache - dir install TA - Lib \\ && rm - rf / tmp /* CMD [ \"/bin/bash\" ]","title":"minimum"},{"location":"docker/#docker-makefile","text":"http://www.jsk.t.u-tokyo.ac.jp/~k-okada/makefile/ Docker \u306e Makefile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 NAME = xxx VERSION = xxx build : docker build -t $( NAME ) : $( VERSION ) . restart : stop start start : docker run -itd \\ -p xxx:xxx \\ -v xxx:xxx \\ --name $( NAME ) \\ $( NAME ) : $( VERSION ) bash contener = ` docker ps -a -q ` image = ` docker images | awk '/^<none>/ { print $$3 }' ` clean : @if [ \" $( image ) \" ! = \"\" ] ; then \\ docker rmi $( image ) ; \\ fi @if [ \" $( contener ) \" ! = \"\" ] ; then \\ docker rm $( contener ) ; \\ fi stop : docker rm -f $( NAME ) attach : docker exec -it $( NAME ) /bin/bash logs : docker logs $( NAME ) Laravel \u958b\u767a\u6642\u306b\u4fbf\u5229\u306a make(Makefile)\u30b3\u30de\u30f3\u30c9\u96c6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 up : docker-compose up -d build : docker-compose build create-project : docker-compose up -d --build docker-compose exec app composer create-project --prefer-dist laravel/laravel . docker-compose exec app composer require predis/predis install : docker-compose up -d --build docker-compose exec app composer install docker-compose exec app cp .env.example .env docker-compose exec app php artisan key:generate docker-compose exec app php artisan migrate:fresh --seed reinstall : @make destroy @make install stop : docker-compose stop restart : docker-compose down docker-compose up -d down : docker-compose down destroy : docker-compose down --rmi all --volumes ps : docker-compose ps app : docker-compose exec app bash fresh : docker-compose exec app php artisan migrate:fresh --seed seed : docker-compose exec app php artisan db:seed tinker : docker-compose exec app php artisan tinker dump : docker-compose exec app php artisan dump-server test : docker-compose exec app php ./vendor/bin/phpunit cache : docker-compose exec app composer dump-autoload -o docker-compose exec app php artisan optimize:clear docker-compose exec app php artisan optimize cache-clear : docker-compose exec app php artisan optimize:clear cs : docker-compose exec app ./vendor/bin/phpcs cbf : docker-compose exec app ./vendor/bin/phpcbf db : docker-compose exec db bash sql : docker-compose exec db bash -c 'mysql -u $$MYSQL_USER -p$$MYSQL_PASSWORD $$MYSQL_DATABASE' node : docker-compose exec node ash npm : docker-compose exec node npm install docker-compose exec node npm run dev yarn : docker-compose exec node yarn docker-compose exec node yarn dev 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 DOCKER_REPOSITORY := shibui/ml-system-in-actions ABSOLUTE_PATH := $( shell pwd ) DOCKERFILE := Dockerfile IMAGE_VERSION := 0 .0.1 WEB_SINGLE_PATTERN := web_single_pattern WEB_SINGLE_PATTERN_PORT := 8000 .PHONY : build build : docker build \\ -t $( DOCKER_REPOSITORY ) : $( WEB_SINGLE_PATTERN ) _ $( IMAGE_VERSION ) \\ -f $( DOCKERFILE ) \\ . .PHONY : run run : docker run \\ -it \\ --name $( WEB_SINGLE_PATTERN ) \\ -p $( WEB_SINGLE_PATTERN_PORT ) : $( WEB_SINGLE_PATTERN_PORT ) \\ $( DOCKER_REPOSITORY ) : $( WEB_SINGLE_PATTERN ) _ $( IMAGE_VERSION ) .PHONY : stop stop : docker rm -f $( WEB_SINGLE_PATTERN ) .PHONY : push push : docker push $( DOCKER_REPOSITORY ) : $( WEB_SINGLE_PATTERN ) _ $( IMAGE_VERSION ) .PHONY : build_all build_all : build .PHONY : run_all run_all : run .PHONY : push_all push_all : push","title":"docker makefile"},{"location":"fastapi/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 task-server: container_name: task-container #task-container image: task-image runtime: nvidia build: context: \"api/task-api\" dockerfile: Dockerfile_dev volumes: - ${PWD}/api/task-api:/app ports: - \"5002:5002\" tty: true environment: TZ: \"Asia/Tokyo\" LC_ALL: C.UTF-8 LANG: C.UTF-8 PORT: \"5002\" DEBUG: \"True\" command: bash -c \"export CUDA_VISIBLE_DEVICES=2 && gunicorn app:app --bind 0.0.0.0:5002 -w 1 -k uvicorn.workers.UvicornWorker --timeout 1000000 --reload\" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # coding=utf-8 import os import io import sys import argparse import json import numpy as np from PIL import Image import cv2 import requests # Fast API from fastapi import FastAPI from fastapi import BackgroundTasks from fastapi.middleware.cors import CORSMiddleware from typing import Any , Dict import uvicorn if not os . path . isdir ( \"log\" ): os . mkdir ( \"log\" ) \"\"\" if( os.path.exists(os.path.join(\"log\", 'app.log')) ): os.remove(os.path.join(\"log\", 'app.log')) \"\"\" logger = logging . getLogger ( __name__ ) logger . setLevel ( 10 ) logger_fh = logging . FileHandler ( os . path . join ( \"log\" , 'app.log' )) logger . addHandler ( logger_fh ) logger . info ( \" {} {} start pf-afn api server\" . format ( datetime . now () . strftime ( \"%Y-%m- %d %H:%M:%S\" ), \"INFO\" )) app = FastAPI () @app . get ( '/' ) def index (): return 'Hello Proxy Server! \\n ' # health check @app . get ( '/health' ) def health (): return _health () # metadata \u306e\u53d6\u5f97 @app . get ( '/metadata' ) def metadata (): # ToDo return { \"metadata\" : None } @app . post ( '/api_server' ) def predict ( request_body : RequestBody ): start_time = time . time () job_id = str ( uuid . uuid4 ())[: 6 ] logger . info ( \" {} {} {} {} job_id= {} \" . format ( datetime . now () . strftime ( \"%Y-%m- %d %H:%M:%S\" ), \"INFO\" , sys . _getframe () . f_code . co_name , \"START\" , job_id )) logger . info ( \"ProxyAPIConfig : {} \" . format ( vars ( ProxyAPIConfig )) ) http_status_code = 200 response = { \"status\" : \"ok\" , \"data\" : data } elapsed_time = 1000 * ( time . time () - start_time ) logger . info ( \" {} {} {} {} job_id= {} , elapsed_time [ms]= {:.5f} \" . format ( datetime . now () . strftime ( \"%Y-%m- %d %H:%M:%S\" ), \"INFO\" , sys . _getframe () . f_code . co_name , \"END\" , job_id , elapsed_time )) return response","title":"Fastapi"},{"location":"gif/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \"\"\"Utility for creating a GIF. Creative Applications of Deep Learning w/ Tensorflow. Kadenze, Inc. Copyright Parag K. Mital, June 2016. \"\"\" import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation def build_gif ( imgs , interval = 0.1 , dpi = 72 , save_gif = True , saveto = 'animation.gif' , show_gif = False , cmap = None ): \"\"\"Take an array or list of images and create a GIF. Parameters ---------- imgs : np.ndarray or list List of images to create a GIF of interval : float, optional Spacing in seconds between successive images. dpi : int, optional Dots per inch. save_gif : bool, optional Whether or not to save the GIF. saveto : str, optional Filename of GIF to save. show_gif : bool, optional Whether or not to render the GIF using plt. cmap : None, optional Optional colormap to apply to the images. Returns ------- ani : matplotlib.animation.ArtistAnimation The artist animation from matplotlib. Likely not useful. \"\"\" imgs = np . asarray ( imgs ) h , w , * c = imgs [ 0 ] . shape fig , ax = plt . subplots ( figsize = ( np . round ( w / dpi ), np . round ( h / dpi ))) fig . subplots_adjust ( bottom = 0 ) fig . subplots_adjust ( top = 1 ) fig . subplots_adjust ( right = 1 ) fig . subplots_adjust ( left = 0 ) ax . set_axis_off () if cmap is not None : axs = list ( map ( lambda x : [ ax . imshow ( x , cmap = cmap )], imgs )) else : axs = list ( map ( lambda x : [ ax . imshow ( x )], imgs )) ani = animation . ArtistAnimation ( fig , axs , interval = interval * 1000 , repeat_delay = 0 , blit = True ) if save_gif : ani . save ( saveto , writer = 'imagemagick' , dpi = dpi ) if show_gif : plt . show () return ani","title":"Gif"},{"location":"git/","text":"Git \u00b6 \u30ea\u30f3\u30af \u00b6 mixi \u306e Git \u7814\u4fee\uff08 \u52d5\u753b \u3001 \u30b9\u30e9\u30a4\u30c9 \uff09 Kaggler \u306e\u305f\u3081\u306e Git \u5165\u9580 Git \u3067\u3088\u304f\u4f7f\u3046\u30b3\u30de\u30f3\u30c9\u4e00\u89a7 Git \u3067\u3084\u3089\u304b\u3057\u305f\u6642\u306b\u4f7f\u3048\u308b 19 \u500b\u306e\u5965\u7fa9 \u77e5\u3089\u306a\u304b\u3063\u305f Git \u30b3\u30de\u30f3\u30c9\u307e\u3068\u3081 \u3044\u307e\u3055\u3089\u3060\u3051\u3069 Git \u3092\u57fa\u672c\u304b\u3089\u5206\u304b\u308a\u3084\u3059\u304f\u307e\u3068\u3081\u3066\u307f\u305f git pull \u3068 git pull \u2013rebase \u306e\u9055\u3044\u3063\u3066\uff1f\u56f3\u3092\u4ea4\u3048\u3066\u8aac\u660e\u3057\u307e\u3059\uff01 Git \u3068\u306f \u00b6 Git \u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306e 1 \u3064\uff08\u5206\u6563\u7ba1\u7406\u65b9\u5f0f\uff09\u3002\u7279\u5b9a\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5dee\u5206\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3001\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3059\u308b\u3002 \u7528\u8a9e \u00b6 \u30ea\u30dd\u30b8\u30c8\u30ea\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5168\u3066\u306e\u30d5\u30a1\u30a4\u30eb\uff08\u5909\u66f4\u5c65\u6b74\u3084\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3082\u3059\u3079\u3066\uff09\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 \u30b3\u30df\u30c3\u30c8\uff1a\u89aa\u5b50\u95a2\u4fc2\u3092\u6301\u3064\u30b0\u30e9\u30d5\u3002\u30d5\u30a1\u30a4\u30eb\u306e\u72b6\u614b\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3053\u3068\u3002Working directory => staging area \uff08\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3082\u547c\u3070\u308c\u308b\uff09=> \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b3\u30df\u30c3\u30c8 \u30d6\u30e9\u30f3\u30c1\uff1a\u30b3\u30df\u30c3\u30c8\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002HEAD \u306f\u4eca\u81ea\u5206\u304c\u4f5c\u696d\u3057\u3066\u3044\u308b\u30d6\u30e9\u30f3\u30c1\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002\u30b3\u30df\u30c3\u30c8\u524d\u306b\u5206\u5c90\u3055\u305b\u308b\u3002\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u3057\u3066\u30de\u30fc\u30b8\u3055\u305b\u308b\u3002 Github \u00b6 \u30cf\u30a4\u30d5\u30f3\u3067\u540d\u524d\u3092\u533a\u5207\u308b\u306e\u304c\u4e00\u822c\u7684 ssh \u3067\u306e\u8a8d\u8a3c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 \u57fa\u672c\u7684\u306a\u6d41\u308c \u00b6 \u30ed\u30fc\u30ab\u30eb\u306b\u30e6\u30fc\u30b6\u60c5\u5831\u3092\u30bb\u30c3\u30c8\u78ba\u8a8d git config --global user.name \"<username, github\u306eusername>\" git config --global user.email \"<email>\" git config --global --list git config --global --replace-all core.pager \"less -F -X\" git config --global pull.rebase true git config core.filemode false \u30ea\u30dd\u30b8\u30c8\u30ea\u3092 clone git clone <remote_repo_url> git remote -v : \u767b\u9332\u3057\u3066\u3042\u308b\u30ea\u30e2\u30fc\u30c8\u30ea\u30dc\u3092\u78ba\u8a8d git clone \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f origin \u304c\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b\u7d10\u4ed8\u3044\u3066\u3044\u308b \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\u6307\u5b9a\uff1a git clone https://github.com/user/{\u30ea\u30dd\u30b8\u30c8\u30ea\u540d}.git {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u30d6\u30e9\u30f3\u30c1\u3092\u6307\u5b9a\u3059\u308b\u5834\u5408 git clone -b [\u30d6\u30e9\u30f3\u30c1\u540d][\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30a2\u30c9\u30ec\u30b9] {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u4f5c\u696d\u958b\u59cb\u524d\u306b pull \u3068 log \u78ba\u8a8d git pull git log \u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\uff08\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308b\uff09 git branch \u30d6\u30e9\u30f3\u30c1\u4e00\u89a7 git branch -a \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u542b\u3080\u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u306e\u4e00\u89a7 git branch <branch name> branch name \u3068\u3044\u3046 branch \u3092\u4f5c\u6210, HEAD \u306e\u30dd\u30a4\u30f3\u30bf\u5148\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u3002 git branch -m <old name> <new name> git branch -d <branch-name> \u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664 git checkout <branch name> branch name \u306b\u79fb\u52d5\u3002HEAD \u30dd\u30a4\u30f3\u30bf\u306e\u5207\u308a\u66ff\u3048 git checkout -b <branch name> \uff08\u5b9f\u7528\u4e0a\uff09branch \u4f5c\u6210\u3057\u3066 branch name \u306b\u79fb\u52d5 git checkout master -- directory_1 : \u5225\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u307e\u305f\u306f\u30d5\u30a1\u30a4\u30eb\u3092\u30b3\u30d4\u30fc git checkout master directory_1 \u30d6\u30e9\u30f3\u30c1\u540d\u306f\u30cf\u30a4\u30d5\u30f3\u3067\u533a\u5207\u308b \u73fe\u5728\u306e branch \u3092\u30c1\u30a7\u30c3\u30af git branch --contains=HEAD git rev-parse --abbrev-ref HEAD \u30ed\u30fc\u30ab\u30eb\u30d5\u30a1\u30a4\u30eb\uff08at working directory, working tree\uff09\u3092\u66f4\u65b0\u3057\u3066 Staging \u30a8\u30ea\u30a2\uff08index\uff09\u306b\u3042\u3052\u308b git diff <filename> \u3067 working directory \u3068 staging area \u306e diff \u3092\u78ba\u8a8d git diff HEAD <filename> \u3067 working directory \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD <filename> \u3067 staging area \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ <filename> 2 \u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main <filename> git diff HEAD..\u30ea\u30e2\u30fc\u30c8\u540d/\u30d6\u30e9\u30f3\u30c1\u540d : \u30ea\u30e2\u30fc\u30c8\u3068 HEAD \u306e\u5dee\u5206 git diff HEAD^ :\u30b3\u30df\u30c3\u30c8\u3068 HEAD \u306e\u5dee\u5206 \u30b9\u30c6\u30fc\u30b8\u30f3\u30b0\u30a8\u30ea\u30a2\u306b add \u3059\u308b\u3002 git add <file name> git add . add \u306e\u53d6\u308a\u6d88\u3057(\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u304c--mixed \u306a\u306e\u3067 HEAD \u306e\u4f4d\u7f6e\u306f\u305d\u306e\u307e\u307e\u3067\u3001index \u3060\u3051 HEAD \u306e\u4f4d\u7f6e\u306b\u623b\u308b) git reset HEAD \u72b6\u6cc1\u78ba\u8a8d git status \u30b3\u30df\u30c3\u30c8\u3059\u308b\uff08working tree, .git\uff09 git commit -m \"commit message\" git commit -m \"README.md #2 \u4fee\u6b63 \" git commit --amend \u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u4fee\u6b63 git commit -m \"README.md create close #1\" issue \u3092\u30af\u30ed\u30fc\u30ba\u3059\u308b git -c user.name='username' -c user.email='XXX@gmail.com' commit -m 'comment' commit \u306e\u53d6\u308a\u6d88\u3057(HEAD \u3092\uff11\u3064\u524d\u306b\u623b\u3059\uff09 git reset --soft HEAD^ commit \u3068 add \u306e\u53d6\u308a\u6d88\u3057\uff08HEAD \u3068 index \u3092\uff11\u3064\u524d\u306b\u623b\u3059\uff09 git reset --mixed HEAD^ commit \u6e08\u307f\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u53d6\u308a\u6d88\u3057 git rm --cached <\u30d5\u30a1\u30a4\u30eb\u540d> echo '<\u30d5\u30a1\u30a4\u30eb\u540d>' >> .gitignore tag \u4ed8 git tag <tagname> git tag --list git log --oneline --all --graph \u30b3\u30df\u30c3\u30c8\u3057\u305f\u5c65\u6b74\u3092\u78ba\u8a8d \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092 pull git pull <remote ref> <branch name> git pull origin main git pull --rebase <remote ref> <branch name> : pull \u3059\u308b\u3068\u304d\u306b rebase \u3059\u308b(\u5dee\u5206\u304c\u304d\u308c\u3044\u306b\u306a\u308b\u3002) \u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306e\u89e3\u6d88 \u30ed\u30fc\u30ab\u30eb\u3067\u64cd\u4f5c\u3059\u308b\u304b\u3001\u307e\u305f\u306f git checkout --ours \u30d5\u30a1\u30a4\u30eb\u540d git checkout --theirs \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306b push\u3002pull \u306f fetch + merge \uff08origin: \u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5834\u6240(URL)\u306e\u5225\u540d\uff09 git push <remote ref> <branch name> git push origin HEAD:push\u3057\u305f\u3044\u30d6\u30e9\u30f3\u30c1\u540d (error: src refspec \u30d6\u30e9\u30f3\u30c1\u540d does not match any) git push origin hoge:hoge :\u30ed\u30fc\u30ab\u30eb\u306e hoge \u30d6\u30e9\u30f3\u30c1\u3092\u30ea\u30e2\u30fc\u30c8\u306e hoge \u30d6\u30e9\u30f3\u30c1\u306b push\uff01 git push origin new-branch tag \u3065\u3051\u3001push git tag -a <tagname> <commitID> commit \u306b tag \u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag \u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b push \u3059\u308b push \u306e\u53d6\u308a\u6d88\u3057 ver1 git reset --hard HEAD^ git push -f origin HEAD push \u306e\u53d6\u308a\u6d88\u3057 ver2 git revert HEAD git push origin HEAD origin/master \u306e push \u53d6\u308a\u6d88\u3057 git push -f origin HEAD^:master git \u7ba1\u7406\u5bfe\u8c61\u304b\u3089\u5916\u3059\u3002 git rm -r --cached push \u3057\u305f\u30d6\u30e9\u30f3\u30c1\u3092 pull request \u3092\u4f5c\u3063\u3066\u30ea\u30e2\u30fc\u30c8\u306e main \u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8 Github \u3067\u4f5c\u696d\u3002 pull request \u3092\u30af\u30ea\u30c3\u30af => base (main)\u3068 compare (new branch)\u3092\u6307\u5b9a,\u81ea\u5206\u306e\u30ea\u30dd\u304b\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u304b\u3092\u78ba\u8a8d => create pull request => Merge pull request \u3092\u62bc\u3059\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e main \u306e\u53cd\u6620\u3092\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u306e main \u306b\u53cd\u6620\uff08pull\uff09 git checkout main git pull origin main OSS \u306a\u3069\u306e\u5834\u5408\u3067\u306f\u3001\u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092 pull \u3059\u308b\u3002 git remote add upstream <repourl> \uff1a\u672c\u5bb6\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u767b\u9332 git pull upstream main git push origin new-branch git pull --rebase origin master : rebase \u3057\u3066\u304b\u3089 pull OSS \u306e\u5834\u5408 git fetch upstream git merge upstream/main git push origin main \u4e0d\u8981\u306a\u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664\u3059\u308b\u3002 git branch -d <branch-name> Gighub \u3067 branches \u304b\u3089\u524a\u9664 \u57fa\u672c\u64cd\u4f5c \u00b6 \u30b9\u30af\u30e9\u30c3\u30c1\u304b\u3089\u4f5c\u6210\uff08.git \u306e\u4f5c\u6210\uff09 git init <project-name> .git \u306e\u524a\u9664 rm -rf .git \u65e2\u5b58\u306e\u30d5\u30a9\u30eb\u30c0\u3092 git \u30ea\u30dd\u306b\u3059\u308b\u3002 git init \u3000\u305d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u304b\u3089.git \u306e\u4f5c\u6210 \u65e2\u5b58\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u81ea\u5206\u306e\u30ea\u30dd\u3092\u30d5\u30a9\u30fc\u30af\u3057\u3066 clone git clone <httsps or ssh> track \u30d5\u30a1\u30a4\u30eb\u3068 untrack \u30d5\u30a1\u30a4\u30eb git ls-files \u3067 track \u3057\u3066\u3044\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u89a7\u3092\u78ba\u8a8d Staging area \u3078\u306e add \u3092\u30ad\u30e3\u30f3\u30bb\u30eb(git \u306e\u5185\u90e8\u3067\u306f\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5185\u5bb9\u3092 staging area \u306b\u4e0a\u66f8\u304d) git reset HEAD <filename> Working directory \u306e\u5185\u5bb9\u3092\u7121\u3057\u306b\u3059\u308b\u3002(git \u306e\u5185\u90e8\u3067\u306f working directory \u306e\u5185\u5bb9\u3092 staging area \u3067\u4e0a\u66f8\u304d\u3057\u3066\u3044\u308b\u3002) git checkout -- <file name> \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5909\u66f4\u3092 git \u3067\u7ba1\u7406 git mv <filename1> <filename2> (\u30b7\u30a7\u30eb\u306e mv \u3067\u5909\u66f4\u3057\u305f\u5834\u5408\u306f git add -A ) \u30d5\u30a1\u30a4\u30eb\u306e\u524a\u9664\u3092 Git \u3067\u7ba1\u7406\u3059\u308b\u3002 git rm <filename> (\u30b3\u30df\u30c3\u30c8\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044) git commit -m \"deleted\" \u524a\u9664\u5185\u5bb9\u306e\u53d6\u308a\u6d88\u3057 git reset HEAD <filename> git checkout -- <file name> \u30b3\u30df\u30c3\u30c8\u306e\u5c65\u6b74\u3092\u78ba\u8a8d\u3059\u308b git log --oneline, --graph, --<filename>, --follow <filename> git show <commitID> Git \u306e\u7ba1\u7406\u304b\u3089\u5916\u3059\u3002 .gitignore \u30d5\u30a1\u30a4\u30eb \u30b5\u30a4\u30ba\u304c\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u3084\u30d0\u30a4\u30ca\u30ea\u30fc\u30d5\u30a1\u30a4\u30eb\u3001\u4e2d\u9593\u30d5\u30a1\u30a4\u30eb\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u542b\u3080\u30d5\u30a1\u30a4\u30eb\u3001\u304f\u30a2\u30c3\u30b7\u30e5\u30d5\u30a1\u30a4\u30eb\u306a\u3069 \u30d6\u30e9\u30f3\u30c1\u3068\u30ed\u30fc\u30ab\u30eb\u3067\u30de\u30fc\u30b8,\u30ed\u30fc\u30ab\u30eb\u3067\u306e\u307f rebase \u3059\u308b \u00b6 git merge <branchname> \uff1abranchname \u3092\u4eca\u3044\u308b\u30d6\u30e9\u30f3\u30c1\uff08\u666e\u901a\u306f main \u30d6\u30e9\u30f3\u30c1\uff09\u306b\u53cd\u6620\u3002 git diff <base> <compare> \uff1abase\uff08main\uff09\u3068 compare\uff08\u30d6\u30e9\u30f3\u30c1\uff09\u3092\u4f5c\u6210 conflict \u304c\u8d77\u304d\u3066\u3044\u308b\u5834\u5408\u306f\u30a8\u30c7\u30a3\u30bf\u3067\u958b\u3044\u3066\u30a2\u30ce\u30fc\u30c6\u30b7\u30e7\u30f3\u7b87\u6240\u3092\u6d88\u3059\u3002 git rebase main : main \u30d6\u30e9\u30f3\u30c1\u3092 rebase \u3059\u308b\u3002rebase \u306f\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea \u00b6 git fetch <remote_ref> :\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092\u3068\u3063\u3066\u304f\u308b\u3002 git pull <remote_ref> <branchname> : git pull \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u304c\u3042\u308b\u5834\u5408\u306f\u5bfe\u51e6\u3059\u308b\u3002 Github \u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30d5\u30a9\u30fc\u30af\u5143\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3078\u306e pull request \u3092\u51fa\u3059\u3002 git remote add upstream <repourl> : \u30ed\u30fc\u30ab\u30eb\u306b\u306f origin \u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd \u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092 pull \u3057\u3066\u304b\u3089\u81ea\u5206\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b push \u3057\u3066 pull request \u3092\u4f5c\u6210\u3002 \u5dee\u5206 diff \u3092\u898b\u308b \u00b6 p4merge \u3092\u5c0e\u5165\u3059\u308b\u3002 git diff \u3067 working directory \u3068 staging area \u306e diss \u3092\u78ba\u8a8d git HEAD \u3067 working directory \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -- <filename> \u3067 working directory \u3068 staging area \u306e diss \u3092\u78ba\u8a8d git diff HEAD -- <filename> \u3067 working directory \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD -- <filename> \u3067 staging area \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ -- <filename> 2 \u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main -- <filename> Stash \u3092\u4f7f\u3046\u3002 \u00b6 \u4f5c\u696d\u5185\u5bb9\u306e\u4e00\u6642\u56de\u907f git stash git stash -a git stash list git stash apply git stash drop git stash show stash @{<i>} conflict \u304c\u3042\u308b\u5834\u5408 git mergetool \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306b\u5bfe\u51e6\u3059\u308b\u3002 ## Commit \u306b tag \u3092\u4f7f\u3046\u3002 \u30de\u30a4\u30eb\u30b9\u30c8\u30fc\u30f3\u306b tag \u3092\u4f7f\u3063\u3066 version \u3092\u7ba1\u7406\u3059\u308b git tag <tagname> git tag --list git tag --delete <tagname> git tag -a <tagname> tag \u3092\u3064\u3051\u308b\u3002 git diff <tagname1> <tagname2> git tag -a <tagname> <commitID> commit \u306b tag \u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag \u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b push \u3059\u308b git push <remote_ref> :<tagname> tag \u3092\u30ea\u30e2\u30fc\u30c8 push \u304b\u3089\u524a\u9664 git checkout tags/<tagname> git fetch --tgas --all submodule \u00b6 git submodule add <submodule_url> git submodule update git -recurse-submodule update submodule \u306e\u4e2d\u3067 git pull \u3059\u308b\u3002 git submodule foreach 'git pull origin main' others \u00b6 convertio.io wiki \u3092\u4f7f\u3046 octotree zenhub \u3092\u4f7f\u3046\uff1a\u30a2\u30b8\u30e3\u30a4\u30eb\u958b\u767a\u306e\u30ab\u30f3\u30d0\u30f3 git revert <commitID> git reset --hard git reset --sorf HEAD \u30d5\u30a1\u30a4\u30eb\u540d \u9593\u9055\u3063\u3066 add \u3057\u305f\u3068\u304d git reset \u2013soft HEAD^ \u9593\u9055\u3063\u3066 commit \u3057\u305f\u3068\u304d Github \u306b ssh \u3067\u63a5\u7d9a \u00b6 \u9375\u306e\u4f5c\u6210 \u00b6 1 2 3 4 5 6 7 8 9 ssh-keygen -t rsa # ssh-keygen -t ed25519 Generating public/private rsa key pair. Enter file in which to save the key ( /Users/yu/.ssh/id_rsa ) : /Users/yu/.ssh/github_rsa Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: # .pub\u3092Github\u306b\u767b\u9332 .ssh/config \u3092\u5909\u66f4 \u00b6 1 2 3 4 Host github github.com HostName github.com IdentityFile ~/.ssh/github_rsa #\u3053\u3053\u306b\u81ea\u5206\u306e\u9375\u306e\u30d5\u30a1\u30a4\u30eb\u540d User git \u63a5\u7d9a\u78ba\u8a8d \u00b6 1 ssh -T git@github.com .git \u30d5\u30a1\u30a4\u30eb\u306e\u5909\u66f4\u307e\u305f\u306f ssh \u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30af\u30ed\u30fc\u30f3 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 [ core ] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true [ remote \"origin\" ] url = git@github.com:xxxxx/yyyyy.git ######\u3053\u3053\u3092\u5909\u66f4\u3059\u308b fetch = +refs/heads/*:refs/remotes/origin/* [ branch \"master\" ] remote = origin merge = refs/heads/master .gitignore \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 Byte-compiled / optimized / DLL files __pycache__/ *.py[cod] *$py.class # C extensions *.so # Distribution / packaging .Python build/ develop-eggs/ dist/ downloads/ eggs/ .eggs/ lib/ lib64/ parts/ sdist/ var/ wheels/ *.egg-info/ .installed.cfg *.egg MANIFEST # PyInstaller # Usually these files are written by a python script from a template # before PyInstaller builds the exe, so as to inject date/other infos into it. *.manifest *.spec # Installer logs pip-log.txt pip-delete-this-directory.txt # Unit test / coverage reports htmlcov/ .tox/ .coverage .coverage.* .cache nosetests.xml coverage.xml *.cover .hypothesis/ .pytest_cache/ # Translations *.mo *.pot # Django stuff: *.log local_settings.py db.sqlite3 # Flask stuff: instance/ .webassets-cache # Scrapy stuff: .scrapy # Sphinx documentation docs/_build/ # PyBuilder target/ # Jupyter Notebook .ipynb_checkpoints # pyenv .python-version # celery beat schedule file celerybeat-schedule # SageMath parsed files *.sage.py # Environments .env .venv env/ venv/ ENV/ env.bak/ venv.bak/ # Spyder project settings .spyderproject .spyproject # Rope project settings .ropeproject # mkdocs documentation /site # mypy .mypy_cache/","title":"Git"},{"location":"git/#git","text":"","title":"Git"},{"location":"git/#_1","text":"mixi \u306e Git \u7814\u4fee\uff08 \u52d5\u753b \u3001 \u30b9\u30e9\u30a4\u30c9 \uff09 Kaggler \u306e\u305f\u3081\u306e Git \u5165\u9580 Git \u3067\u3088\u304f\u4f7f\u3046\u30b3\u30de\u30f3\u30c9\u4e00\u89a7 Git \u3067\u3084\u3089\u304b\u3057\u305f\u6642\u306b\u4f7f\u3048\u308b 19 \u500b\u306e\u5965\u7fa9 \u77e5\u3089\u306a\u304b\u3063\u305f Git \u30b3\u30de\u30f3\u30c9\u307e\u3068\u3081 \u3044\u307e\u3055\u3089\u3060\u3051\u3069 Git \u3092\u57fa\u672c\u304b\u3089\u5206\u304b\u308a\u3084\u3059\u304f\u307e\u3068\u3081\u3066\u307f\u305f git pull \u3068 git pull \u2013rebase \u306e\u9055\u3044\u3063\u3066\uff1f\u56f3\u3092\u4ea4\u3048\u3066\u8aac\u660e\u3057\u307e\u3059\uff01","title":"\u30ea\u30f3\u30af"},{"location":"git/#git_1","text":"Git \u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306e 1 \u3064\uff08\u5206\u6563\u7ba1\u7406\u65b9\u5f0f\uff09\u3002\u7279\u5b9a\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5dee\u5206\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3001\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3059\u308b\u3002","title":"Git \u3068\u306f"},{"location":"git/#_2","text":"\u30ea\u30dd\u30b8\u30c8\u30ea\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5168\u3066\u306e\u30d5\u30a1\u30a4\u30eb\uff08\u5909\u66f4\u5c65\u6b74\u3084\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3082\u3059\u3079\u3066\uff09\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 \u30b3\u30df\u30c3\u30c8\uff1a\u89aa\u5b50\u95a2\u4fc2\u3092\u6301\u3064\u30b0\u30e9\u30d5\u3002\u30d5\u30a1\u30a4\u30eb\u306e\u72b6\u614b\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3053\u3068\u3002Working directory => staging area \uff08\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3082\u547c\u3070\u308c\u308b\uff09=> \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b3\u30df\u30c3\u30c8 \u30d6\u30e9\u30f3\u30c1\uff1a\u30b3\u30df\u30c3\u30c8\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002HEAD \u306f\u4eca\u81ea\u5206\u304c\u4f5c\u696d\u3057\u3066\u3044\u308b\u30d6\u30e9\u30f3\u30c1\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002\u30b3\u30df\u30c3\u30c8\u524d\u306b\u5206\u5c90\u3055\u305b\u308b\u3002\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u3057\u3066\u30de\u30fc\u30b8\u3055\u305b\u308b\u3002","title":"\u7528\u8a9e"},{"location":"git/#github","text":"\u30cf\u30a4\u30d5\u30f3\u3067\u540d\u524d\u3092\u533a\u5207\u308b\u306e\u304c\u4e00\u822c\u7684 ssh \u3067\u306e\u8a8d\u8a3c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002","title":"Github"},{"location":"git/#_3","text":"\u30ed\u30fc\u30ab\u30eb\u306b\u30e6\u30fc\u30b6\u60c5\u5831\u3092\u30bb\u30c3\u30c8\u78ba\u8a8d git config --global user.name \"<username, github\u306eusername>\" git config --global user.email \"<email>\" git config --global --list git config --global --replace-all core.pager \"less -F -X\" git config --global pull.rebase true git config core.filemode false \u30ea\u30dd\u30b8\u30c8\u30ea\u3092 clone git clone <remote_repo_url> git remote -v : \u767b\u9332\u3057\u3066\u3042\u308b\u30ea\u30e2\u30fc\u30c8\u30ea\u30dc\u3092\u78ba\u8a8d git clone \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f origin \u304c\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b\u7d10\u4ed8\u3044\u3066\u3044\u308b \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\u6307\u5b9a\uff1a git clone https://github.com/user/{\u30ea\u30dd\u30b8\u30c8\u30ea\u540d}.git {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u30d6\u30e9\u30f3\u30c1\u3092\u6307\u5b9a\u3059\u308b\u5834\u5408 git clone -b [\u30d6\u30e9\u30f3\u30c1\u540d][\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30a2\u30c9\u30ec\u30b9] {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u4f5c\u696d\u958b\u59cb\u524d\u306b pull \u3068 log \u78ba\u8a8d git pull git log \u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\uff08\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308b\uff09 git branch \u30d6\u30e9\u30f3\u30c1\u4e00\u89a7 git branch -a \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u542b\u3080\u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u306e\u4e00\u89a7 git branch <branch name> branch name \u3068\u3044\u3046 branch \u3092\u4f5c\u6210, HEAD \u306e\u30dd\u30a4\u30f3\u30bf\u5148\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u3002 git branch -m <old name> <new name> git branch -d <branch-name> \u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664 git checkout <branch name> branch name \u306b\u79fb\u52d5\u3002HEAD \u30dd\u30a4\u30f3\u30bf\u306e\u5207\u308a\u66ff\u3048 git checkout -b <branch name> \uff08\u5b9f\u7528\u4e0a\uff09branch \u4f5c\u6210\u3057\u3066 branch name \u306b\u79fb\u52d5 git checkout master -- directory_1 : \u5225\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304b\u3089\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u307e\u305f\u306f\u30d5\u30a1\u30a4\u30eb\u3092\u30b3\u30d4\u30fc git checkout master directory_1 \u30d6\u30e9\u30f3\u30c1\u540d\u306f\u30cf\u30a4\u30d5\u30f3\u3067\u533a\u5207\u308b \u73fe\u5728\u306e branch \u3092\u30c1\u30a7\u30c3\u30af git branch --contains=HEAD git rev-parse --abbrev-ref HEAD \u30ed\u30fc\u30ab\u30eb\u30d5\u30a1\u30a4\u30eb\uff08at working directory, working tree\uff09\u3092\u66f4\u65b0\u3057\u3066 Staging \u30a8\u30ea\u30a2\uff08index\uff09\u306b\u3042\u3052\u308b git diff <filename> \u3067 working directory \u3068 staging area \u306e diff \u3092\u78ba\u8a8d git diff HEAD <filename> \u3067 working directory \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD <filename> \u3067 staging area \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ <filename> 2 \u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main <filename> git diff HEAD..\u30ea\u30e2\u30fc\u30c8\u540d/\u30d6\u30e9\u30f3\u30c1\u540d : \u30ea\u30e2\u30fc\u30c8\u3068 HEAD \u306e\u5dee\u5206 git diff HEAD^ :\u30b3\u30df\u30c3\u30c8\u3068 HEAD \u306e\u5dee\u5206 \u30b9\u30c6\u30fc\u30b8\u30f3\u30b0\u30a8\u30ea\u30a2\u306b add \u3059\u308b\u3002 git add <file name> git add . add \u306e\u53d6\u308a\u6d88\u3057(\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u304c--mixed \u306a\u306e\u3067 HEAD \u306e\u4f4d\u7f6e\u306f\u305d\u306e\u307e\u307e\u3067\u3001index \u3060\u3051 HEAD \u306e\u4f4d\u7f6e\u306b\u623b\u308b) git reset HEAD \u72b6\u6cc1\u78ba\u8a8d git status \u30b3\u30df\u30c3\u30c8\u3059\u308b\uff08working tree, .git\uff09 git commit -m \"commit message\" git commit -m \"README.md #2 \u4fee\u6b63 \" git commit --amend \u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u4fee\u6b63 git commit -m \"README.md create close #1\" issue \u3092\u30af\u30ed\u30fc\u30ba\u3059\u308b git -c user.name='username' -c user.email='XXX@gmail.com' commit -m 'comment' commit \u306e\u53d6\u308a\u6d88\u3057(HEAD \u3092\uff11\u3064\u524d\u306b\u623b\u3059\uff09 git reset --soft HEAD^ commit \u3068 add \u306e\u53d6\u308a\u6d88\u3057\uff08HEAD \u3068 index \u3092\uff11\u3064\u524d\u306b\u623b\u3059\uff09 git reset --mixed HEAD^ commit \u6e08\u307f\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u53d6\u308a\u6d88\u3057 git rm --cached <\u30d5\u30a1\u30a4\u30eb\u540d> echo '<\u30d5\u30a1\u30a4\u30eb\u540d>' >> .gitignore tag \u4ed8 git tag <tagname> git tag --list git log --oneline --all --graph \u30b3\u30df\u30c3\u30c8\u3057\u305f\u5c65\u6b74\u3092\u78ba\u8a8d \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092 pull git pull <remote ref> <branch name> git pull origin main git pull --rebase <remote ref> <branch name> : pull \u3059\u308b\u3068\u304d\u306b rebase \u3059\u308b(\u5dee\u5206\u304c\u304d\u308c\u3044\u306b\u306a\u308b\u3002) \u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306e\u89e3\u6d88 \u30ed\u30fc\u30ab\u30eb\u3067\u64cd\u4f5c\u3059\u308b\u304b\u3001\u307e\u305f\u306f git checkout --ours \u30d5\u30a1\u30a4\u30eb\u540d git checkout --theirs \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306b push\u3002pull \u306f fetch + merge \uff08origin: \u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5834\u6240(URL)\u306e\u5225\u540d\uff09 git push <remote ref> <branch name> git push origin HEAD:push\u3057\u305f\u3044\u30d6\u30e9\u30f3\u30c1\u540d (error: src refspec \u30d6\u30e9\u30f3\u30c1\u540d does not match any) git push origin hoge:hoge :\u30ed\u30fc\u30ab\u30eb\u306e hoge \u30d6\u30e9\u30f3\u30c1\u3092\u30ea\u30e2\u30fc\u30c8\u306e hoge \u30d6\u30e9\u30f3\u30c1\u306b push\uff01 git push origin new-branch tag \u3065\u3051\u3001push git tag -a <tagname> <commitID> commit \u306b tag \u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag \u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b push \u3059\u308b push \u306e\u53d6\u308a\u6d88\u3057 ver1 git reset --hard HEAD^ git push -f origin HEAD push \u306e\u53d6\u308a\u6d88\u3057 ver2 git revert HEAD git push origin HEAD origin/master \u306e push \u53d6\u308a\u6d88\u3057 git push -f origin HEAD^:master git \u7ba1\u7406\u5bfe\u8c61\u304b\u3089\u5916\u3059\u3002 git rm -r --cached push \u3057\u305f\u30d6\u30e9\u30f3\u30c1\u3092 pull request \u3092\u4f5c\u3063\u3066\u30ea\u30e2\u30fc\u30c8\u306e main \u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8 Github \u3067\u4f5c\u696d\u3002 pull request \u3092\u30af\u30ea\u30c3\u30af => base (main)\u3068 compare (new branch)\u3092\u6307\u5b9a,\u81ea\u5206\u306e\u30ea\u30dd\u304b\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u304b\u3092\u78ba\u8a8d => create pull request => Merge pull request \u3092\u62bc\u3059\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e main \u306e\u53cd\u6620\u3092\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u306e main \u306b\u53cd\u6620\uff08pull\uff09 git checkout main git pull origin main OSS \u306a\u3069\u306e\u5834\u5408\u3067\u306f\u3001\u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092 pull \u3059\u308b\u3002 git remote add upstream <repourl> \uff1a\u672c\u5bb6\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u767b\u9332 git pull upstream main git push origin new-branch git pull --rebase origin master : rebase \u3057\u3066\u304b\u3089 pull OSS \u306e\u5834\u5408 git fetch upstream git merge upstream/main git push origin main \u4e0d\u8981\u306a\u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664\u3059\u308b\u3002 git branch -d <branch-name> Gighub \u3067 branches \u304b\u3089\u524a\u9664","title":"\u57fa\u672c\u7684\u306a\u6d41\u308c"},{"location":"git/#_4","text":"\u30b9\u30af\u30e9\u30c3\u30c1\u304b\u3089\u4f5c\u6210\uff08.git \u306e\u4f5c\u6210\uff09 git init <project-name> .git \u306e\u524a\u9664 rm -rf .git \u65e2\u5b58\u306e\u30d5\u30a9\u30eb\u30c0\u3092 git \u30ea\u30dd\u306b\u3059\u308b\u3002 git init \u3000\u305d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u304b\u3089.git \u306e\u4f5c\u6210 \u65e2\u5b58\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u81ea\u5206\u306e\u30ea\u30dd\u3092\u30d5\u30a9\u30fc\u30af\u3057\u3066 clone git clone <httsps or ssh> track \u30d5\u30a1\u30a4\u30eb\u3068 untrack \u30d5\u30a1\u30a4\u30eb git ls-files \u3067 track \u3057\u3066\u3044\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u89a7\u3092\u78ba\u8a8d Staging area \u3078\u306e add \u3092\u30ad\u30e3\u30f3\u30bb\u30eb(git \u306e\u5185\u90e8\u3067\u306f\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5185\u5bb9\u3092 staging area \u306b\u4e0a\u66f8\u304d) git reset HEAD <filename> Working directory \u306e\u5185\u5bb9\u3092\u7121\u3057\u306b\u3059\u308b\u3002(git \u306e\u5185\u90e8\u3067\u306f working directory \u306e\u5185\u5bb9\u3092 staging area \u3067\u4e0a\u66f8\u304d\u3057\u3066\u3044\u308b\u3002) git checkout -- <file name> \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5909\u66f4\u3092 git \u3067\u7ba1\u7406 git mv <filename1> <filename2> (\u30b7\u30a7\u30eb\u306e mv \u3067\u5909\u66f4\u3057\u305f\u5834\u5408\u306f git add -A ) \u30d5\u30a1\u30a4\u30eb\u306e\u524a\u9664\u3092 Git \u3067\u7ba1\u7406\u3059\u308b\u3002 git rm <filename> (\u30b3\u30df\u30c3\u30c8\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044) git commit -m \"deleted\" \u524a\u9664\u5185\u5bb9\u306e\u53d6\u308a\u6d88\u3057 git reset HEAD <filename> git checkout -- <file name> \u30b3\u30df\u30c3\u30c8\u306e\u5c65\u6b74\u3092\u78ba\u8a8d\u3059\u308b git log --oneline, --graph, --<filename>, --follow <filename> git show <commitID> Git \u306e\u7ba1\u7406\u304b\u3089\u5916\u3059\u3002 .gitignore \u30d5\u30a1\u30a4\u30eb \u30b5\u30a4\u30ba\u304c\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u3084\u30d0\u30a4\u30ca\u30ea\u30fc\u30d5\u30a1\u30a4\u30eb\u3001\u4e2d\u9593\u30d5\u30a1\u30a4\u30eb\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u542b\u3080\u30d5\u30a1\u30a4\u30eb\u3001\u304f\u30a2\u30c3\u30b7\u30e5\u30d5\u30a1\u30a4\u30eb\u306a\u3069","title":"\u57fa\u672c\u64cd\u4f5c"},{"location":"git/#rebase","text":"git merge <branchname> \uff1abranchname \u3092\u4eca\u3044\u308b\u30d6\u30e9\u30f3\u30c1\uff08\u666e\u901a\u306f main \u30d6\u30e9\u30f3\u30c1\uff09\u306b\u53cd\u6620\u3002 git diff <base> <compare> \uff1abase\uff08main\uff09\u3068 compare\uff08\u30d6\u30e9\u30f3\u30c1\uff09\u3092\u4f5c\u6210 conflict \u304c\u8d77\u304d\u3066\u3044\u308b\u5834\u5408\u306f\u30a8\u30c7\u30a3\u30bf\u3067\u958b\u3044\u3066\u30a2\u30ce\u30fc\u30c6\u30b7\u30e7\u30f3\u7b87\u6240\u3092\u6d88\u3059\u3002 git rebase main : main \u30d6\u30e9\u30f3\u30c1\u3092 rebase \u3059\u308b\u3002rebase \u306f\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3002","title":"\u30d6\u30e9\u30f3\u30c1\u3068\u30ed\u30fc\u30ab\u30eb\u3067\u30de\u30fc\u30b8,\u30ed\u30fc\u30ab\u30eb\u3067\u306e\u307f rebase \u3059\u308b"},{"location":"git/#_5","text":"git fetch <remote_ref> :\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092\u3068\u3063\u3066\u304f\u308b\u3002 git pull <remote_ref> <branchname> : git pull \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u304c\u3042\u308b\u5834\u5408\u306f\u5bfe\u51e6\u3059\u308b\u3002 Github \u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30d5\u30a9\u30fc\u30af\u5143\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3078\u306e pull request \u3092\u51fa\u3059\u3002 git remote add upstream <repourl> : \u30ed\u30fc\u30ab\u30eb\u306b\u306f origin \u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd \u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092 pull \u3057\u3066\u304b\u3089\u81ea\u5206\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b push \u3057\u3066 pull request \u3092\u4f5c\u6210\u3002","title":"\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea"},{"location":"git/#diff","text":"p4merge \u3092\u5c0e\u5165\u3059\u308b\u3002 git diff \u3067 working directory \u3068 staging area \u306e diss \u3092\u78ba\u8a8d git HEAD \u3067 working directory \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -- <filename> \u3067 working directory \u3068 staging area \u306e diss \u3092\u78ba\u8a8d git diff HEAD -- <filename> \u3067 working directory \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD -- <filename> \u3067 staging area \u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ -- <filename> 2 \u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main -- <filename>","title":"\u5dee\u5206 diff \u3092\u898b\u308b"},{"location":"git/#stash","text":"\u4f5c\u696d\u5185\u5bb9\u306e\u4e00\u6642\u56de\u907f git stash git stash -a git stash list git stash apply git stash drop git stash show stash @{<i>} conflict \u304c\u3042\u308b\u5834\u5408 git mergetool \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306b\u5bfe\u51e6\u3059\u308b\u3002 ## Commit \u306b tag \u3092\u4f7f\u3046\u3002 \u30de\u30a4\u30eb\u30b9\u30c8\u30fc\u30f3\u306b tag \u3092\u4f7f\u3063\u3066 version \u3092\u7ba1\u7406\u3059\u308b git tag <tagname> git tag --list git tag --delete <tagname> git tag -a <tagname> tag \u3092\u3064\u3051\u308b\u3002 git diff <tagname1> <tagname2> git tag -a <tagname> <commitID> commit \u306b tag \u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag \u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b push \u3059\u308b git push <remote_ref> :<tagname> tag \u3092\u30ea\u30e2\u30fc\u30c8 push \u304b\u3089\u524a\u9664 git checkout tags/<tagname> git fetch --tgas --all","title":"Stash \u3092\u4f7f\u3046\u3002"},{"location":"git/#submodule","text":"git submodule add <submodule_url> git submodule update git -recurse-submodule update submodule \u306e\u4e2d\u3067 git pull \u3059\u308b\u3002 git submodule foreach 'git pull origin main'","title":"submodule"},{"location":"git/#others","text":"convertio.io wiki \u3092\u4f7f\u3046 octotree zenhub \u3092\u4f7f\u3046\uff1a\u30a2\u30b8\u30e3\u30a4\u30eb\u958b\u767a\u306e\u30ab\u30f3\u30d0\u30f3 git revert <commitID> git reset --hard git reset --sorf HEAD \u30d5\u30a1\u30a4\u30eb\u540d \u9593\u9055\u3063\u3066 add \u3057\u305f\u3068\u304d git reset \u2013soft HEAD^ \u9593\u9055\u3063\u3066 commit \u3057\u305f\u3068\u304d","title":"others"},{"location":"git/#github-ssh","text":"","title":"Github \u306b ssh \u3067\u63a5\u7d9a"},{"location":"git/#_6","text":"1 2 3 4 5 6 7 8 9 ssh-keygen -t rsa # ssh-keygen -t ed25519 Generating public/private rsa key pair. Enter file in which to save the key ( /Users/yu/.ssh/id_rsa ) : /Users/yu/.ssh/github_rsa Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: # .pub\u3092Github\u306b\u767b\u9332","title":"\u9375\u306e\u4f5c\u6210"},{"location":"git/#sshconfig","text":"1 2 3 4 Host github github.com HostName github.com IdentityFile ~/.ssh/github_rsa #\u3053\u3053\u306b\u81ea\u5206\u306e\u9375\u306e\u30d5\u30a1\u30a4\u30eb\u540d User git","title":".ssh/config \u3092\u5909\u66f4"},{"location":"git/#_7","text":"1 ssh -T git@github.com","title":"\u63a5\u7d9a\u78ba\u8a8d"},{"location":"git/#git-ssh","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 [ core ] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true [ remote \"origin\" ] url = git@github.com:xxxxx/yyyyy.git ######\u3053\u3053\u3092\u5909\u66f4\u3059\u308b fetch = +refs/heads/*:refs/remotes/origin/* [ branch \"master\" ] remote = origin merge = refs/heads/master","title":".git \u30d5\u30a1\u30a4\u30eb\u306e\u5909\u66f4\u307e\u305f\u306f ssh \u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30af\u30ed\u30fc\u30f3"},{"location":"git/#gitignore","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 Byte-compiled / optimized / DLL files __pycache__/ *.py[cod] *$py.class # C extensions *.so # Distribution / packaging .Python build/ develop-eggs/ dist/ downloads/ eggs/ .eggs/ lib/ lib64/ parts/ sdist/ var/ wheels/ *.egg-info/ .installed.cfg *.egg MANIFEST # PyInstaller # Usually these files are written by a python script from a template # before PyInstaller builds the exe, so as to inject date/other infos into it. *.manifest *.spec # Installer logs pip-log.txt pip-delete-this-directory.txt # Unit test / coverage reports htmlcov/ .tox/ .coverage .coverage.* .cache nosetests.xml coverage.xml *.cover .hypothesis/ .pytest_cache/ # Translations *.mo *.pot # Django stuff: *.log local_settings.py db.sqlite3 # Flask stuff: instance/ .webassets-cache # Scrapy stuff: .scrapy # Sphinx documentation docs/_build/ # PyBuilder target/ # Jupyter Notebook .ipynb_checkpoints # pyenv .python-version # celery beat schedule file celerybeat-schedule # SageMath parsed files *.sage.py # Environments .env .venv env/ venv/ ENV/ env.bak/ venv.bak/ # Spyder project settings .spyderproject .spyproject # Rope project settings .ropeproject # mkdocs documentation /site # mypy .mypy_cache/","title":".gitignore \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8"},{"location":"linux_command/","text":"Linux \u00b6 \u30ea\u30f3\u30af \u00b6 https://dotinstall.com/lessons/basic_unix_v2 https://prog-8.com/courses/commandline https://uguisu.skr.jp/Windows/ \u201c\u5fdc\u7528\u529b\u201d\u3092\u3064\u3051\u308b\u305f\u3081\u306e Linux \u518d\u5165\u9580 Linux \u8a2d\u5b9a\u95a2\u9023 \u00b6 \u30b7\u30a7\u30eb\u306e Alias \u306e\u8a2d\u5b9a \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # \u3088\u304f\u5229\u7528\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u982d\u6587\u5b57\u306e\u9023\u7d50 alias abc = 'cd ~/aaa/bbb/ccc' alias g = 'git' alias ga = 'git add' alias gd = 'git diff' alias gs = 'git status' alias gp = 'git push' alias gb = 'git branch' alias gst = 'git status' alias gco = 'git checkout' alias gf = 'git fetch' alias gc = 'git commit' alias cp = 'cp -i' alias mv = 'mv -i' alias rm = 'rm -i' alias hg = 'history | grep' alias tma = 'tmux a -t' alias tmn = 'tmux new-session -s' # u \u304a\u3059\u3059\u3081.vimrc \u00b6 1 2 3 4 5 6 7 8 9 10 11 set number set expandtab set hlsearch set ignorecase set incsearch set smartcase set laststatus=2 set nocompatible set clipboard=unnamed,autoselect set clipboard& clipboard^=unnamedplus syntax on .netrc \u306e\u66f8\u304d\u65b9 \u00b6 1 2 3 4 5 6 7 machine api.wandb.ai login user password **** machine github.com login your_username password *** Linux \u30b3\u30de\u30f3\u30c9 \u00b6 \u53c2\u8003\u30ea\u30f3\u30af \u00b6 https://atmarkit.itmedia.co.jp/ait/articles/1906/05/news004.html https://qiita.com/nmrmsys/items/03f97f5eabec18a3a18b https://qiita.com/arene-calix/items/41d8d4ba572f1d652727 https://qiita.com/savaniased/items/d2c5c699188a0f1623ef https://tech-blog.rakus.co.jp/entry/20210604/linux https://pg-happy.jp/linux-command.html \u30d5\u30a1\u30a4\u30eb\u30fb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u95a2\u9023 \u00b6 pwd (print working directory) \u00b6 1 2 3 # pwd: \u4eca\u3044\u308b\u30d5\u30a9\u30eb\u30c0\u306e\u7d76\u5bfe\u30d1\u30b9\u3092\u8868\u793a yseeker@~/Desktop $ pwd /home/yseeker/Desktop ls (list) \u00b6 1 2 3 4 5 6 7 ls -alh ls -ltr # -a: \u96a0\u3057\u30d5\u30a1\u30a4\u30eb\u3082\u8868\u793a(\u7531\u6765: all) # -l: \u8a73\u7d30\u306a\u60c5\u5831\u3092\u8868\u793a # -h: M(\u30e1\u30ac)\u3001G(\u30ae\u30ac)\u306a\u3069\u3092\u4ed8\u3051\u3066\u30b5\u30a4\u30ba\u3092\u898b\u3084\u3059\u304f\u3059\u308b(\u7531\u6765:human readable) # -t: \u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u9806\u3067\u8868\u793a(\u7531\u6765: time) # -r: \u9006\u9806\u3067\u8868\u793a(\u7531\u6765: reverse) cd (change directory) \u00b6 1 cd - #\u76f4\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5 mkdir (make directory) \u00b6 1 mkdir -p aaa/bbb/ccc #\u89aa\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u4f5c\u6210 touch \u00b6 \u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\u3001\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u66f4\u65b0 files \u00b6 file \u306e\u60c5\u5831\u3092\u78ba\u8a8d\u3067\u304d\u308b\u3002 mv \u00b6 \u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5\u3001\u540d\u524d\u5909\u66f4 cp \u00b6 1 2 3 4 # cp -r source/path destination/path: \u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30d4\u30fc # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u30b3\u30d4\u30fc(ecursive) # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(force) # * -p: \u30b3\u30d4\u30fc\u524d\u5f8c\u3067\u30d1\u30fc\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u4fdd\u6301(permission) rm \u00b6 1 2 3 4 5 6 # rm -rf directory_name: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664 # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(\u7531\u6765: force) # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u524a\u9664(\u7531\u6765: recursive) $ rm -f *.txt # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066txt\u30d5\u30a1\u30a4\u30eb\u3092\u5168\u524a\u9664 $ rm -rf dir* # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066\u4e00\u62ec\u524a\u9664 tar \u00b6 1 2 3 4 5 # tar -czvf xxx.tgz file1 file2 dir1 : \u5727\u7e2e(file1 file2 dir1\u3092\u30a2\u30fc\u30ab\u30a4\u30d6\u3057\u305f\u5727\u7e2e\u30d5\u30a1\u30a4\u30ebxxx.tgz\u3092\u4f5c\u6210) # tar -tzvf xxx.tgz: \u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306b\u542b\u307e\u308c\u308b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8868\u793a(=\u5c55\u958b\u306e\u30c6\u30b9\u30c8) # tar -xzvf xxx.tgz: \u5c55\u958b # * c(create), t(test), x(extract) + zvf\u3068\u899a\u3048\u308b tar czvf something.tgz dir* file* zip, unzip \u00b6 1 2 3 $ zip -r \u00abZIP\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb $ tar cvzf \u00abTARGZ\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb unzip -q foo.zip -d bar ln \u00b6 1 2 # \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u306e\u4f5c\u6210 ln -s oroginal_file symbolic_link \u30c6\u30ad\u30b9\u30c8\u51e6\u7406 \u00b6 cat (concatenate) \u00b6 1 2 access.log error1.log error2.log $ cat error*.log # error1.log\u3068error2.log\u3092\u307e\u3068\u3081\u3066\u78ba\u8a8d tail \u00b6 1 2 3 4 5 # \u91cd\u305f\u3044\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u6700\u5f8c\u306e\u65b9\u3060\u3051\u898b\u308b tail -n 3 file1.txt 11 kkk KKK 12 lll LLL 13 mmm MMM less \u00b6 1 2 3 4 5 6 7 8 9 10 # less file1: file1\u3092\u898b\u308b(read only) cat file1 | cmd1 | cmd2 | less: file1\u3092\u3044\u308d\u3044\u308d\u52a0\u5de5\u3057\u305f\u7d50\u679c\u3092\u898b\u308b # command | less - # grep 080 testData | less -N less +F output # * \u30bf\u30fc\u30df\u30ca\u30eb\u306b\u51fa\u529b\u305b\u305a\u3001\u4f55\u304b\u3092\u898b\u305f\u3044\u3068\u304d\u306b\u3068\u308a\u3042\u3048\u305a\u4f7f\u3046\u30b3\u30de\u30f3\u30c9 # gg: \u5148\u982d\u884c\u3078\u79fb\u52d5 # G: \u6700\u7d42\u884c\u3078\u79fb\u52d5 # /pattern: pattern\u3067\u30d5\u30a1\u30a4\u30eb\u5185\u691c\u7d22 # q: \u9589\u3058\u308b wc (word count) \u00b6 1 2 3 4 5 6 7 8 $ wc -l error.log # \u884c\u6570\u30ab\u30a6\u30f3\u30c8(1) 7 error.log # \u30d5\u30a1\u30a4\u30eb\u6570\u30ab\u30a6\u30f3\u30c8 ls | wc -w ls -U1 | wc -l find . -name \"*.jpg\" | wc -l ls -F | grep -v / | wc -l sort, uniq \u00b6 1 2 3 4 5 6 7 # sort file1: file1\u3092\u884c\u5358\u4f4d\u3067\u30bd\u30fc\u30c8 # uniq file1: file1\u306e\u91cd\u8907\u696d\u3092\u524a\u9664 # cat file1 | sort | uniq: file1\u3092\u30bd\u30fc\u30c8\u3057\u3066\u3001\u91cd\u8907\u696d\u3092\u6392\u9664 # * sort\u3068uniq\u306f\u30ef\u30f3\u30bb\u30c3\u30c8\u7684\u306a\u3068\u3053\u308d\u304c\u3042\u308b\u306e\u3067\u307e\u3068\u3081\u3066\u7d39\u4ecb # # * sort\u306f-r\u3067\u9006\u9806\u30bd\u30fc\u30c8\u3001-R\u3067\u30e9\u30f3\u30c0\u30e0\u30bd\u30fc\u30c8\u3001\u307f\u305f\u3044\u306b\u7d50\u69cb\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u591a\u5f69 # * ls -l\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u9806\u3067sort\u3059\u308b\u3001\u307f\u305f\u3044\u306b grep \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # grep ERROR *.log: \u62e1\u5f35\u5b50\u304clog\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u3001ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep -2 ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3068\u305d\u306e\u524d\u5f8c2\u884c\u3092\u51fa\u529b # cat error.log | grep -e ERROR -e WARN: error.log\u304b\u3089ERROR\u307e\u305f\u306fWARN\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa # cat error.log | grep ERROR | grep -v 400: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa\u3057\u3066\u3001400\u3092\u542b\u3080\u884c\u3092\u6392\u9664\u3057\u305f\u7d50\u679c\u3092\u8868\u793a # * -e: \u8907\u6570\u30ad\u30fc\u30ef\u30fc\u30c9\u3092AND\u6761\u4ef6\u3067\u6307\u5b9a(\u7531\u6765: ?? \u305f\u3076\u3093\u9055\u3046\u3051\u3069\u3001\u500b\u4eba\u7684\u306b\u306f\u30d5\u30e9\u30f3\u30b9\u8a9e\u306eet(=and)\u3060\u3068\u89e3\u91c8\u3057\u3066\u308b) # * -v: \u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u542b\u3080\u884c\u3092\u6392\u9664(\u7531\u6765: verbose??) # \u3069\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u5206\u304b\u3089\u306a\u3044\u3051\u3069\u3001Hoge\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u3067\u3001piyo\u3068\u3044\u3046\u6587\u5b57\u5217\u3092\u542b\u3093\u3067\u3044\u308b\u90e8\u5206\u3068\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u77e5\u308a\u305f\u3044 find ~/Hoge -name '*.txt' | xargs grep piyo $ pgrep -f vagrant # \u59cb\u672b $ pkill -f vagrant # \u30b7\u30b0\u30ca\u30eb\u3092\u6307\u5b9a $ pkill -SIGKILL -f vagrant https://qiita.com/uraura/items/12ff6112fd392f1be424 find \u00b6 1 2 3 4 5 6 7 8 9 10 # find dir1 -type f: dir1\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type f -name \"*.js\": dir1\u4ee5\u4e0b\u306ejs\u30d5\u30a1\u30a4\u30eb\u306e\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type d: dir1\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4e00\u89a7\u3092\u8868\u793a # * ls\u3068\u9055\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u304c\u51fa\u529b\u3055\u308c\u308b\u305f\u3081\u3001find xxx | xargs rm -rf \u307f\u305f\u3044\u306b\u4e00\u62ec\u64cd\u4f5c\u306b\u5411\u3044\u3066\u3044\u308b $ find src/ -type f find . -name '*.php' find . -name '???.txt' find . -name \"*.jpg\" | wc -l https://uguisu.skr.jp/Windows/find_xargs2.html sed \u00b6 1 2 3 4 5 6 7 # cat file1 | sed 's/BEFORE/AFTER/g': file1\u4e2d\u306eBEFORE\u3092AFTER\u306b\u4e00\u62ec\u7f6e\u63db # * s/BEFORE/AFTER/g: BEFORE\u3092AFTER\u306b\u7f6e\u63db(\u7531\u6765: substitute\u3068global?) #\u30b5\u30d6\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u542b\u3081\u305f\u6587\u5b57\u5217\u306e\u4e00\u62ec\u5909\u63db find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} \\; find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} + find ./ -type f | xargs sed -i \"s/hoge/fuga/g\" xargs \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # cmd1 | xargs cmd2: cmd1\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3068\u3057\u3066\u53d7\u3051\u53d6\u3063\u3066\u3001cmd2\u3092\u5b9f\u884c find . -name \"*.log\" | xargs rm -fv find TARGET -type d -empty | xargs rm -r find . -name \"*.log\" | xargs -i cp {} /tmp/. #\u6587\u5b57\u5217\u5909\u63db grep -rl 'hogehoge' ./* | xargs perl -i -pe \"s/hogehoge/fugafuga/g\" #100\u30d5\u30a1\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3093\u3067\u30b3\u30d4\u30fc\uff0e find /some/dir -type f -name \"*.jpg\" | shuf -n 100 | xargs cp -vt /target/dir/ #\u691c\u7d22\u3057\u3066\u898b\u3064\u304b\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5 find . -type f -print0 | xargs -0 mv -t /var/tmp/ https://uguisu.skr.jp/Windows/find_xargs2.html <, \uff06> , >> (\u30ea\u30c0\u30a4\u30ec\u30af\u30c8) \u00b6 https://hibiki-press.tech/dev-env/redirect_pipline/1571 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ echo \"4 ddd DDD\" >> file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u8ffd\u8a18) $ cat file1.txt $ echo \"4 ddd DDD\" > file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u4e0a\u66f8\u304d) #Python\u30b9\u30af\u30ea\u30d7\u30c8\u3078\u306e\u5165\u529b\u3092\u30d5\u30a1\u30a4\u30ebinput_data\u3078\u5909\u66f4\u3057\u3001 #\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30ebresult01\u306b\u51fa\u529b $ python3 sample02.py < input_data > result01 \u30a8\u30e9\u30fc\u51fa\u529b\u3082\u66f8\u304f python test.py & > output #sample02.py\u306e\u5b9f\u884c\u7d50\u679c\u3068\u30a8\u30e9\u30fc\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30ebresult02\u306b\u51fa\u529b $ python3 sample02.py > result02 2 > & 1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u00b6 echo \u00b6 1 2 3 # echo abc: \u6587\u5b57\u5217abc\u3092\u51fa\u529b # echo $PATH: \u74b0\u5883\u5909\u6570PATH\u3092\u51fa\u529b # print\u3068\u4e00\u7dd2 env \u00b6 1 2 # env | less: \u74b0\u5883\u5909\u6570\u3092\u78ba\u8a8d # * env\u3060\u3051\u3067\u3082\u898b\u308c\u308b\u304c\u3001\u74b0\u5883\u5909\u6570\u304c\u591a\u3044\u5834\u5408\u898b\u5207\u308c\u3066\u3057\u307e\u3046\u305f\u3081less\u3067\u78ba\u8a8d which \u00b6 1 # which cmd: cmd\u306e\u5b9f\u4f53\u304c\u7f6e\u304b\u308c\u3066\u3044\u308b\u5834\u6240\u3092\u8868\u793a source \u00b6 1 2 3 # source ~/.bashrc: .bashrc\u3092\u518d\u8aad\u307f\u8fbc\u307f # . ~/.bashrc: \u2191\u3068\u540c\u3058(.\u306fsource\u306e\u30a8\u30a4\u30ea\u30a2\u30b9) # * \u30b7\u30a7\u30eb\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u5909\u66f4\u3057\u305f\u5f8c\u306e\u518d\u8aad\u307f\u8fbc\u307f\u3067\u4f7f\u3046\u30b1\u30fc\u30b9\u304c100% chmod \u00b6 1 2 3 chmod 755 *.sh #sh\u30d5\u30a1\u30a4\u30eb\u306b\u5b9f\u884c\u6a29\u9650\u3092\u4ed8\u4e0e chmod 644 *.js #js\u30d5\u30a1\u30a4\u30eb\u3092\u666e\u901a\u306b\u8aad\u307f\u66f8\u304d\u3067\u304d\u308b\u8a2d\u5b9a\u306b\u3059\u308b chmod ugo+rwx -R /* OS \u95a2\u9023 \u00b6 df \u00b6 1 2 # df -h: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # df: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u8868 du \u00b6 1 2 3 4 5 # du -h: \u5404\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # \u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u8868\u793a\uff0e\u6df1\u30551 du -h -d1 . du -h -d 1 | sort -h free \u00b6 1 free -h #\u30e1\u30e2\u30ea\u4f7f\u7528\u72b6\u6cc1\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) top, ps, kill \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 top #CPU\u3084\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u72b6\u6cc1\u3092\u78ba\u8a8d # * \u30c7\u30d5\u30a9\u30eb\u30c8\u3060\u3068CPU\u4f7f\u7528\u7387\u306e\u591a\u3044\u30d7\u30ed\u30bb\u30b9\u304c\u4e0a\u306b\u6765\u308b # * %CPU\u304cCPU\u4f7f\u7528\u7387\u3002\u3069\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u9ad8\u8ca0\u8377\u304b\u3092\u78ba\u8a8d\u3067\u304d\u308b\u3002 ps -ef #\u5168\u3066\u306e\u30d7\u30ed\u30bb\u30b9\u306e\u8a73\u7d30\u306a\u60c5\u5831\u3092\u898b\u308b(\u7531\u6765: every, full) # * \u7528\u90141: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u304c\u751f\u304d\u3066\u308b\u304b\u3069\u3046\u304b\u30c1\u30a7\u30c3\u30af (web\u30b5\u30fc\u30d0\u8d77\u52d5\u3057\u3066\u308b?) # * \u7528\u90142: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u306ePID(\u30d7\u30ed\u30bb\u30b9ID)\u3092\u30c1\u30a7\u30c3\u30af -> kill ${PID} ps aux # kill 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u505c\u6b62\u3055\u305b\u308b(SIGTERM\u3092\u9001\u308b) # kill -9 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u554f\u7b54\u7121\u7528\u3067\u6bba\u3059(9\u306fSIGKILL\u306e\u30b7\u30b0\u30ca\u30eb\u756a\u53f7) # kill -KILL 123: -9\u3068\u540c\u3058 kill -9 [ \u756a\u53f7 ] # pkill process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u7d42\u4e86\u3055\u305b\u308b # pkill -9 process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u554f\u7b54\u7121\u7528\u3067\u7d42\u4e86\u3055\u305b\u308b \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c \u00b6 1 2 3 4 5 6 7 8 cmd1 #cmd1\u3092\u30d5\u30a9\u30a2\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c cmd1 & #cmd1\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c #\u30d5\u30a9\u30a2\u30b0\u30e9\u30a6\u30f3\u30c9 \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u306b\u5207\u308a\u66ff\u3048 jobs #job\u756a\u53f7\u3092\u8abf\u3079\u308b fg %1 bg %1 \u306b\u5207\u308a\u66ff\u3048 # * \u91cd\u305f\u3044\u30d0\u30c3\u30c1\u51e6\u7406\u3084\u3001\u4e00\u6642\u7684\u306bweb\u30b5\u30fc\u30d0\u3092\u52d5\u304b\u3057\u305f\u3044\u3068\u304d\u306f\u3001 # \u30b3\u30de\u30f3\u30c9\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c\u3059\u308b\u3068\u4fbf\u5229 &&, || \u00b6 1 2 3 4 # cmd1 && cmd2: cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # cmd1 || cmd2: cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # * \u7528\u90141: \u30ef\u30f3\u30e9\u30a4\u30ca\u30fc\u3067\u3061\u3087\u3063\u3068\u3057\u305f\u9010\u6b21\u51e6\u7406\u3092\u66f8\u304f # * \u7528\u90142: cmd1 || echo \"error message\" \u30ea\u30e2\u30fc\u30c8 \u00b6 ssh \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -L: \u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0 ssh -L <host port>:localhost:<remote port> user@remote #https://qiita.com/wnoguchi/items/a72a042bb8159c35d056 # ECDSA521 bit ssh-keygen -t ecdsa -b 521 -C \"wnoguchi-mbp\" # Ed25519 ssh-keygen -t ed25519 -P \"\" -f serial-server.pem ssh-keygen -t ed25519 cat public_key >> ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys scp \u00b6 1 2 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -r : \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u79fb\u52d5 useful commands \u00b6 \u5bb9\u91cf\u306e\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u306e\u7279\u5b9a \u00b6 \u4f7f\u7528\u91cf\u304c\u591a\u3044\u9806\u306b 5 \u4ef6\u3092\u53d6\u5f97\u3059\u308b 1 du -sm ./* | sort -rn | head -5 \u5bb9\u91cf\u3092\u304f\u3063\u3066\u308b\u30d9\u30b9\u30c8 100 1 du -ma | sort -rn | head -100 \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u307f\u3092\u691c\u7d22\u3059\u308b\u5834\u5408 1 du -m | sort -rn | head -100 30 \u65e5\u9593\u7de8\u96c6\u304c\u306a\u3044\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b5\u30a4\u30ba\u9806\u3067\u8868\u793a 1 find ./ -mtime +30 -prune -exec du -sh {} \\;|sort -hr \u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u8868\u793a\uff0e\u6df1\u3055 1\uff0e 1 du -h -d1 . 100 \u30d5\u30a1\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3093\u3067\u30b3\u30d4\u30fc\uff0e 1 find /some/dir -type f -name \"*.jpg\" | shuf -n 100 | xargs cp -vt /target/dir/ tgt_dir \u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5229\u7528\u3057\u3066\u3001src_dir \u304b\u3089 dst_fir \u3078\u3068\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5 1 ls tgt_dir | while read name; do mv src_dir/${name:0:-4}* dst_dir/; done tmux \u00b6 https://golang.hateblo.jp/entry/2019/10/11/133000 https://qiita.com/nmrmsys/items/03f97f5eabec18a3a18b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # \u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux # \u540d\u524d\u3092\u3064\u3051\u3066\u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux new -s <\u30bb\u30c3\u30b7\u30e7\u30f3\u540d> # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4e00\u89a7\u8868\u793a tmux ls # \u63a5\u7d9a\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u4e00\u89a7\u8868\u793a tmux lsc # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u518d\u958b \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux a -t [ session-name ] # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u7d42\u4e86 \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux kill-session -t [ session-name ] # tmux\u5168\u4f53\u3092\u7d42\u4e86 tmux kill-server # \u305d\u306e\u4ed6\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c tmux [ command [ flags ]] ctrl-b c : \u65b0\u3057\u3044\u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u8ffd\u52a0 ctrl-b b : \u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u79fb\u52d5 ctrl-b & : \u30a2\u30af\u30c6\u30a3\u30d6\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u524a\u9664 ctrl-b % : \u5de6\u53f3\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 ctrl-b \" : \u4e0a\u4e0b\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 : ctrl-b o or \u30ab\u30fc\u30bd\u30eb\uff1a\u30da\u30a4\u30f3\u9593\u306e\u79fb\u52d5 ctrl-b x : \u30da\u30a4\u30f3\u5206\u5272\u306e\u89e3\u9664 ctrl-b d: detach gunicorn, uvicorn \u00b6","title":"Linux"},{"location":"linux_command/#linux","text":"","title":"Linux"},{"location":"linux_command/#_1","text":"https://dotinstall.com/lessons/basic_unix_v2 https://prog-8.com/courses/commandline https://uguisu.skr.jp/Windows/ \u201c\u5fdc\u7528\u529b\u201d\u3092\u3064\u3051\u308b\u305f\u3081\u306e Linux \u518d\u5165\u9580","title":"\u30ea\u30f3\u30af"},{"location":"linux_command/#linux_1","text":"","title":"Linux \u8a2d\u5b9a\u95a2\u9023"},{"location":"linux_command/#alias","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # \u3088\u304f\u5229\u7528\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u982d\u6587\u5b57\u306e\u9023\u7d50 alias abc = 'cd ~/aaa/bbb/ccc' alias g = 'git' alias ga = 'git add' alias gd = 'git diff' alias gs = 'git status' alias gp = 'git push' alias gb = 'git branch' alias gst = 'git status' alias gco = 'git checkout' alias gf = 'git fetch' alias gc = 'git commit' alias cp = 'cp -i' alias mv = 'mv -i' alias rm = 'rm -i' alias hg = 'history | grep' alias tma = 'tmux a -t' alias tmn = 'tmux new-session -s' # u","title":"\u30b7\u30a7\u30eb\u306e Alias \u306e\u8a2d\u5b9a"},{"location":"linux_command/#vimrc","text":"1 2 3 4 5 6 7 8 9 10 11 set number set expandtab set hlsearch set ignorecase set incsearch set smartcase set laststatus=2 set nocompatible set clipboard=unnamed,autoselect set clipboard& clipboard^=unnamedplus syntax on","title":"\u304a\u3059\u3059\u3081.vimrc"},{"location":"linux_command/#netrc","text":"1 2 3 4 5 6 7 machine api.wandb.ai login user password **** machine github.com login your_username password ***","title":".netrc \u306e\u66f8\u304d\u65b9"},{"location":"linux_command/#linux_2","text":"","title":"Linux \u30b3\u30de\u30f3\u30c9"},{"location":"linux_command/#_2","text":"https://atmarkit.itmedia.co.jp/ait/articles/1906/05/news004.html https://qiita.com/nmrmsys/items/03f97f5eabec18a3a18b https://qiita.com/arene-calix/items/41d8d4ba572f1d652727 https://qiita.com/savaniased/items/d2c5c699188a0f1623ef https://tech-blog.rakus.co.jp/entry/20210604/linux https://pg-happy.jp/linux-command.html","title":"\u53c2\u8003\u30ea\u30f3\u30af"},{"location":"linux_command/#_3","text":"","title":"\u30d5\u30a1\u30a4\u30eb\u30fb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u95a2\u9023"},{"location":"linux_command/#pwd-print-working-directory","text":"1 2 3 # pwd: \u4eca\u3044\u308b\u30d5\u30a9\u30eb\u30c0\u306e\u7d76\u5bfe\u30d1\u30b9\u3092\u8868\u793a yseeker@~/Desktop $ pwd /home/yseeker/Desktop","title":"pwd (print working directory)"},{"location":"linux_command/#ls-list","text":"1 2 3 4 5 6 7 ls -alh ls -ltr # -a: \u96a0\u3057\u30d5\u30a1\u30a4\u30eb\u3082\u8868\u793a(\u7531\u6765: all) # -l: \u8a73\u7d30\u306a\u60c5\u5831\u3092\u8868\u793a # -h: M(\u30e1\u30ac)\u3001G(\u30ae\u30ac)\u306a\u3069\u3092\u4ed8\u3051\u3066\u30b5\u30a4\u30ba\u3092\u898b\u3084\u3059\u304f\u3059\u308b(\u7531\u6765:human readable) # -t: \u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u9806\u3067\u8868\u793a(\u7531\u6765: time) # -r: \u9006\u9806\u3067\u8868\u793a(\u7531\u6765: reverse)","title":"ls (list)"},{"location":"linux_command/#cd-change-directory","text":"1 cd - #\u76f4\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5","title":"cd (change directory)"},{"location":"linux_command/#mkdir-make-directory","text":"1 mkdir -p aaa/bbb/ccc #\u89aa\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u4f5c\u6210","title":"mkdir (make directory)"},{"location":"linux_command/#touch","text":"\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\u3001\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u66f4\u65b0","title":"touch"},{"location":"linux_command/#files","text":"file \u306e\u60c5\u5831\u3092\u78ba\u8a8d\u3067\u304d\u308b\u3002","title":"files"},{"location":"linux_command/#mv","text":"\u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5\u3001\u540d\u524d\u5909\u66f4","title":"mv"},{"location":"linux_command/#cp","text":"1 2 3 4 # cp -r source/path destination/path: \u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30d4\u30fc # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u30b3\u30d4\u30fc(ecursive) # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(force) # * -p: \u30b3\u30d4\u30fc\u524d\u5f8c\u3067\u30d1\u30fc\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u4fdd\u6301(permission)","title":"cp"},{"location":"linux_command/#rm","text":"1 2 3 4 5 6 # rm -rf directory_name: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664 # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(\u7531\u6765: force) # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u524a\u9664(\u7531\u6765: recursive) $ rm -f *.txt # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066txt\u30d5\u30a1\u30a4\u30eb\u3092\u5168\u524a\u9664 $ rm -rf dir* # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066\u4e00\u62ec\u524a\u9664","title":"rm"},{"location":"linux_command/#tar","text":"1 2 3 4 5 # tar -czvf xxx.tgz file1 file2 dir1 : \u5727\u7e2e(file1 file2 dir1\u3092\u30a2\u30fc\u30ab\u30a4\u30d6\u3057\u305f\u5727\u7e2e\u30d5\u30a1\u30a4\u30ebxxx.tgz\u3092\u4f5c\u6210) # tar -tzvf xxx.tgz: \u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306b\u542b\u307e\u308c\u308b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8868\u793a(=\u5c55\u958b\u306e\u30c6\u30b9\u30c8) # tar -xzvf xxx.tgz: \u5c55\u958b # * c(create), t(test), x(extract) + zvf\u3068\u899a\u3048\u308b tar czvf something.tgz dir* file*","title":"tar"},{"location":"linux_command/#zip-unzip","text":"1 2 3 $ zip -r \u00abZIP\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb $ tar cvzf \u00abTARGZ\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb unzip -q foo.zip -d bar","title":"zip, unzip"},{"location":"linux_command/#ln","text":"1 2 # \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u306e\u4f5c\u6210 ln -s oroginal_file symbolic_link","title":"ln"},{"location":"linux_command/#_4","text":"","title":"\u30c6\u30ad\u30b9\u30c8\u51e6\u7406"},{"location":"linux_command/#cat-concatenate","text":"1 2 access.log error1.log error2.log $ cat error*.log # error1.log\u3068error2.log\u3092\u307e\u3068\u3081\u3066\u78ba\u8a8d","title":"cat (concatenate)"},{"location":"linux_command/#tail","text":"1 2 3 4 5 # \u91cd\u305f\u3044\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u6700\u5f8c\u306e\u65b9\u3060\u3051\u898b\u308b tail -n 3 file1.txt 11 kkk KKK 12 lll LLL 13 mmm MMM","title":"tail"},{"location":"linux_command/#less","text":"1 2 3 4 5 6 7 8 9 10 # less file1: file1\u3092\u898b\u308b(read only) cat file1 | cmd1 | cmd2 | less: file1\u3092\u3044\u308d\u3044\u308d\u52a0\u5de5\u3057\u305f\u7d50\u679c\u3092\u898b\u308b # command | less - # grep 080 testData | less -N less +F output # * \u30bf\u30fc\u30df\u30ca\u30eb\u306b\u51fa\u529b\u305b\u305a\u3001\u4f55\u304b\u3092\u898b\u305f\u3044\u3068\u304d\u306b\u3068\u308a\u3042\u3048\u305a\u4f7f\u3046\u30b3\u30de\u30f3\u30c9 # gg: \u5148\u982d\u884c\u3078\u79fb\u52d5 # G: \u6700\u7d42\u884c\u3078\u79fb\u52d5 # /pattern: pattern\u3067\u30d5\u30a1\u30a4\u30eb\u5185\u691c\u7d22 # q: \u9589\u3058\u308b","title":"less"},{"location":"linux_command/#wc-word-count","text":"1 2 3 4 5 6 7 8 $ wc -l error.log # \u884c\u6570\u30ab\u30a6\u30f3\u30c8(1) 7 error.log # \u30d5\u30a1\u30a4\u30eb\u6570\u30ab\u30a6\u30f3\u30c8 ls | wc -w ls -U1 | wc -l find . -name \"*.jpg\" | wc -l ls -F | grep -v / | wc -l","title":"wc (word count)"},{"location":"linux_command/#sort-uniq","text":"1 2 3 4 5 6 7 # sort file1: file1\u3092\u884c\u5358\u4f4d\u3067\u30bd\u30fc\u30c8 # uniq file1: file1\u306e\u91cd\u8907\u696d\u3092\u524a\u9664 # cat file1 | sort | uniq: file1\u3092\u30bd\u30fc\u30c8\u3057\u3066\u3001\u91cd\u8907\u696d\u3092\u6392\u9664 # * sort\u3068uniq\u306f\u30ef\u30f3\u30bb\u30c3\u30c8\u7684\u306a\u3068\u3053\u308d\u304c\u3042\u308b\u306e\u3067\u307e\u3068\u3081\u3066\u7d39\u4ecb # # * sort\u306f-r\u3067\u9006\u9806\u30bd\u30fc\u30c8\u3001-R\u3067\u30e9\u30f3\u30c0\u30e0\u30bd\u30fc\u30c8\u3001\u307f\u305f\u3044\u306b\u7d50\u69cb\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u591a\u5f69 # * ls -l\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u9806\u3067sort\u3059\u308b\u3001\u307f\u305f\u3044\u306b","title":"sort, uniq"},{"location":"linux_command/#grep","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # grep ERROR *.log: \u62e1\u5f35\u5b50\u304clog\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u3001ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep -2 ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3068\u305d\u306e\u524d\u5f8c2\u884c\u3092\u51fa\u529b # cat error.log | grep -e ERROR -e WARN: error.log\u304b\u3089ERROR\u307e\u305f\u306fWARN\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa # cat error.log | grep ERROR | grep -v 400: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa\u3057\u3066\u3001400\u3092\u542b\u3080\u884c\u3092\u6392\u9664\u3057\u305f\u7d50\u679c\u3092\u8868\u793a # * -e: \u8907\u6570\u30ad\u30fc\u30ef\u30fc\u30c9\u3092AND\u6761\u4ef6\u3067\u6307\u5b9a(\u7531\u6765: ?? \u305f\u3076\u3093\u9055\u3046\u3051\u3069\u3001\u500b\u4eba\u7684\u306b\u306f\u30d5\u30e9\u30f3\u30b9\u8a9e\u306eet(=and)\u3060\u3068\u89e3\u91c8\u3057\u3066\u308b) # * -v: \u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u542b\u3080\u884c\u3092\u6392\u9664(\u7531\u6765: verbose??) # \u3069\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u5206\u304b\u3089\u306a\u3044\u3051\u3069\u3001Hoge\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u3067\u3001piyo\u3068\u3044\u3046\u6587\u5b57\u5217\u3092\u542b\u3093\u3067\u3044\u308b\u90e8\u5206\u3068\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u77e5\u308a\u305f\u3044 find ~/Hoge -name '*.txt' | xargs grep piyo $ pgrep -f vagrant # \u59cb\u672b $ pkill -f vagrant # \u30b7\u30b0\u30ca\u30eb\u3092\u6307\u5b9a $ pkill -SIGKILL -f vagrant https://qiita.com/uraura/items/12ff6112fd392f1be424","title":"grep"},{"location":"linux_command/#find","text":"1 2 3 4 5 6 7 8 9 10 # find dir1 -type f: dir1\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type f -name \"*.js\": dir1\u4ee5\u4e0b\u306ejs\u30d5\u30a1\u30a4\u30eb\u306e\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type d: dir1\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4e00\u89a7\u3092\u8868\u793a # * ls\u3068\u9055\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u304c\u51fa\u529b\u3055\u308c\u308b\u305f\u3081\u3001find xxx | xargs rm -rf \u307f\u305f\u3044\u306b\u4e00\u62ec\u64cd\u4f5c\u306b\u5411\u3044\u3066\u3044\u308b $ find src/ -type f find . -name '*.php' find . -name '???.txt' find . -name \"*.jpg\" | wc -l https://uguisu.skr.jp/Windows/find_xargs2.html","title":"find"},{"location":"linux_command/#sed","text":"1 2 3 4 5 6 7 # cat file1 | sed 's/BEFORE/AFTER/g': file1\u4e2d\u306eBEFORE\u3092AFTER\u306b\u4e00\u62ec\u7f6e\u63db # * s/BEFORE/AFTER/g: BEFORE\u3092AFTER\u306b\u7f6e\u63db(\u7531\u6765: substitute\u3068global?) #\u30b5\u30d6\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u542b\u3081\u305f\u6587\u5b57\u5217\u306e\u4e00\u62ec\u5909\u63db find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} \\; find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} + find ./ -type f | xargs sed -i \"s/hoge/fuga/g\"","title":"sed"},{"location":"linux_command/#xargs","text":"1 2 3 4 5 6 7 8 9 10 11 12 # cmd1 | xargs cmd2: cmd1\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3068\u3057\u3066\u53d7\u3051\u53d6\u3063\u3066\u3001cmd2\u3092\u5b9f\u884c find . -name \"*.log\" | xargs rm -fv find TARGET -type d -empty | xargs rm -r find . -name \"*.log\" | xargs -i cp {} /tmp/. #\u6587\u5b57\u5217\u5909\u63db grep -rl 'hogehoge' ./* | xargs perl -i -pe \"s/hogehoge/fugafuga/g\" #100\u30d5\u30a1\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3093\u3067\u30b3\u30d4\u30fc\uff0e find /some/dir -type f -name \"*.jpg\" | shuf -n 100 | xargs cp -vt /target/dir/ #\u691c\u7d22\u3057\u3066\u898b\u3064\u304b\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5 find . -type f -print0 | xargs -0 mv -t /var/tmp/ https://uguisu.skr.jp/Windows/find_xargs2.html","title":"xargs"},{"location":"linux_command/#_5","text":"https://hibiki-press.tech/dev-env/redirect_pipline/1571 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ echo \"4 ddd DDD\" >> file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u8ffd\u8a18) $ cat file1.txt $ echo \"4 ddd DDD\" > file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u4e0a\u66f8\u304d) #Python\u30b9\u30af\u30ea\u30d7\u30c8\u3078\u306e\u5165\u529b\u3092\u30d5\u30a1\u30a4\u30ebinput_data\u3078\u5909\u66f4\u3057\u3001 #\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30ebresult01\u306b\u51fa\u529b $ python3 sample02.py < input_data > result01 \u30a8\u30e9\u30fc\u51fa\u529b\u3082\u66f8\u304f python test.py & > output #sample02.py\u306e\u5b9f\u884c\u7d50\u679c\u3068\u30a8\u30e9\u30fc\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30ebresult02\u306b\u51fa\u529b $ python3 sample02.py > result02 2 > & 1","title":"&lt;, \uff06&gt; , &gt;&gt; (\u30ea\u30c0\u30a4\u30ec\u30af\u30c8)"},{"location":"linux_command/#_6","text":"","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"linux_command/#echo","text":"1 2 3 # echo abc: \u6587\u5b57\u5217abc\u3092\u51fa\u529b # echo $PATH: \u74b0\u5883\u5909\u6570PATH\u3092\u51fa\u529b # print\u3068\u4e00\u7dd2","title":"echo"},{"location":"linux_command/#env","text":"1 2 # env | less: \u74b0\u5883\u5909\u6570\u3092\u78ba\u8a8d # * env\u3060\u3051\u3067\u3082\u898b\u308c\u308b\u304c\u3001\u74b0\u5883\u5909\u6570\u304c\u591a\u3044\u5834\u5408\u898b\u5207\u308c\u3066\u3057\u307e\u3046\u305f\u3081less\u3067\u78ba\u8a8d","title":"env"},{"location":"linux_command/#which","text":"1 # which cmd: cmd\u306e\u5b9f\u4f53\u304c\u7f6e\u304b\u308c\u3066\u3044\u308b\u5834\u6240\u3092\u8868\u793a","title":"which"},{"location":"linux_command/#source","text":"1 2 3 # source ~/.bashrc: .bashrc\u3092\u518d\u8aad\u307f\u8fbc\u307f # . ~/.bashrc: \u2191\u3068\u540c\u3058(.\u306fsource\u306e\u30a8\u30a4\u30ea\u30a2\u30b9) # * \u30b7\u30a7\u30eb\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u5909\u66f4\u3057\u305f\u5f8c\u306e\u518d\u8aad\u307f\u8fbc\u307f\u3067\u4f7f\u3046\u30b1\u30fc\u30b9\u304c100%","title":"source"},{"location":"linux_command/#chmod","text":"1 2 3 chmod 755 *.sh #sh\u30d5\u30a1\u30a4\u30eb\u306b\u5b9f\u884c\u6a29\u9650\u3092\u4ed8\u4e0e chmod 644 *.js #js\u30d5\u30a1\u30a4\u30eb\u3092\u666e\u901a\u306b\u8aad\u307f\u66f8\u304d\u3067\u304d\u308b\u8a2d\u5b9a\u306b\u3059\u308b chmod ugo+rwx -R /*","title":"chmod"},{"location":"linux_command/#os","text":"","title":"OS \u95a2\u9023"},{"location":"linux_command/#df","text":"1 2 # df -h: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # df: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u8868","title":"df"},{"location":"linux_command/#du","text":"1 2 3 4 5 # du -h: \u5404\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # \u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u8868\u793a\uff0e\u6df1\u30551 du -h -d1 . du -h -d 1 | sort -h","title":"du"},{"location":"linux_command/#free","text":"1 free -h #\u30e1\u30e2\u30ea\u4f7f\u7528\u72b6\u6cc1\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable)","title":"free"},{"location":"linux_command/#top-ps-kill","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 top #CPU\u3084\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u72b6\u6cc1\u3092\u78ba\u8a8d # * \u30c7\u30d5\u30a9\u30eb\u30c8\u3060\u3068CPU\u4f7f\u7528\u7387\u306e\u591a\u3044\u30d7\u30ed\u30bb\u30b9\u304c\u4e0a\u306b\u6765\u308b # * %CPU\u304cCPU\u4f7f\u7528\u7387\u3002\u3069\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u9ad8\u8ca0\u8377\u304b\u3092\u78ba\u8a8d\u3067\u304d\u308b\u3002 ps -ef #\u5168\u3066\u306e\u30d7\u30ed\u30bb\u30b9\u306e\u8a73\u7d30\u306a\u60c5\u5831\u3092\u898b\u308b(\u7531\u6765: every, full) # * \u7528\u90141: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u304c\u751f\u304d\u3066\u308b\u304b\u3069\u3046\u304b\u30c1\u30a7\u30c3\u30af (web\u30b5\u30fc\u30d0\u8d77\u52d5\u3057\u3066\u308b?) # * \u7528\u90142: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u306ePID(\u30d7\u30ed\u30bb\u30b9ID)\u3092\u30c1\u30a7\u30c3\u30af -> kill ${PID} ps aux # kill 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u505c\u6b62\u3055\u305b\u308b(SIGTERM\u3092\u9001\u308b) # kill -9 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u554f\u7b54\u7121\u7528\u3067\u6bba\u3059(9\u306fSIGKILL\u306e\u30b7\u30b0\u30ca\u30eb\u756a\u53f7) # kill -KILL 123: -9\u3068\u540c\u3058 kill -9 [ \u756a\u53f7 ] # pkill process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u7d42\u4e86\u3055\u305b\u308b # pkill -9 process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u554f\u7b54\u7121\u7528\u3067\u7d42\u4e86\u3055\u305b\u308b","title":"top, ps, kill"},{"location":"linux_command/#_7","text":"1 2 3 4 5 6 7 8 cmd1 #cmd1\u3092\u30d5\u30a9\u30a2\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c cmd1 & #cmd1\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c #\u30d5\u30a9\u30a2\u30b0\u30e9\u30a6\u30f3\u30c9 \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u306b\u5207\u308a\u66ff\u3048 jobs #job\u756a\u53f7\u3092\u8abf\u3079\u308b fg %1 bg %1 \u306b\u5207\u308a\u66ff\u3048 # * \u91cd\u305f\u3044\u30d0\u30c3\u30c1\u51e6\u7406\u3084\u3001\u4e00\u6642\u7684\u306bweb\u30b5\u30fc\u30d0\u3092\u52d5\u304b\u3057\u305f\u3044\u3068\u304d\u306f\u3001 # \u30b3\u30de\u30f3\u30c9\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c\u3059\u308b\u3068\u4fbf\u5229","title":"\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c"},{"location":"linux_command/#_8","text":"1 2 3 4 # cmd1 && cmd2: cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # cmd1 || cmd2: cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # * \u7528\u90141: \u30ef\u30f3\u30e9\u30a4\u30ca\u30fc\u3067\u3061\u3087\u3063\u3068\u3057\u305f\u9010\u6b21\u51e6\u7406\u3092\u66f8\u304f # * \u7528\u90142: cmd1 || echo \"error message\"","title":"&amp;&amp;, ||"},{"location":"linux_command/#_9","text":"","title":"\u30ea\u30e2\u30fc\u30c8"},{"location":"linux_command/#ssh","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -L: \u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0 ssh -L <host port>:localhost:<remote port> user@remote #https://qiita.com/wnoguchi/items/a72a042bb8159c35d056 # ECDSA521 bit ssh-keygen -t ecdsa -b 521 -C \"wnoguchi-mbp\" # Ed25519 ssh-keygen -t ed25519 -P \"\" -f serial-server.pem ssh-keygen -t ed25519 cat public_key >> ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys","title":"ssh"},{"location":"linux_command/#scp","text":"1 2 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -r : \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u79fb\u52d5","title":"scp"},{"location":"linux_command/#useful-commands","text":"","title":"useful commands"},{"location":"linux_command/#_10","text":"\u4f7f\u7528\u91cf\u304c\u591a\u3044\u9806\u306b 5 \u4ef6\u3092\u53d6\u5f97\u3059\u308b 1 du -sm ./* | sort -rn | head -5 \u5bb9\u91cf\u3092\u304f\u3063\u3066\u308b\u30d9\u30b9\u30c8 100 1 du -ma | sort -rn | head -100 \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u307f\u3092\u691c\u7d22\u3059\u308b\u5834\u5408 1 du -m | sort -rn | head -100 30 \u65e5\u9593\u7de8\u96c6\u304c\u306a\u3044\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b5\u30a4\u30ba\u9806\u3067\u8868\u793a 1 find ./ -mtime +30 -prune -exec du -sh {} \\;|sort -hr \u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u8868\u793a\uff0e\u6df1\u3055 1\uff0e 1 du -h -d1 . 100 \u30d5\u30a1\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3093\u3067\u30b3\u30d4\u30fc\uff0e 1 find /some/dir -type f -name \"*.jpg\" | shuf -n 100 | xargs cp -vt /target/dir/ tgt_dir \u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u5229\u7528\u3057\u3066\u3001src_dir \u304b\u3089 dst_fir \u3078\u3068\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5 1 ls tgt_dir | while read name; do mv src_dir/${name:0:-4}* dst_dir/; done","title":"\u5bb9\u91cf\u306e\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u306e\u7279\u5b9a"},{"location":"linux_command/#tmux","text":"https://golang.hateblo.jp/entry/2019/10/11/133000 https://qiita.com/nmrmsys/items/03f97f5eabec18a3a18b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # \u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux # \u540d\u524d\u3092\u3064\u3051\u3066\u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux new -s <\u30bb\u30c3\u30b7\u30e7\u30f3\u540d> # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4e00\u89a7\u8868\u793a tmux ls # \u63a5\u7d9a\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u4e00\u89a7\u8868\u793a tmux lsc # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u518d\u958b \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux a -t [ session-name ] # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u7d42\u4e86 \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux kill-session -t [ session-name ] # tmux\u5168\u4f53\u3092\u7d42\u4e86 tmux kill-server # \u305d\u306e\u4ed6\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c tmux [ command [ flags ]] ctrl-b c : \u65b0\u3057\u3044\u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u8ffd\u52a0 ctrl-b b : \u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u79fb\u52d5 ctrl-b & : \u30a2\u30af\u30c6\u30a3\u30d6\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u524a\u9664 ctrl-b % : \u5de6\u53f3\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 ctrl-b \" : \u4e0a\u4e0b\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 : ctrl-b o or \u30ab\u30fc\u30bd\u30eb\uff1a\u30da\u30a4\u30f3\u9593\u306e\u79fb\u52d5 ctrl-b x : \u30da\u30a4\u30f3\u5206\u5272\u306e\u89e3\u9664 ctrl-b d: detach","title":"tmux"},{"location":"linux_command/#gunicorn-uvicorn","text":"","title":"gunicorn, uvicorn"},{"location":"linux_shell/","text":"Shell arts \u00b6 Basic \u00b6 1 2 3 4 man # \u30b3\u30de\u30f3\u30c9\u3092\u8abf\u3079\u308b seq > /dev/null # \u5165\u529b\u3055\u308c\u305f\u6587\u5b57\u3092\u305d\u306e\u307e\u307e\u6368\u3066\u308b \u30b5\u30d6\u30b7\u30a7\u30eb \u00b6 1 2 #\u5b50\u30d7\u30ed\u30bb\u30b9\u3092\u8d77\u52d5\u3057\u3001\u30ab\u30c3\u30b3\u306a\u3044\u306e\u3082\u306e\u3092\u5b9f\u884c\u3059\u308b ( cd /etc/ ; ls *.conf ) sed \u00b6 1 2 3 echo abcdefgabc | sed 's/a/b/' # \u6587\u5b571\u3064\u3060\u3051 echo abcdefgabc | sed 's/a/b/g' #\u6587\u5b57\u5217\u5168\u90e8 echo abcdefgabc | sed 's/ab/&&/' #&\u306f\u691c\u7d22\u5bfe\u8c61\u306e\u6587\u5b57\u5217\u3092\u6307\u3059\u306e\u3067abab\u3068\u540c\u7fa9 awk \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 ## awk, grep, sed \u306a\u3069\u306fbash\u306e\u5909\u6570\u3068\u3057\u3066\u89e3\u91c8\u3055\u308c\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u306b\u30b7\u30f3\u30b0\u30eb\u30af\u30aa\u30fc\u30c8\u3067\u56f2\u3080 # grep '\u6b63\u898f\u8868\u73fe'\u3068\u540c\u7fa9 seq 5 | awk '/[24]/' # $1\u306f\u8aad\u307f\u8fbc\u3093\u3060\u884c\u306e\uff11\u5217\u76ee # \u6587\u5b57\u5217\u306f\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c8\u3067\u56f2\u3080 seq 5 | '$1%2==0' # \u30d1\u30a4\u30d7\u306f\u6a19\u6e96\u5165\u529b\u3068\u3057\u3066\u6e21\u3059\u304c\u3001xargs\u306f\u5f15\u6570\u3068\u3057\u3066\u6e21\u3059 seq 4 | mkdir ls **.sh | xargs less docker ps -a \u306e 1 \u5217\u76ee\u3060\u3051\u5207\u308a\u51fa\u3059 \u00b6 1 docker ps -a | grep Up | awk '{print $1}' 5 \u500b\u306e sleep \u306e\u30d7\u30ed\u30bb\u30b9\u304b\u3089\u3092\u53d6\u308a\u51fa\u3059 \u00b6 1 ps | awk '$4==\"sleep\"{print $1}' | sort -u | wc -l \u4e09\u9805\u6f14\u7b97\u5b50 \u00b6 1 2 3 4 # {}\u306e\u610f\u5473\u3001https://qiita.com/yohm/items/3527d517768402efbcb6 seq 5 | awk '{print $1%2 ? \"odd\":\"even\"}' # -c \u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u540c\u3058\u884c\u304c\u4f55\u500b\u9023\u7d9a\u3057\u3066\u3044\u308b\u304b\u3092\u6570\u3048\u308b seq 5 | awk '{print $1%2 ? \"odd\":\"even\"}' | sort | uniq -c 1 \u5217\u76ee\u306b pl \u3068\u66f8\u304b\u308c\u3066\u3044\u308b\u884c\u306e 2 \u5217\u76ee\u3092\u62bd\u51fa \u00b6 1 cat data.txt | awk '$1==\"pl\"' | awk '{print $2}' \u968e\u6bb5\u578b\u306e\u6a21\u69d8\u3092\u63cf\u304f \u00b6 1 2 seq 5 | awk '{for(i=1;i<$1;i++){printf \" \"};print \"x\"}' | tac seq 5 | awk '{a++;for(i=5;i>a;i--){printf \" \"}' ; print \"x\" } ' \u30ab\u30f3\u30de\u533a\u5207\u308a\u306e 2 \u5217\u76ee\u3092\u53d6\u308a\u51fa\u3059 \u00b6 1 2 echo 'aaa,\"bbb,ccc\",ddd' | \\ gawk -v FPAT = '([^,]+)|(\\\"[^\\\"]+\\\")' '{print $2}' xargs \u00b6 1 2 3 4 5 6 7 8 9 mkdir 1 3 seq 4 | xargs -n2 mv # -n\u500b\u6570 \u500b\u6570\u305a\u3064\u5f15\u6570\u3092\u6e21\u3059 seq 4 | xargs -I@ mkdir dir_@ # @\u306e\u4e2d\u306bxargs\u304c\u53d7\u3051\u53d6\u308b\u5f15\u6570\u3092\uff11\u3064\u305a\u3064\u5165\u308c\u308b seq 4 | awk '{print \"mkdir\" ($1%2 ? \"odd_\" : \"even_\")$1}' | bash even_2 even_4 odd_1 even_4 file manipulation \u00b6 png \u3092 jpg \u306b\u4e00\u62ec\u5909\u63db+\u6642\u9593\u8a08\u6e2c\uff0b\u30d7\u30ed\u30bb\u30c3\u30b5\u6570\u3067\u4e26\u5217\u51e6\u7406 \u00b6 1 2 3 time ls *.png | sed 's/\\.exe$//' | xargs -P $( nproc ) -I@ convert @.png # \\\u306f\u30a8\u30b9\u30b1\u30fc\u30d7\u6587\u5b57.\u4eca\u306f\u62e1\u5f35\u5b50\u3092\u691c\u7d22\u3057\u305f\u3044\u3002$\u306f\u7d42\u7aef\u3092\u8868\u3059\u3002 # \u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u304b\u3089.exe\u3068\u3044\u3046\u540d\u524d\u306e\u3082\u306e\u3092\u63a2\u3059 \u540d\u524d\u4e00\u62ec\u5909\u66f4+-U \u3067\u30bd\u30fc\u30c8\u77ed\u7e2e \u00b6 1 2 3 time ls -U | xargs -P $( nproc ) rename 's/^/000000/;/0\\*([0-9]{7}/$1/' ) # \u524a\u9664 grep -l '^10$' -R | xargs rm \u5909\u6570\u306e\u6271\u3044 \u00b6 1 2 3 4 5 6 7 8 a = 234 b = 678 c = $a$b ; echo $c #234678 c = ${ a : 1 : 2 } ; echo $c #0\u30b9\u30bf\u30fc\u30c8\u30011\u6587\u5b57\u76ee\u304b\u3089\u6570\u3048\u30662\u3064 #34 for/while \u6587 \u00b6 1 2 set aa bb cc for x in \" $1 \" \" $2 \" \" $3 \" ; do echo $x ; done \u7f8a\u3092\u6570\u3048\u308b \u00b6 1 2 3 4 5 6 7 8 #!/bin/bash n = 1 while [ $n -le 100 ] do echo \"\u7f8a\u304c $n\u5f15\u304d \" n = $(( n + 1 )) sleep 1 done 1 2 for n in $( seq 100 ) ; do echo \u7f8a\u304c $n\u5f15\u304d ; sleep 1 ; done seq 1 100 | xargs -I@ bash -c 'echo \u7f8a\u304c@\u5339; sleep 1' if \u6587 \u00b6 \u5076\u6570\u5947\u6570\u5224\u5b9a \u00b6 1 2 3 4 a = 0 if echo $a | grep '[02468]$' ; then echo even ; elif echo $a | grep '[123579]$' then echo odd ; else echo other ; fi # \u5225\u89e3 echo $a | awk '{print /[0-9]/ ? ($1%2 ? \"odd\" : \"even\")' : \"other\" } ' \u30d6\u30ec\u30fc\u30b9\u5c55\u958b \u00b6 1 2 3 echo { a, b } c ac bc echo { 1 ..5 } . { txt,pdf } \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9 \u00b6 1 2 3 * #\u4efb\u610f\u306e\u6587\u5b57\u5217 ? #\u4efb\u610f\u306e\u4e00\u6587\u5b57 [] #\u5185\u5074\u306e\u4e00\u6587\u5b57 Python \u30ef\u30f3\u30e9\u30a4\u30ca\u30fc \u00b6 1 seq 3 | python3 -c 'import sys, math;[print(math.sqrt(int(a))) for a in sys.stdin]' \u30d0\u30c3\u30af\u30af\u30aa\u30fc\u30c8\u3067\u56f2\u3080\u3068\u6a19\u6e96\u51fa\u529b\u3092\u5909\u6570\u306b\u5165\u529b\u3067\u304d\u308b \u00b6 1 2 A=`git rev-parse --short HEAD` echo $A \u30d5\u30a1\u30a4\u30eb\u306e UID, GID \u306e\u5909\u66f4 \u00b6 1 2 find / -uid {\u65e7ID} -print0 | xargs -0 chown {\u5bfe\u8c61\u30e6\u30fc\u30b6\u540d} find / -gid {\u65e7ID} -print0 | xargs -0 chgrp {\u5bfe\u8c61\u30b0\u30eb\u30fc\u30d7\u540d}","title":"Shell arts"},{"location":"linux_shell/#shell-arts","text":"","title":"Shell arts"},{"location":"linux_shell/#basic","text":"1 2 3 4 man # \u30b3\u30de\u30f3\u30c9\u3092\u8abf\u3079\u308b seq > /dev/null # \u5165\u529b\u3055\u308c\u305f\u6587\u5b57\u3092\u305d\u306e\u307e\u307e\u6368\u3066\u308b","title":"Basic"},{"location":"linux_shell/#_1","text":"1 2 #\u5b50\u30d7\u30ed\u30bb\u30b9\u3092\u8d77\u52d5\u3057\u3001\u30ab\u30c3\u30b3\u306a\u3044\u306e\u3082\u306e\u3092\u5b9f\u884c\u3059\u308b ( cd /etc/ ; ls *.conf )","title":"\u30b5\u30d6\u30b7\u30a7\u30eb"},{"location":"linux_shell/#sed","text":"1 2 3 echo abcdefgabc | sed 's/a/b/' # \u6587\u5b571\u3064\u3060\u3051 echo abcdefgabc | sed 's/a/b/g' #\u6587\u5b57\u5217\u5168\u90e8 echo abcdefgabc | sed 's/ab/&&/' #&\u306f\u691c\u7d22\u5bfe\u8c61\u306e\u6587\u5b57\u5217\u3092\u6307\u3059\u306e\u3067abab\u3068\u540c\u7fa9","title":"sed"},{"location":"linux_shell/#awk","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 ## awk, grep, sed \u306a\u3069\u306fbash\u306e\u5909\u6570\u3068\u3057\u3066\u89e3\u91c8\u3055\u308c\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u306b\u30b7\u30f3\u30b0\u30eb\u30af\u30aa\u30fc\u30c8\u3067\u56f2\u3080 # grep '\u6b63\u898f\u8868\u73fe'\u3068\u540c\u7fa9 seq 5 | awk '/[24]/' # $1\u306f\u8aad\u307f\u8fbc\u3093\u3060\u884c\u306e\uff11\u5217\u76ee # \u6587\u5b57\u5217\u306f\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c8\u3067\u56f2\u3080 seq 5 | '$1%2==0' # \u30d1\u30a4\u30d7\u306f\u6a19\u6e96\u5165\u529b\u3068\u3057\u3066\u6e21\u3059\u304c\u3001xargs\u306f\u5f15\u6570\u3068\u3057\u3066\u6e21\u3059 seq 4 | mkdir ls **.sh | xargs less","title":"awk"},{"location":"linux_shell/#docker-ps-a-1","text":"1 docker ps -a | grep Up | awk '{print $1}'","title":"docker ps -a \u306e 1 \u5217\u76ee\u3060\u3051\u5207\u308a\u51fa\u3059"},{"location":"linux_shell/#5-sleep","text":"1 ps | awk '$4==\"sleep\"{print $1}' | sort -u | wc -l","title":"5 \u500b\u306e sleep \u306e\u30d7\u30ed\u30bb\u30b9\u304b\u3089\u3092\u53d6\u308a\u51fa\u3059"},{"location":"linux_shell/#_2","text":"1 2 3 4 # {}\u306e\u610f\u5473\u3001https://qiita.com/yohm/items/3527d517768402efbcb6 seq 5 | awk '{print $1%2 ? \"odd\":\"even\"}' # -c \u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u540c\u3058\u884c\u304c\u4f55\u500b\u9023\u7d9a\u3057\u3066\u3044\u308b\u304b\u3092\u6570\u3048\u308b seq 5 | awk '{print $1%2 ? \"odd\":\"even\"}' | sort | uniq -c","title":"\u4e09\u9805\u6f14\u7b97\u5b50"},{"location":"linux_shell/#1-pl-2","text":"1 cat data.txt | awk '$1==\"pl\"' | awk '{print $2}'","title":"1 \u5217\u76ee\u306b pl \u3068\u66f8\u304b\u308c\u3066\u3044\u308b\u884c\u306e 2 \u5217\u76ee\u3092\u62bd\u51fa"},{"location":"linux_shell/#_3","text":"1 2 seq 5 | awk '{for(i=1;i<$1;i++){printf \" \"};print \"x\"}' | tac seq 5 | awk '{a++;for(i=5;i>a;i--){printf \" \"}' ; print \"x\" } '","title":"\u968e\u6bb5\u578b\u306e\u6a21\u69d8\u3092\u63cf\u304f"},{"location":"linux_shell/#2","text":"1 2 echo 'aaa,\"bbb,ccc\",ddd' | \\ gawk -v FPAT = '([^,]+)|(\\\"[^\\\"]+\\\")' '{print $2}'","title":"\u30ab\u30f3\u30de\u533a\u5207\u308a\u306e 2 \u5217\u76ee\u3092\u53d6\u308a\u51fa\u3059"},{"location":"linux_shell/#xargs","text":"1 2 3 4 5 6 7 8 9 mkdir 1 3 seq 4 | xargs -n2 mv # -n\u500b\u6570 \u500b\u6570\u305a\u3064\u5f15\u6570\u3092\u6e21\u3059 seq 4 | xargs -I@ mkdir dir_@ # @\u306e\u4e2d\u306bxargs\u304c\u53d7\u3051\u53d6\u308b\u5f15\u6570\u3092\uff11\u3064\u305a\u3064\u5165\u308c\u308b seq 4 | awk '{print \"mkdir\" ($1%2 ? \"odd_\" : \"even_\")$1}' | bash even_2 even_4 odd_1 even_4","title":"xargs"},{"location":"linux_shell/#file-manipulation","text":"","title":"file manipulation"},{"location":"linux_shell/#png-jpg","text":"1 2 3 time ls *.png | sed 's/\\.exe$//' | xargs -P $( nproc ) -I@ convert @.png # \\\u306f\u30a8\u30b9\u30b1\u30fc\u30d7\u6587\u5b57.\u4eca\u306f\u62e1\u5f35\u5b50\u3092\u691c\u7d22\u3057\u305f\u3044\u3002$\u306f\u7d42\u7aef\u3092\u8868\u3059\u3002 # \u30d5\u30a1\u30a4\u30eb\u306e\u4e2d\u304b\u3089.exe\u3068\u3044\u3046\u540d\u524d\u306e\u3082\u306e\u3092\u63a2\u3059","title":"png \u3092 jpg \u306b\u4e00\u62ec\u5909\u63db+\u6642\u9593\u8a08\u6e2c\uff0b\u30d7\u30ed\u30bb\u30c3\u30b5\u6570\u3067\u4e26\u5217\u51e6\u7406"},{"location":"linux_shell/#-u","text":"1 2 3 time ls -U | xargs -P $( nproc ) rename 's/^/000000/;/0\\*([0-9]{7}/$1/' ) # \u524a\u9664 grep -l '^10$' -R | xargs rm","title":"\u540d\u524d\u4e00\u62ec\u5909\u66f4+-U \u3067\u30bd\u30fc\u30c8\u77ed\u7e2e"},{"location":"linux_shell/#_4","text":"1 2 3 4 5 6 7 8 a = 234 b = 678 c = $a$b ; echo $c #234678 c = ${ a : 1 : 2 } ; echo $c #0\u30b9\u30bf\u30fc\u30c8\u30011\u6587\u5b57\u76ee\u304b\u3089\u6570\u3048\u30662\u3064 #34","title":"\u5909\u6570\u306e\u6271\u3044"},{"location":"linux_shell/#forwhile","text":"1 2 set aa bb cc for x in \" $1 \" \" $2 \" \" $3 \" ; do echo $x ; done","title":"for/while \u6587"},{"location":"linux_shell/#_5","text":"1 2 3 4 5 6 7 8 #!/bin/bash n = 1 while [ $n -le 100 ] do echo \"\u7f8a\u304c $n\u5f15\u304d \" n = $(( n + 1 )) sleep 1 done 1 2 for n in $( seq 100 ) ; do echo \u7f8a\u304c $n\u5f15\u304d ; sleep 1 ; done seq 1 100 | xargs -I@ bash -c 'echo \u7f8a\u304c@\u5339; sleep 1'","title":"\u7f8a\u3092\u6570\u3048\u308b"},{"location":"linux_shell/#if","text":"","title":"if \u6587"},{"location":"linux_shell/#_6","text":"1 2 3 4 a = 0 if echo $a | grep '[02468]$' ; then echo even ; elif echo $a | grep '[123579]$' then echo odd ; else echo other ; fi # \u5225\u89e3 echo $a | awk '{print /[0-9]/ ? ($1%2 ? \"odd\" : \"even\")' : \"other\" } '","title":"\u5076\u6570\u5947\u6570\u5224\u5b9a"},{"location":"linux_shell/#_7","text":"1 2 3 echo { a, b } c ac bc echo { 1 ..5 } . { txt,pdf }","title":"\u30d6\u30ec\u30fc\u30b9\u5c55\u958b"},{"location":"linux_shell/#_8","text":"1 2 3 * #\u4efb\u610f\u306e\u6587\u5b57\u5217 ? #\u4efb\u610f\u306e\u4e00\u6587\u5b57 [] #\u5185\u5074\u306e\u4e00\u6587\u5b57","title":"\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9"},{"location":"linux_shell/#python","text":"1 seq 3 | python3 -c 'import sys, math;[print(math.sqrt(int(a))) for a in sys.stdin]'","title":"Python \u30ef\u30f3\u30e9\u30a4\u30ca\u30fc"},{"location":"linux_shell/#_9","text":"1 2 A=`git rev-parse --short HEAD` echo $A","title":"\u30d0\u30c3\u30af\u30af\u30aa\u30fc\u30c8\u3067\u56f2\u3080\u3068\u6a19\u6e96\u51fa\u529b\u3092\u5909\u6570\u306b\u5165\u529b\u3067\u304d\u308b"},{"location":"linux_shell/#uid-gid","text":"1 2 find / -uid {\u65e7ID} -print0 | xargs -0 chown {\u5bfe\u8c61\u30e6\u30fc\u30b6\u540d} find / -gid {\u65e7ID} -print0 | xargs -0 chgrp {\u5bfe\u8c61\u30b0\u30eb\u30fc\u30d7\u540d}","title":"\u30d5\u30a1\u30a4\u30eb\u306e UID, GID \u306e\u5909\u66f4"},{"location":"ml_basic/","text":"\u30b9\u30e2\u30fc\u30eb\u30c7\u30fc\u30bf\u89e3\u6790 ML \u57fa\u790e\u672c x3 \u6b21\u5143\u524a\u6e1b U-MAP t-SNE arcface https://www.kaggle.com/code/cla0709/dimensionality-reduction-pca-lda-t-sne-umap/notebook https://blog.brainpad.co.jp/entry/2022/03/09/160000 https://www.kaggle.com/competitions/shopee-product-matching/discussion/226279 \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u306e\u53ef\u8996\u5316\u30c4\u30fc\u30eb XGBoost \u306e\u53ef\u8996\u5316\u30c4\u30fc\u30eb \u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0 MLflow https://cpp-learning.com/pca-umap/","title":"Ml basic"},{"location":"mlflow/","text":"mlflow server -h 0.0.0.0 -p 5000","title":"Mlflow"},{"location":"mloops/","text":"MLOps \u672c x2 https://masatakashiwagi.github.io/mlops-practices/knowledge/ https://adventar.org/calendars/5089","title":"Mloops"},{"location":"mlutils/","text":"\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3 https://amateur-engineer-blog.com/kaggle-cv-template-lightgbm/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def randomname ( n ): return \"\" . join ( random . choices ( string . ascii_letters + string . digits , k = n )) JOBNAME = datetime . datetime . now () . strftime ( \"%Y%m %d %H%M\" ) + \"_\" + randomname ( 10 ) with open ( file = \"./src/conf/config.yaml\" , mode = \"r\" , encoding = \"utf-8\" , ) as f : obj = yaml . safe_load ( f ) obj [ \"hydra\" ][ \"run\" ][ \"dir\" ] = \"./logs/${\" + \"now:\" + f \" { JOBNAME } \" + \"}\" with open ( \"./src/conf/config.yaml\" , \"w\" ) as f : yaml . safe_dump ( obj , f ) def seed_everything ( seed = 2021 ): random . seed ( seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False def git_commits ( rand ): def func_decorator ( my_func ): repo = git . Repo ( \"/work\" ) repo . config_writer () . set_value ( \"user\" , \"name\" , \"yseeker\" ) . release () repo . config_writer () . set_value ( \"user\" , \"email\" , \"maxwell8313@gmail.com\" ) . release () # os.system(\"git config --global user.name \\\"yseeker\\\"\") # os.system(\"git config --global user.email \\\"maxwell8313@gmail.com\\\"\") repo . git . diff ( \"HEAD\" ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _running\" ) repo . git . push ( \"origin\" , \"main\" ) def decorator_wrapper ( * args , ** kwargs ): my_func ( * args , ** kwargs ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _done\" ) repo . git . push ( \"origin\" , \"main\" ) return decorator_wrapper return func_decorator stacking \u00b6 1 2 3 4 5 6 7 from sklearn.ensemble import StackingClassifier estimators = [( 'logreg' , LogisticRegression ( random_state = 0 )), ( 'lsvc' , LinearSVC ( random_state = 0 ))] stacking_clf = StackingClassifier ( estimators = estimators , final_estimator = DecisionTreeClassifier ( random_state = 0 )) clf . fit ( X_train , y_train ) . score ( X_test , y_test ) print ( \"prediction score:\" , stacking_clf . score ( X_test , y_test )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 import xgboost as xgb class Model1Xgb : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): xgb_params = { 'objective' : 'reg:squarederror' , #binary:logistic #multi:softprob, 'random_state' : 10 , #'eval_metric': 'rmse' } dtrain = xgb . DMatrix ( tr_x , label = tr_y ) dvalid = xgb . DMatrix ( va_x , label = va_y ) evals = [( dtrain , 'train' ), ( dvalid , 'eval' )] self . model = xgb . train ( xgb_params , dtrain , num_boost_round = 10000 , early_stopping_rounds = 50 , evals = evals ) def predict ( self , x ): data = xgb . DMatrix ( x ) pred = self . model . predict ( data ) return pred import lightgbm as lgb class Model1lgb : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): lgb_params = { 'objective' : 'rmse' , 'random_state' : 10 , 'metric' : 'rmse' } lgb_train = lgb . Dataset ( tr_x , label = tr_y ) lgb_eval = lgb . Dataset ( va_x , label = va_y , reference = lgb_train ) self . model = lgb . train ( lgb_params , lgb_train , valid_sets = lgb_eval , num_boost_round = 10000 , early_stopping_rounds = 50 ) def predict ( self , x ): pred = self . model . predict ( x , num_iteration = self . model . best_iteration ) return pred import catboost class Model1catboost : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): #https://catboost.ai/docs/concepts/python-reference_catboostregressor.html #catb = catboost.CatBoostClassifier( catb = catboost . CatBoostRegressor ( iterations = 10000 , use_best_model = True , random_seed = 10 , l2_leaf_reg = 3 , depth = 6 , loss_function = \"RMSE\" , #\"CrossEntropy\", #eval_metric = \"RMSE\", #'AUC', #classes_coun=3 ) self . model = catb . fit ( tr_x , tr_y , eval_set = ( va_x , va_y ), early_stopping_rounds = 50 ) print ( self . model . score ( va_x , va_y )) def predict ( self , x ): pred = self . model . predict ( x ) return pred rom keras . models import Sequential from keras.callbacks import EarlyStopping from keras.layers import Dense , Dropout class Model1NN : def __init__ ( self ): self . model = None self . scaler = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) batch_size = 128 epochs = 10000 tr_x = self . scaler . transform ( tr_x ) va_x = self . scaler . transform ( va_x ) early_stopping = EarlyStopping ( monitor = 'val_loss' , min_delta = 0.0 , patience = 20 , ) model = Sequential () model . add ( Dense ( 32 , activation = 'relu' , input_shape = ( tr_x . shape [ 1 ],))) model . add ( Dropout ( 0.5 )) model . add ( Dense ( 32 , activation = 'relu' )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'mean_squared_error' , #'categorical_crossentropy',#categorical_crossentropy optimizer = 'adam' ) history = model . fit ( tr_x , tr_y , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( va_x , va_y ), callbacks = [ early_stopping ]) self . model = model def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict_proba ( x ) . reshape ( - 1 ) return pred from sklearn.svm import LinearSVR class Model1LinearSVR : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) #params = {\"C\":np.logspace(0,1,params_cnt),\"epsilon\":np.logspace(-1,1,params_cnt)} self . model = LinearSVR ( max_iter = 1000 , random_state = 10 , C = 1.0 , #\u640d\u5931\u306e\u4fc2\u6570\uff08\u6b63\u5247\u5316\u4fc2\u6570\u306e\u9006\u6570\uff09 epsilon = 5.0 ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.svm import SVR class Model1KernelSVR : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) #params = {\"kernel\":['rbf'],\"C\":np.logspace(0,1,params_cnt), \"epsilon\":np.logspace(-1,1,params_cnt)} self . model = SVR ( kernel = 'rbf' , gamma = 'auto' , max_iter = 1000 , C = 1.0 , #\u640d\u5931\u306e\u4fc2\u6570\uff08\u6b63\u5247\u5316\u4fc2\u6570\u306e\u9006\u6570\uff09 epsilon = 5.0 ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import Lasso class Model1Lasso : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): ''' (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html ''' self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = Lasso ( alpha = 1 , #L1\u4fc2\u6570 fit_intercept = True , max_iter = 1000 , random_state = 10 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import Ridge class Model1Ridge : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): ''' |y - Xw||^2_2 + alpha * ||w||^2_2 #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html ''' self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = Ridge ( alpha = 1 , #L2\u4fc2\u6570 max_iter = 1000 , random_state = 10 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import ElasticNet class Model1ElasticNet : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): '''1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2 ref) https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html ''' self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = ElasticNet ( alpha = 1 , #L1\u4fc2\u6570 l1_ratio = 0.5 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.ensemble import RandomForestRegressor class Model1RF : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = RandomForestRegressor ( max_depth = 5 , n_estimators = 100 , random_state = 10 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.neighbors import KNeighborsRegressor class Model1KNN : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) #params = {\"kernel\":['rbf'],\"C\":np.logspace(0,1,params_cnt), \"epsilon\":np.logspace(-1,1,params_cnt)} self . model = KNeighborsRegressor ( n_neighbors = 5 , #weights='uniform' ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import LinearRegression class Model2Linear : def __init__ ( self ): self . model = None self . scaler = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = LinearRegression () self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.model_selection import KFold from sklearn.preprocessing import StandardScaler def predict_cv ( model , train_x , train_y , test_x ): preds = [] preds_test = [] va_idxes = [] kf = KFold ( n_splits = 4 , shuffle = True , random_state = 10 ) # \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u5b66\u7fd2\u30fb\u4e88\u6e2c\u3092\u884c\u3044\u3001\u4e88\u6e2c\u5024\u3068\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4fdd\u5b58\u3059\u308b for i , ( tr_idx , va_idx ) in enumerate ( kf . split ( train_x )): tr_x , va_x = train_x . iloc [ tr_idx ], train_x . iloc [ va_idx ] tr_y , va_y = train_y . iloc [ tr_idx ], train_y . iloc [ va_idx ] model . fit ( tr_x , tr_y , va_x , va_y ) pred = model . predict ( va_x ) preds . append ( pred ) pred_test = model . predict ( test_x ) preds_test . append ( pred_test ) va_idxes . append ( va_idx ) # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u3092\u9023\u7d50\u3057\u3001\u305d\u306e\u5f8c\u5143\u306e\u9806\u5e8f\u306b\u4e26\u3079\u76f4\u3059 va_idxes = np . concatenate ( va_idxes ) preds = np . concatenate ( preds , axis = 0 ) order = np . argsort ( va_idxes ) pred_train = preds [ order ] # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u3068\u308b preds_test = np . mean ( preds_test , axis = 0 ) return pred_train , preds_test model_1a = Model1Xgb () pred_train_1a , pred_test_1a = predict_cv ( model_1a , df_trainval , y_trainval , df_test ) model_1b = Model1lgb () pred_train_1b , pred_test_1b = predict_cv ( model_1b , df_trainval , y_trainval , df_test ) model_1c = Model1NN () pred_train_1c , pred_test_1c = predict_cv ( model_1c , df_trainval , y_trainval , df_test ) model_1d = Model1LinearSVR () pred_train_1d , pred_test_1d = predict_cv ( model_1d , df_trainval , y_trainval , df_test ) model_1e = Model1KernelSVR () pred_train_1e , pred_test_1e = predict_cv ( model_1e , df_trainval , y_trainval , df_test ) model_1f = Model1catboost () pred_train_1f , pred_test_1f = predict_cv ( model_1f , df_trainval , y_trainval , df_test ) model_1g = Model1KNN () pred_train_1g , pred_test_1g = predict_cv ( model_1g , df_trainval , y_trainval , df_test ) model_1h = Model1Lasso () pred_train_1h , pred_test_1h = predict_cv ( model_1h , df_trainval , y_trainval , df_test ) model_1i = Model1Ridge () pred_train_1i , pred_test_1i = predict_cv ( model_1i , df_trainval , y_trainval , df_test ) model_1j = Model1ElasticNet () pred_train_1j , pred_test_1j = predict_cv ( model_1j , df_trainval , y_trainval , df_test ) model_1k = Model1RF () pred_train_1k , pred_test_1k = predict_cv ( model_1k , df_trainval , y_trainval , df_test ) from sklearn.metrics import mean_absolute_error ''' #Calculate RMSE from sklearn.metrics import mean_square_error def rmse(y_true,y_pred): rmse = np.sqrt(mean_squared_error(y_true,y_pred)) print('rmse',rmse) return rmse ''' ''' \u4ee5\u4e0b\u3067\u3001\uff11\u5c64\u76ee\u306e\u5404\u4e88\u6e2c\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u3092\u78ba\u8a8d\u3057\u3066\u3044\u308b\u3002 \u4eca\u56de\u306f\u3001mean_absolute_error(MAE)\u3067\u7cbe\u5ea6\u78ba\u8a8d ''' print ( f 'a LGBM mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1a ) : .4f } ' ) print ( f 'b XGBoostmean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1b ) : .4f } ' ) print ( f 'c MLP mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1c ) : .4f } ' ) print ( f 'd LinearSVR mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1d ) : .4f } ' ) print ( f 'e KernelSVR mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1e ) : .4f } ' ) print ( f 'f Catboost mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1f ) : .4f } ' ) print ( f 'g KNN mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1g ) : .4f } ' ) print ( f 'h Lasso mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1h ) : .4f } ' ) print ( f 'i Ridge mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1i ) : .4f } ' ) print ( f 'j ElasticNet mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1j ) : .4f } ' ) print ( f 'k RandomForest mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1k ) : .4f } ' ) ''' \u4ee5\u4e0b\u3067\u306f\u3001\uff11\u5c64\u76ee\u306e\u4e88\u6e2c\u5024\u3092\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u307e\u3068\u3081\u3066\u3044\u308b\u3002 \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u307e\u3068\u3081\u3066\u3001\uff12\u5c64\u76ee\u306e\u7279\u5fb4\u91cf\u3068\u3057\u3066\u3001\u307e\u3068\u3081\u3066\u3044\u308b ''' train_x_2 = pd . DataFrame ({ 'pred_1a' : pred_train_1a , 'pred_1b' : pred_train_1b , 'pred_1c' : pred_train_1c , #'pred_1d': pred_train_1d, 'pred_1e' : pred_train_1e , 'pred_1f' : pred_train_1f , 'pred_1g' : pred_train_1g , 'pred_1h' : pred_train_1h , 'pred_1i' : pred_train_1i , 'pred_1j' : pred_train_1j , 'pred_1k' : pred_train_1k , }) test_x_2 = pd . DataFrame ({ 'pred_1a' : pred_test_1a , 'pred_1b' : pred_test_1b , 'pred_1c' : pred_test_1c , #'pred_1d': pred_test_1d, 'pred_1e' : pred_test_1e , 'pred_1f' : pred_test_1f , 'pred_1g' : pred_test_1g , 'pred_1h' : pred_test_1h , 'pred_1i' : pred_test_1i , 'pred_1j' : pred_test_1j , 'pred_1k' : pred_test_1k }) model2 = Model2Linear () pred_train_2 , pred_test_2 = predict_cv ( model2 , train_x_2 , y_trainval , test_x_2 ) #\uff12\u5c64\u306e\u30b9\u30bf\u30c3\u30ad\u30f3\u30b0\u5f8c\u306e\u3001\u7cbe\u5ea6\u3092\u78ba\u8a8d\u3002 print ( f 'mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_2 ) : .4f } ' )","title":"Mlutils"},{"location":"mlutils/#stacking","text":"1 2 3 4 5 6 7 from sklearn.ensemble import StackingClassifier estimators = [( 'logreg' , LogisticRegression ( random_state = 0 )), ( 'lsvc' , LinearSVC ( random_state = 0 ))] stacking_clf = StackingClassifier ( estimators = estimators , final_estimator = DecisionTreeClassifier ( random_state = 0 )) clf . fit ( X_train , y_train ) . score ( X_test , y_test ) print ( \"prediction score:\" , stacking_clf . score ( X_test , y_test )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 import xgboost as xgb class Model1Xgb : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): xgb_params = { 'objective' : 'reg:squarederror' , #binary:logistic #multi:softprob, 'random_state' : 10 , #'eval_metric': 'rmse' } dtrain = xgb . DMatrix ( tr_x , label = tr_y ) dvalid = xgb . DMatrix ( va_x , label = va_y ) evals = [( dtrain , 'train' ), ( dvalid , 'eval' )] self . model = xgb . train ( xgb_params , dtrain , num_boost_round = 10000 , early_stopping_rounds = 50 , evals = evals ) def predict ( self , x ): data = xgb . DMatrix ( x ) pred = self . model . predict ( data ) return pred import lightgbm as lgb class Model1lgb : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): lgb_params = { 'objective' : 'rmse' , 'random_state' : 10 , 'metric' : 'rmse' } lgb_train = lgb . Dataset ( tr_x , label = tr_y ) lgb_eval = lgb . Dataset ( va_x , label = va_y , reference = lgb_train ) self . model = lgb . train ( lgb_params , lgb_train , valid_sets = lgb_eval , num_boost_round = 10000 , early_stopping_rounds = 50 ) def predict ( self , x ): pred = self . model . predict ( x , num_iteration = self . model . best_iteration ) return pred import catboost class Model1catboost : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): #https://catboost.ai/docs/concepts/python-reference_catboostregressor.html #catb = catboost.CatBoostClassifier( catb = catboost . CatBoostRegressor ( iterations = 10000 , use_best_model = True , random_seed = 10 , l2_leaf_reg = 3 , depth = 6 , loss_function = \"RMSE\" , #\"CrossEntropy\", #eval_metric = \"RMSE\", #'AUC', #classes_coun=3 ) self . model = catb . fit ( tr_x , tr_y , eval_set = ( va_x , va_y ), early_stopping_rounds = 50 ) print ( self . model . score ( va_x , va_y )) def predict ( self , x ): pred = self . model . predict ( x ) return pred rom keras . models import Sequential from keras.callbacks import EarlyStopping from keras.layers import Dense , Dropout class Model1NN : def __init__ ( self ): self . model = None self . scaler = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) batch_size = 128 epochs = 10000 tr_x = self . scaler . transform ( tr_x ) va_x = self . scaler . transform ( va_x ) early_stopping = EarlyStopping ( monitor = 'val_loss' , min_delta = 0.0 , patience = 20 , ) model = Sequential () model . add ( Dense ( 32 , activation = 'relu' , input_shape = ( tr_x . shape [ 1 ],))) model . add ( Dropout ( 0.5 )) model . add ( Dense ( 32 , activation = 'relu' )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( 1 , activation = 'sigmoid' )) model . compile ( loss = 'mean_squared_error' , #'categorical_crossentropy',#categorical_crossentropy optimizer = 'adam' ) history = model . fit ( tr_x , tr_y , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( va_x , va_y ), callbacks = [ early_stopping ]) self . model = model def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict_proba ( x ) . reshape ( - 1 ) return pred from sklearn.svm import LinearSVR class Model1LinearSVR : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) #params = {\"C\":np.logspace(0,1,params_cnt),\"epsilon\":np.logspace(-1,1,params_cnt)} self . model = LinearSVR ( max_iter = 1000 , random_state = 10 , C = 1.0 , #\u640d\u5931\u306e\u4fc2\u6570\uff08\u6b63\u5247\u5316\u4fc2\u6570\u306e\u9006\u6570\uff09 epsilon = 5.0 ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.svm import SVR class Model1KernelSVR : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) #params = {\"kernel\":['rbf'],\"C\":np.logspace(0,1,params_cnt), \"epsilon\":np.logspace(-1,1,params_cnt)} self . model = SVR ( kernel = 'rbf' , gamma = 'auto' , max_iter = 1000 , C = 1.0 , #\u640d\u5931\u306e\u4fc2\u6570\uff08\u6b63\u5247\u5316\u4fc2\u6570\u306e\u9006\u6570\uff09 epsilon = 5.0 ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import Lasso class Model1Lasso : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): ''' (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html ''' self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = Lasso ( alpha = 1 , #L1\u4fc2\u6570 fit_intercept = True , max_iter = 1000 , random_state = 10 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import Ridge class Model1Ridge : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): ''' |y - Xw||^2_2 + alpha * ||w||^2_2 #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html ''' self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = Ridge ( alpha = 1 , #L2\u4fc2\u6570 max_iter = 1000 , random_state = 10 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import ElasticNet class Model1ElasticNet : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): '''1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2 ref) https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html ''' self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = ElasticNet ( alpha = 1 , #L1\u4fc2\u6570 l1_ratio = 0.5 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.ensemble import RandomForestRegressor class Model1RF : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = RandomForestRegressor ( max_depth = 5 , n_estimators = 100 , random_state = 10 , ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.neighbors import KNeighborsRegressor class Model1KNN : def __init__ ( self ): self . model = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) #params = {\"kernel\":['rbf'],\"C\":np.logspace(0,1,params_cnt), \"epsilon\":np.logspace(-1,1,params_cnt)} self . model = KNeighborsRegressor ( n_neighbors = 5 , #weights='uniform' ) self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.linear_model import LinearRegression class Model2Linear : def __init__ ( self ): self . model = None self . scaler = None def fit ( self , tr_x , tr_y , va_x , va_y ): self . scaler = StandardScaler () self . scaler . fit ( tr_x ) tr_x = self . scaler . transform ( tr_x ) self . model = LinearRegression () self . model . fit ( tr_x , tr_y ) def predict ( self , x ): x = self . scaler . transform ( x ) pred = self . model . predict ( x ) return pred from sklearn.model_selection import KFold from sklearn.preprocessing import StandardScaler def predict_cv ( model , train_x , train_y , test_x ): preds = [] preds_test = [] va_idxes = [] kf = KFold ( n_splits = 4 , shuffle = True , random_state = 10 ) # \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3067\u5b66\u7fd2\u30fb\u4e88\u6e2c\u3092\u884c\u3044\u3001\u4e88\u6e2c\u5024\u3068\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4fdd\u5b58\u3059\u308b for i , ( tr_idx , va_idx ) in enumerate ( kf . split ( train_x )): tr_x , va_x = train_x . iloc [ tr_idx ], train_x . iloc [ va_idx ] tr_y , va_y = train_y . iloc [ tr_idx ], train_y . iloc [ va_idx ] model . fit ( tr_x , tr_y , va_x , va_y ) pred = model . predict ( va_x ) preds . append ( pred ) pred_test = model . predict ( test_x ) preds_test . append ( pred_test ) va_idxes . append ( va_idx ) # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u3092\u9023\u7d50\u3057\u3001\u305d\u306e\u5f8c\u5143\u306e\u9806\u5e8f\u306b\u4e26\u3079\u76f4\u3059 va_idxes = np . concatenate ( va_idxes ) preds = np . concatenate ( preds , axis = 0 ) order = np . argsort ( va_idxes ) pred_train = preds [ order ] # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u3068\u308b preds_test = np . mean ( preds_test , axis = 0 ) return pred_train , preds_test model_1a = Model1Xgb () pred_train_1a , pred_test_1a = predict_cv ( model_1a , df_trainval , y_trainval , df_test ) model_1b = Model1lgb () pred_train_1b , pred_test_1b = predict_cv ( model_1b , df_trainval , y_trainval , df_test ) model_1c = Model1NN () pred_train_1c , pred_test_1c = predict_cv ( model_1c , df_trainval , y_trainval , df_test ) model_1d = Model1LinearSVR () pred_train_1d , pred_test_1d = predict_cv ( model_1d , df_trainval , y_trainval , df_test ) model_1e = Model1KernelSVR () pred_train_1e , pred_test_1e = predict_cv ( model_1e , df_trainval , y_trainval , df_test ) model_1f = Model1catboost () pred_train_1f , pred_test_1f = predict_cv ( model_1f , df_trainval , y_trainval , df_test ) model_1g = Model1KNN () pred_train_1g , pred_test_1g = predict_cv ( model_1g , df_trainval , y_trainval , df_test ) model_1h = Model1Lasso () pred_train_1h , pred_test_1h = predict_cv ( model_1h , df_trainval , y_trainval , df_test ) model_1i = Model1Ridge () pred_train_1i , pred_test_1i = predict_cv ( model_1i , df_trainval , y_trainval , df_test ) model_1j = Model1ElasticNet () pred_train_1j , pred_test_1j = predict_cv ( model_1j , df_trainval , y_trainval , df_test ) model_1k = Model1RF () pred_train_1k , pred_test_1k = predict_cv ( model_1k , df_trainval , y_trainval , df_test ) from sklearn.metrics import mean_absolute_error ''' #Calculate RMSE from sklearn.metrics import mean_square_error def rmse(y_true,y_pred): rmse = np.sqrt(mean_squared_error(y_true,y_pred)) print('rmse',rmse) return rmse ''' ''' \u4ee5\u4e0b\u3067\u3001\uff11\u5c64\u76ee\u306e\u5404\u4e88\u6e2c\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u3092\u78ba\u8a8d\u3057\u3066\u3044\u308b\u3002 \u4eca\u56de\u306f\u3001mean_absolute_error(MAE)\u3067\u7cbe\u5ea6\u78ba\u8a8d ''' print ( f 'a LGBM mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1a ) : .4f } ' ) print ( f 'b XGBoostmean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1b ) : .4f } ' ) print ( f 'c MLP mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1c ) : .4f } ' ) print ( f 'd LinearSVR mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1d ) : .4f } ' ) print ( f 'e KernelSVR mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1e ) : .4f } ' ) print ( f 'f Catboost mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1f ) : .4f } ' ) print ( f 'g KNN mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1g ) : .4f } ' ) print ( f 'h Lasso mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1h ) : .4f } ' ) print ( f 'i Ridge mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1i ) : .4f } ' ) print ( f 'j ElasticNet mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1j ) : .4f } ' ) print ( f 'k RandomForest mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_1k ) : .4f } ' ) ''' \u4ee5\u4e0b\u3067\u306f\u3001\uff11\u5c64\u76ee\u306e\u4e88\u6e2c\u5024\u3092\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u307e\u3068\u3081\u3066\u3044\u308b\u3002 \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u307e\u3068\u3081\u3066\u3001\uff12\u5c64\u76ee\u306e\u7279\u5fb4\u91cf\u3068\u3057\u3066\u3001\u307e\u3068\u3081\u3066\u3044\u308b ''' train_x_2 = pd . DataFrame ({ 'pred_1a' : pred_train_1a , 'pred_1b' : pred_train_1b , 'pred_1c' : pred_train_1c , #'pred_1d': pred_train_1d, 'pred_1e' : pred_train_1e , 'pred_1f' : pred_train_1f , 'pred_1g' : pred_train_1g , 'pred_1h' : pred_train_1h , 'pred_1i' : pred_train_1i , 'pred_1j' : pred_train_1j , 'pred_1k' : pred_train_1k , }) test_x_2 = pd . DataFrame ({ 'pred_1a' : pred_test_1a , 'pred_1b' : pred_test_1b , 'pred_1c' : pred_test_1c , #'pred_1d': pred_test_1d, 'pred_1e' : pred_test_1e , 'pred_1f' : pred_test_1f , 'pred_1g' : pred_test_1g , 'pred_1h' : pred_test_1h , 'pred_1i' : pred_test_1i , 'pred_1j' : pred_test_1j , 'pred_1k' : pred_test_1k }) model2 = Model2Linear () pred_train_2 , pred_test_2 = predict_cv ( model2 , train_x_2 , y_trainval , test_x_2 ) #\uff12\u5c64\u306e\u30b9\u30bf\u30c3\u30ad\u30f3\u30b0\u5f8c\u306e\u3001\u7cbe\u5ea6\u3092\u78ba\u8a8d\u3002 print ( f 'mean_absolute_error: { mean_absolute_error ( y_trainval , pred_train_2 ) : .4f } ' )","title":"stacking"},{"location":"pararell-async/","text":"\u4e26\u5217\u51e6\u7406\u3068\u975e\u540c\u671f\u51e6\u7406 \u00b6 \u30ea\u30f3\u30af \u00b6 subprocess\u306b\u3064\u3044\u3066\u3088\u308a\u6df1\u304f\uff083\u7cfb\uff0c\u66f4\u65b0\u7248\uff09 python\u306esubprocess\u3067\u304b\u3093\u305f\u3093\u4e26\u5217\u5b9f\u884c Python \u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4e26\u5217\u51e6\u7406\u3067\u9ad8\u901f\u5316\u3059\u308b Python\u3067concurrent.futures\u3092\u4f7f\u3063\u305f\u4e26\u5217\u30bf\u30b9\u30af\u5b9f\u884c Python\u306b\u304a\u3051\u308b\u975e\u540c\u671f\u51e6\u7406: asyncio\u9006\u5f15\u304d\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9 \u4e26\u5217\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306b\u5165\u9580\u3059\u308b Python\u306ethreading\u3068multiprocessing\u3092\u5b8c\u5168\u7406\u89e3 Python3 Subprocess.Popen()\u3068communicate()\u306b\u3064\u3044\u3066 Python multiprocessing vs threading vs asyncio Python\u306e\u4e26\u884c\u51e6\u7406\u3092\u7406\u89e3\u3057\u305f\u3044 [\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u7de8] Python Popen - wait vs communicate vs CalledProcessError https://stackoverflow.com/questions/51117209/combining-asyncio-with-a-multi-worker-processpoolexecutor?noredirect=1&lq=1 https://stackoverflow.com/questions/55993833/combining-asyncio-with-a-multi-worker-processpoolexecutor-and-for-async?noredirect=1&lq=1 https://qiita.com/matsui-k20xx/items/4d1c00c4eefd60ba635b Python\u306easync/await\u306f\u3001C#\u306a\u3069\u3068\u9055\u3063\u3066\u3001\u5168\u90e8\u4e00\u3064\u306e\u30b9\u30ec\u30c3\u30c9\u4e0a\u3067\u3084\u308b\u306e\u3067\u3001CPU\u30d0\u30a6\u30f3\u30c9\u306a\u51e6\u7406\u3084\u3001\u5927\u304d\u306a\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3059\u308b\u3088\u3046\u306aI/O\u30d0\u30a6\u30f3\u30c9\u306a\u51e6\u7406\u3092\u901f\u304f\u3059\u308b\u306e\u306f\u7121\u7406\u3002\u901a\u4fe1\u3092\u5f85\u3063\u3066\u308b\u81ea\u5206\u304c\u4f55\u3082\u3057\u3066\u306a\u3044\u6642\u9593\u306b\u4ed6\u306e\u51e6\u7406\u3092\u3084\u308a\u305f\u3044\u3088\u3046\u306a\u3068\u304d\u306b\u4f7f\u3048\u308b\u3002CPU\u30d0\u30a6\u30f3\u30c9\u306a\u3068\u304d\u306fmultiprocessing\u3084concurrent.futures.ProccessPoolExecutor\u3092\u4f7f\u3046\u3079\u3057. I/O\u30d0\u30a6\u30f3\u30c9\u306a\u3068\u304d\u306fconcurrent.futures.ThreadPoolExecutor\u3092\u4f7f\u3046\u3079\u304d \u4e26\u5217\u51e6\u7406 \u00b6 joblib \u00b6 subprocess.Popen \u00b6 1 2 3 4 5 6 7 8 def subprocess_popen ( max_process , loop_number , cmd ): for i in range ( loop_num ): proc_list = [] proc = subprocess . Popen ( cmd ) proc_list . append ( proc ) if ( i + 1 ) % max_process == 0 or ( i + 1 ) == loop_num : for subproc in proc_list : subproc . wait () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # https://qiita.com/takey/items/917a48aa5d1882e125f4 import cv2 from pathlib import Path import time import subprocess IMAGE_PATH = \"../../../tmp/image\" def main (): \"\"\" 100\u679a\u306e\u753b\u50cf\u304b\u3089\u3001100\u679a\u306e\u30e2\u30b6\u30a4\u30af\u51e6\u7406\u3057\u305f\u753b\u50cf\u3092\u751f\u6210\u3059\u308b Popen\u3092\u4f7f\u3063\u3066\u4e26\u5217\u51e6\u7406\u3067\u30e2\u30b6\u30a4\u30af\u753b\u50cf\u3092\u751f\u6210\u3059\u308b \"\"\" files = Path ( IMAGE_PATH + \"/origin/\" ) . glob ( \"*\" ) procs = [] N = 5 # \u30e1\u30e2\u30ea\u4e0d\u8db3\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306bN\u3092\u9069\u5207\u306b\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b for file in files : proc = subprocess . Popen ([ \"python\" , \"image2mosaic2_sub.py\" , str ( file ), file . name , IMAGE_PATH ]) procs . append ( proc ) if len ( procs ) == N : # \u30e1\u30e2\u30ea\u4e0d\u8db3\u3067\u5b9f\u884c\u306b\u5931\u6557\u3059\u308b\u306e\u3067\u3001 # \u5b50\u30d7\u30ed\u30bb\u30b9\u306e\u6570\u304cN\u306b\u306a\u3063\u305f\u3089\u3001\u4e00\u65e6\u5168\u3066\u306e\u5b50\u30d7\u30ed\u30bb\u30b9\u306e\u7d42\u4e86\u3092\u5f85\u3064 for proc in procs : proc . communicate () #proc.wait() procs . clear () for proc in procs : proc . communicate () if __name__ == \"__main__\" : start = time . time () main () end = time . time () print ( \"Finished in {} seconds.\" . format ( end - start )) 1 2 3 4 5 6 7 8 import subprocess proc = subprocess . Popen ( 'ls' , shell = True , stdout = subprocess . PIPE , ) while proc . poll () is None : output = proc . stdout . readline () print output 1 2 3 4 p1 = subprocess . Popen ([ 'ps' , 'aux' ], stdout = subprocess . PIPE ) # \u51fa\u529b\u5148\u306b\u30d1\u30a4\u30d7 p2 = subprocess . Popen ([ 'grep' , 'python' ], stdin = p1 . stdout , stdout = subprocess . PIPE ) # \u5165\u529b\u306bp1\u306e\u53d7\u53d6 p1 . stdout . close () # Allow p1 to receive a SIGPIPE if p2 exits. print ( p2 . communicate ()[ 0 ] . decode () . strip () . split ( ' \\n ' )) # stdout\u53d6\u5f97 threading \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import time import threading def func1 (): while True : print ( \"func1\" ) time . sleep ( 1 ) def func2 (): while True : print ( \"func2\" ) time . sleep ( 1 ) if __name__ == \"__main__\" : thread_1 = threading . Thread ( target = func1 ) thread_2 = threading . Thread ( target = func2 ) thread_1 . start () thread_2 . start () concurrent.futures \u00b6 1 2 3 4 5 6 7 8 9 def main (): with ThreadPoolExecutor ( max_workers = 2 , thread_name_prefix = \"thread\" ) as executor : results = executor . map ( task , files , ** kwargs ) return list ( results ) def main (): with ProcessPoolExecutor ( max_workers = max_workers ) as executor : results = executor . map ( task , files , chunksize = chunk_size ) return list ( results ) 1 2 3 4 5 with confu.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor: futures = [executor.submit(target_func, x) for x in range(8)] (done, notdone) = confu.wait(futures) for future in futures: print(future.result()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import cv2 from pathlib import Path import time from concurrent.futures import ThreadPoolExecutor , ProcessPoolExecutor IMAGE_PATH = \"../../../tmp/image\" def main ( file ): \"\"\" \u4e0e\u3048\u3089\u308c\u305f\u753b\u50cf\u304b\u3089\u3001\u30e2\u30b6\u30a4\u30af\u51e6\u7406\u3057\u305f\u753b\u50cf\u3092\u751f\u6210\u3059\u308b \"\"\" src = cv2 . imread ( str ( file )) dst = img2mosaic ( src , ratio = 0.01 ) cv2 . imwrite ( IMAGE_PATH + \"/mosaic/\" + file . name , dst ) if __name__ == \"__main__\" : files = Path ( IMAGE_PATH + \"/origin/\" ) . glob ( \"*\" ) start = time . time () #pool = ThreadPoolExecutor(max_workers=6) # CPU\u30b3\u30a2\u65706\u306a\u306e\u3067 pool = ProcessPoolExecutor ( max_workers = 6 ) results = list ( pool . map ( main , files )) # list()\u3067\u56f2\u307e\u306a\u3044\u3068\u3059\u3050\u306b\u7d42\u4e86\u3059\u308b\u306e\u3067\u6ce8\u610f end = time . time () print ( \"Finished in {} seconds.\" . format ( end - start )) \u975e\u540c\u671f\u51e6\u7406\u3001\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\uff08\u30b3\u30eb\u30fc\u30c1\u30f3\uff09 \u00b6 \u30b7\u30f3\u30b0\u30eb\u30b9\u30ec\u30c3\u30c9\u306e\u51e6\u7406 https://note.crohaco.net/2019/python-asyncio/ https://qiita.com/icoxfog417/items/07cbf5110ca82629aca0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 async def parallel_by_gather (): # execute by parallel def notify ( order ): print ( order + \" has just finished.\" ) cors = [ sleeping ( s [ 0 ], s [ 1 ], hook = notify ) for s in Seconds ] results = await asyncio . gather ( * cors ) return results if __name__ == \"__main__\" : loop = asyncio . get_event_loop () results = loop . run_until_complete ( parallel_by_gather ()) for r in results : print ( \"asyncio.gather result: {0} \" . format ( r )) async def parallel_by_wait (): # execute by parallel def notify ( order ): print ( order + \" has just finished.\" ) cors = [ sleeping ( s [ 0 ], s [ 1 ], hook = notify ) for s in Seconds ] done , pending = await asyncio . wait ( cors ) return done , pending if __name__ == \"__main__\" : loop = asyncio . get_event_loop () done , pending = loop . run_until_complete ( parallel_by_wait ()) for d in done : dr = d . result () print ( \"asyncio.wait result: {0} \" . format ( dr )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 async def queue_execution ( arg_urls , callback , parallel = 2 ): loop = asyncio . get_event_loop () queue = asyncio . Queue () for u in arg_urls : queue . put_nowait ( u ) async def fetch ( q ): while not q . empty (): u = await q . get () future = loop . run_in_executor ( None , requests . get , u ) future . add_done_callback ( callback ) await future tasks = [ fetch ( queue ) for i in range ( parallel )] return await asyncio . wait ( tasks ) if __name__ == \"__main__\" : loop = asyncio . get_event_loop () results = [] def store_result ( f ): results . append ( f . result ()) loop . run_until_complete ( queue_execution ([ \"http://www.google.com\" , \"http://www.yahoo.com\" , \"https://github.com/\" ], store_result )) for r in results : print ( \"queue execution: {0} \" . format ( r . url )) \u5408\u308f\u305b\u6280 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import asyncio import time import random import concurrent.futures # \u30e9\u30f3\u30c0\u30e0\u51e6\u7406 def print_num ( text ): i = random . randrange ( 10 ) print ( f \" { text } - { i } sec - \u958b\u59cb\" ) time . sleep ( i ) print ( f \" { text } \u7d42\u4e86\" ) async def multi_process ( loop , stock ): executor = concurrent . futures . ProcessPoolExecutor () queue = asyncio . Queue () [ queue . put_nowait ( x ) for x in stock ] async def p ( q ): while not q . empty (): i = await q . get () future = loop . run_in_executor ( executor , print_num , i ) await future # 8\u30d7\u30ed\u30bb\u30b9\u3067\u51e6\u7406 tasks = [ asyncio . create_task ( p ( queue )) for i in range ( 8 )] return await asyncio . wait ( tasks ) def main (): stock = [ \"aaa\" , \"bbb\" , \"ccc\" , \"ddd\" , \"eee\" , \"fff\" , \"ggg\" , \"hhh\" ] loop = asyncio . get_event_loop () loop . run_until_complete ( multi_process ( loop , stock )) if __name__ == \"__main__\" : main () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import asyncio from time import sleep , time from concurrent.futures import ProcessPoolExecutor num_jobs = 4 queue = asyncio . Queue () executor = ProcessPoolExecutor ( max_workers = num_jobs ) loop = asyncio . get_event_loop () def work (): sleep ( 1 ) async def producer (): tasks = [ loop . run_in_executor ( executor , work ) for _ in range ( num_jobs )] for f in asyncio . as_completed ( tasks , loop = loop ): results = await f await queue . put ( results ) async def consumer (): completed = 0 while completed < num_jobs : job = await queue . get () completed += 1 s = time () loop . run_until_complete ( asyncio . gather ( producer (), consumer ())) print ( \"duration\" , time () - s ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from concurrent.futures import ProcessPoolExecutor import asyncio import time async def mygen ( u : int = 2 ): i = 0 while i < u : yield i i += 1 def blocking ( delay ): time . sleep ( delay + 1 ) return ( 'EXECUTOR: Completed blocking task number ' + str ( delay + 1 )) async def run_blocking ( executor , task_no , delay ): print ( 'MASTER: Sending to executor blocking task number ' + str ( task_no )) result = await loop . run_in_executor ( executor , blocking , delay ) print ( result ) print ( 'MASTER: Well done executor - you seem to have completed ' 'blocking task number ' + str ( task_no )) async def non_blocking ( loop ): tasks = [] with ProcessPoolExecutor ( max_workers = 2 ) as executor : async for i in mygen (): # spawn the task and let it run in the background tasks . append ( asyncio . create_task ( run_blocking ( executor , i + 1 , i ))) # if there was an exception, retrieve it now await asyncio . gather ( * tasks ) loop = asyncio . get_event_loop () loop . run_until_complete ( non_blocking ( loop )) aiohttp \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import aiohttp import asyncio import time start_time = time . time () async def get_data ( session ): async with session . post ( url = url , params = params , headers = headers , data = open ( file_path , \"rb\" ) . read (), timeout = 10 , ) as resp : data = await resp . json () return data async def main ( total_number ): async with aiohttp . ClientSession () as session : tasks = [] for number in range ( total_number ): tasks . append ( asyncio . ensure_future ( get_data ( session ))) all_data = await asyncio . gather ( * tasks )","title":"\u4e26\u5217\u51e6\u7406\u3068\u975e\u540c\u671f\u51e6\u7406"},{"location":"pararell-async/#_1","text":"","title":"\u4e26\u5217\u51e6\u7406\u3068\u975e\u540c\u671f\u51e6\u7406"},{"location":"pararell-async/#_2","text":"subprocess\u306b\u3064\u3044\u3066\u3088\u308a\u6df1\u304f\uff083\u7cfb\uff0c\u66f4\u65b0\u7248\uff09 python\u306esubprocess\u3067\u304b\u3093\u305f\u3093\u4e26\u5217\u5b9f\u884c Python \u306e\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u4e26\u5217\u51e6\u7406\u3067\u9ad8\u901f\u5316\u3059\u308b Python\u3067concurrent.futures\u3092\u4f7f\u3063\u305f\u4e26\u5217\u30bf\u30b9\u30af\u5b9f\u884c Python\u306b\u304a\u3051\u308b\u975e\u540c\u671f\u51e6\u7406: asyncio\u9006\u5f15\u304d\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9 \u4e26\u5217\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306b\u5165\u9580\u3059\u308b Python\u306ethreading\u3068multiprocessing\u3092\u5b8c\u5168\u7406\u89e3 Python3 Subprocess.Popen()\u3068communicate()\u306b\u3064\u3044\u3066 Python multiprocessing vs threading vs asyncio Python\u306e\u4e26\u884c\u51e6\u7406\u3092\u7406\u89e3\u3057\u305f\u3044 [\u30de\u30eb\u30c1\u30b9\u30ec\u30c3\u30c9\u7de8] Python Popen - wait vs communicate vs CalledProcessError https://stackoverflow.com/questions/51117209/combining-asyncio-with-a-multi-worker-processpoolexecutor?noredirect=1&lq=1 https://stackoverflow.com/questions/55993833/combining-asyncio-with-a-multi-worker-processpoolexecutor-and-for-async?noredirect=1&lq=1 https://qiita.com/matsui-k20xx/items/4d1c00c4eefd60ba635b Python\u306easync/await\u306f\u3001C#\u306a\u3069\u3068\u9055\u3063\u3066\u3001\u5168\u90e8\u4e00\u3064\u306e\u30b9\u30ec\u30c3\u30c9\u4e0a\u3067\u3084\u308b\u306e\u3067\u3001CPU\u30d0\u30a6\u30f3\u30c9\u306a\u51e6\u7406\u3084\u3001\u5927\u304d\u306a\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3059\u308b\u3088\u3046\u306aI/O\u30d0\u30a6\u30f3\u30c9\u306a\u51e6\u7406\u3092\u901f\u304f\u3059\u308b\u306e\u306f\u7121\u7406\u3002\u901a\u4fe1\u3092\u5f85\u3063\u3066\u308b\u81ea\u5206\u304c\u4f55\u3082\u3057\u3066\u306a\u3044\u6642\u9593\u306b\u4ed6\u306e\u51e6\u7406\u3092\u3084\u308a\u305f\u3044\u3088\u3046\u306a\u3068\u304d\u306b\u4f7f\u3048\u308b\u3002CPU\u30d0\u30a6\u30f3\u30c9\u306a\u3068\u304d\u306fmultiprocessing\u3084concurrent.futures.ProccessPoolExecutor\u3092\u4f7f\u3046\u3079\u3057. I/O\u30d0\u30a6\u30f3\u30c9\u306a\u3068\u304d\u306fconcurrent.futures.ThreadPoolExecutor\u3092\u4f7f\u3046\u3079\u304d","title":"\u30ea\u30f3\u30af"},{"location":"pararell-async/#_3","text":"","title":"\u4e26\u5217\u51e6\u7406"},{"location":"pararell-async/#joblib","text":"","title":"joblib"},{"location":"pararell-async/#subprocesspopen","text":"1 2 3 4 5 6 7 8 def subprocess_popen ( max_process , loop_number , cmd ): for i in range ( loop_num ): proc_list = [] proc = subprocess . Popen ( cmd ) proc_list . append ( proc ) if ( i + 1 ) % max_process == 0 or ( i + 1 ) == loop_num : for subproc in proc_list : subproc . wait () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # https://qiita.com/takey/items/917a48aa5d1882e125f4 import cv2 from pathlib import Path import time import subprocess IMAGE_PATH = \"../../../tmp/image\" def main (): \"\"\" 100\u679a\u306e\u753b\u50cf\u304b\u3089\u3001100\u679a\u306e\u30e2\u30b6\u30a4\u30af\u51e6\u7406\u3057\u305f\u753b\u50cf\u3092\u751f\u6210\u3059\u308b Popen\u3092\u4f7f\u3063\u3066\u4e26\u5217\u51e6\u7406\u3067\u30e2\u30b6\u30a4\u30af\u753b\u50cf\u3092\u751f\u6210\u3059\u308b \"\"\" files = Path ( IMAGE_PATH + \"/origin/\" ) . glob ( \"*\" ) procs = [] N = 5 # \u30e1\u30e2\u30ea\u4e0d\u8db3\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306bN\u3092\u9069\u5207\u306b\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b for file in files : proc = subprocess . Popen ([ \"python\" , \"image2mosaic2_sub.py\" , str ( file ), file . name , IMAGE_PATH ]) procs . append ( proc ) if len ( procs ) == N : # \u30e1\u30e2\u30ea\u4e0d\u8db3\u3067\u5b9f\u884c\u306b\u5931\u6557\u3059\u308b\u306e\u3067\u3001 # \u5b50\u30d7\u30ed\u30bb\u30b9\u306e\u6570\u304cN\u306b\u306a\u3063\u305f\u3089\u3001\u4e00\u65e6\u5168\u3066\u306e\u5b50\u30d7\u30ed\u30bb\u30b9\u306e\u7d42\u4e86\u3092\u5f85\u3064 for proc in procs : proc . communicate () #proc.wait() procs . clear () for proc in procs : proc . communicate () if __name__ == \"__main__\" : start = time . time () main () end = time . time () print ( \"Finished in {} seconds.\" . format ( end - start )) 1 2 3 4 5 6 7 8 import subprocess proc = subprocess . Popen ( 'ls' , shell = True , stdout = subprocess . PIPE , ) while proc . poll () is None : output = proc . stdout . readline () print output 1 2 3 4 p1 = subprocess . Popen ([ 'ps' , 'aux' ], stdout = subprocess . PIPE ) # \u51fa\u529b\u5148\u306b\u30d1\u30a4\u30d7 p2 = subprocess . Popen ([ 'grep' , 'python' ], stdin = p1 . stdout , stdout = subprocess . PIPE ) # \u5165\u529b\u306bp1\u306e\u53d7\u53d6 p1 . stdout . close () # Allow p1 to receive a SIGPIPE if p2 exits. print ( p2 . communicate ()[ 0 ] . decode () . strip () . split ( ' \\n ' )) # stdout\u53d6\u5f97","title":"subprocess.Popen"},{"location":"pararell-async/#threading","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import time import threading def func1 (): while True : print ( \"func1\" ) time . sleep ( 1 ) def func2 (): while True : print ( \"func2\" ) time . sleep ( 1 ) if __name__ == \"__main__\" : thread_1 = threading . Thread ( target = func1 ) thread_2 = threading . Thread ( target = func2 ) thread_1 . start () thread_2 . start ()","title":"threading"},{"location":"pararell-async/#concurrentfutures","text":"1 2 3 4 5 6 7 8 9 def main (): with ThreadPoolExecutor ( max_workers = 2 , thread_name_prefix = \"thread\" ) as executor : results = executor . map ( task , files , ** kwargs ) return list ( results ) def main (): with ProcessPoolExecutor ( max_workers = max_workers ) as executor : results = executor . map ( task , files , chunksize = chunk_size ) return list ( results ) 1 2 3 4 5 with confu.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor: futures = [executor.submit(target_func, x) for x in range(8)] (done, notdone) = confu.wait(futures) for future in futures: print(future.result()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import cv2 from pathlib import Path import time from concurrent.futures import ThreadPoolExecutor , ProcessPoolExecutor IMAGE_PATH = \"../../../tmp/image\" def main ( file ): \"\"\" \u4e0e\u3048\u3089\u308c\u305f\u753b\u50cf\u304b\u3089\u3001\u30e2\u30b6\u30a4\u30af\u51e6\u7406\u3057\u305f\u753b\u50cf\u3092\u751f\u6210\u3059\u308b \"\"\" src = cv2 . imread ( str ( file )) dst = img2mosaic ( src , ratio = 0.01 ) cv2 . imwrite ( IMAGE_PATH + \"/mosaic/\" + file . name , dst ) if __name__ == \"__main__\" : files = Path ( IMAGE_PATH + \"/origin/\" ) . glob ( \"*\" ) start = time . time () #pool = ThreadPoolExecutor(max_workers=6) # CPU\u30b3\u30a2\u65706\u306a\u306e\u3067 pool = ProcessPoolExecutor ( max_workers = 6 ) results = list ( pool . map ( main , files )) # list()\u3067\u56f2\u307e\u306a\u3044\u3068\u3059\u3050\u306b\u7d42\u4e86\u3059\u308b\u306e\u3067\u6ce8\u610f end = time . time () print ( \"Finished in {} seconds.\" . format ( end - start ))","title":"concurrent.futures"},{"location":"pararell-async/#_4","text":"\u30b7\u30f3\u30b0\u30eb\u30b9\u30ec\u30c3\u30c9\u306e\u51e6\u7406 https://note.crohaco.net/2019/python-asyncio/ https://qiita.com/icoxfog417/items/07cbf5110ca82629aca0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 async def parallel_by_gather (): # execute by parallel def notify ( order ): print ( order + \" has just finished.\" ) cors = [ sleeping ( s [ 0 ], s [ 1 ], hook = notify ) for s in Seconds ] results = await asyncio . gather ( * cors ) return results if __name__ == \"__main__\" : loop = asyncio . get_event_loop () results = loop . run_until_complete ( parallel_by_gather ()) for r in results : print ( \"asyncio.gather result: {0} \" . format ( r )) async def parallel_by_wait (): # execute by parallel def notify ( order ): print ( order + \" has just finished.\" ) cors = [ sleeping ( s [ 0 ], s [ 1 ], hook = notify ) for s in Seconds ] done , pending = await asyncio . wait ( cors ) return done , pending if __name__ == \"__main__\" : loop = asyncio . get_event_loop () done , pending = loop . run_until_complete ( parallel_by_wait ()) for d in done : dr = d . result () print ( \"asyncio.wait result: {0} \" . format ( dr )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 async def queue_execution ( arg_urls , callback , parallel = 2 ): loop = asyncio . get_event_loop () queue = asyncio . Queue () for u in arg_urls : queue . put_nowait ( u ) async def fetch ( q ): while not q . empty (): u = await q . get () future = loop . run_in_executor ( None , requests . get , u ) future . add_done_callback ( callback ) await future tasks = [ fetch ( queue ) for i in range ( parallel )] return await asyncio . wait ( tasks ) if __name__ == \"__main__\" : loop = asyncio . get_event_loop () results = [] def store_result ( f ): results . append ( f . result ()) loop . run_until_complete ( queue_execution ([ \"http://www.google.com\" , \"http://www.yahoo.com\" , \"https://github.com/\" ], store_result )) for r in results : print ( \"queue execution: {0} \" . format ( r . url ))","title":"\u975e\u540c\u671f\u51e6\u7406\u3001\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\uff08\u30b3\u30eb\u30fc\u30c1\u30f3\uff09"},{"location":"pararell-async/#_5","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import asyncio import time import random import concurrent.futures # \u30e9\u30f3\u30c0\u30e0\u51e6\u7406 def print_num ( text ): i = random . randrange ( 10 ) print ( f \" { text } - { i } sec - \u958b\u59cb\" ) time . sleep ( i ) print ( f \" { text } \u7d42\u4e86\" ) async def multi_process ( loop , stock ): executor = concurrent . futures . ProcessPoolExecutor () queue = asyncio . Queue () [ queue . put_nowait ( x ) for x in stock ] async def p ( q ): while not q . empty (): i = await q . get () future = loop . run_in_executor ( executor , print_num , i ) await future # 8\u30d7\u30ed\u30bb\u30b9\u3067\u51e6\u7406 tasks = [ asyncio . create_task ( p ( queue )) for i in range ( 8 )] return await asyncio . wait ( tasks ) def main (): stock = [ \"aaa\" , \"bbb\" , \"ccc\" , \"ddd\" , \"eee\" , \"fff\" , \"ggg\" , \"hhh\" ] loop = asyncio . get_event_loop () loop . run_until_complete ( multi_process ( loop , stock )) if __name__ == \"__main__\" : main () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import asyncio from time import sleep , time from concurrent.futures import ProcessPoolExecutor num_jobs = 4 queue = asyncio . Queue () executor = ProcessPoolExecutor ( max_workers = num_jobs ) loop = asyncio . get_event_loop () def work (): sleep ( 1 ) async def producer (): tasks = [ loop . run_in_executor ( executor , work ) for _ in range ( num_jobs )] for f in asyncio . as_completed ( tasks , loop = loop ): results = await f await queue . put ( results ) async def consumer (): completed = 0 while completed < num_jobs : job = await queue . get () completed += 1 s = time () loop . run_until_complete ( asyncio . gather ( producer (), consumer ())) print ( \"duration\" , time () - s ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from concurrent.futures import ProcessPoolExecutor import asyncio import time async def mygen ( u : int = 2 ): i = 0 while i < u : yield i i += 1 def blocking ( delay ): time . sleep ( delay + 1 ) return ( 'EXECUTOR: Completed blocking task number ' + str ( delay + 1 )) async def run_blocking ( executor , task_no , delay ): print ( 'MASTER: Sending to executor blocking task number ' + str ( task_no )) result = await loop . run_in_executor ( executor , blocking , delay ) print ( result ) print ( 'MASTER: Well done executor - you seem to have completed ' 'blocking task number ' + str ( task_no )) async def non_blocking ( loop ): tasks = [] with ProcessPoolExecutor ( max_workers = 2 ) as executor : async for i in mygen (): # spawn the task and let it run in the background tasks . append ( asyncio . create_task ( run_blocking ( executor , i + 1 , i ))) # if there was an exception, retrieve it now await asyncio . gather ( * tasks ) loop = asyncio . get_event_loop () loop . run_until_complete ( non_blocking ( loop ))","title":"\u5408\u308f\u305b\u6280"},{"location":"pararell-async/#aiohttp","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import aiohttp import asyncio import time start_time = time . time () async def get_data ( session ): async with session . post ( url = url , params = params , headers = headers , data = open ( file_path , \"rb\" ) . read (), timeout = 10 , ) as resp : data = await resp . json () return data async def main ( total_number ): async with aiohttp . ClientSession () as session : tasks = [] for number in range ( total_number ): tasks . append ( asyncio . ensure_future ( get_data ( session ))) all_data = await asyncio . gather ( * tasks )","title":"aiohttp"},{"location":"python_books_note/","text":"Python books \u00b6 \u81ea\u8d70\u30d7\u30ed\u30b0\u30e9\u30de\u30fc/Efficient Python/Python \u30c8\u30ea\u30c3\u30af/Python \u30cf\u30c3\u30ab\u30fc\u30ac\u30a4\u30c9\u30d6\u30c3\u30af \u00b6 \u30b7\u30b9\u30c6\u30e0\u8a2d\u8a08 \u00b6 \u76f8\u5bfe\u30d1\u30b9\u3067\u306e\u7d44\u307f\u7acb\u3066/Pathlib \u00b6 1 2 3 4 from pathlib import Path root = Path ( __file__ ) . parent DATA_DIR = root / \"target.dat\" \u4f8b 1 2 3 4 file_path = Path ( __file__ ) . resolve () root_path = file_path . parent log_path = root_path / \"log\" log_path . mkdir ( parents = True , exist_ok = True ) \u7af6\u5408\u3057\u306a\u3044\u4e00\u6642\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210/Tempfile \u00b6 1 2 3 4 5 import tempfile with tempfile . NamedTemporaryFile ( prefix = \"test-\" ) as f : f . write ( data ) send_file ( f . name ) \u30d5\u30a1\u30a4\u30eb\u3092\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5358\u4f4d\u3067\u5206\u6563/Hashlib \u00b6 1 2 3 4 5 6 import hashlib from pathlib import Path filename = \"somefile\" hash = hashlib . md5 ( filename . encode ()) . hexdigest () path = Path ( f \"/root/ { hash } /somefile.dat\" ) \u74b0\u5883\u5909\u6570\u306e\u6709\u52b9\u6d3b\u7528 \u00b6 os.envision \u00b6 1 2 import os DEBUG = bool ( os . envision . get ( \"DEBUG\" , False )) python_dotenv \u00b6 1 2 # .env DEBUG=True 1 pip install python-dotenv 1 2 3 4 5 6 7 8 9 10 # \u4f7f\u7528\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30a4\u30f3\u30dd\u30fc\u30c8 from dotenv import load_dotenv import os # .env\u304b\u3089\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u3080 load_dotenv () # \u74b0\u5883\u5909\u6570\u3092\u53c2\u7167 DATABASE_NAME = os . getenv ( 'DATABASE_NAME' ) print ( DATABASE_NAME ) # \u300csample.db\u300d\u304c\u8868\u793a\u3055\u308c\u308b \u30c7\u30d0\u30c3\u30b0 \u00b6 logger.debug \u3092\u4f7f\u3046 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import logging from logging import DEBUG , StreamHandler , getLogger from logging.handlers import RotatingFileHandler from pathlib import Path def setup_logger ( name ): file_path = Path ( __file__ ) . resolve () root_path = file_path . parent log_path = root_path / \"log\" log_path . mkdir ( parents = True , exist_ok = True ) logger = logging . getLogger ( name ) logger . setLevel ( DEBUG ) logger . propagate = False formatter = logging . Formatter ( \"[ %(asctime)s ][ %(name)s ][ %(funcName)s , %(lineno)d ] %(message)s \" ) fh = RotatingFileHandler ( log_path / f \" { file_path . name } .log\" , maxBytes = 100000 , backupCount = 10 ) fh . setLevel ( DEBUG ) fh . setFormatter ( formatter ) ch = logging . StreamHandler () ch . setLevel ( DEBUG ) ch . setFormatter ( formatter ) logger . addHandler ( fh ) logger . addHandler ( ch ) return logger logger = setup_logger ( __name__ ) \u4f8b\u5916\uff0b logger + exc_info=True \u00b6 1 2 3 4 try : do_something () except Exception as e : logger . warning ( \"something raised an exception: \" , exc_info = True ) json \u30d5\u30a1\u30a4\u30eb\u3067\u7ba1\u7406 \u00b6 https://zenn.dev/re24_1986/articles/32a92b75d2bbf7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 { \"version\" : 1 , \"disable_existing_loggers\" : false , \"formatters\" : { \"simple\" : { \"format\" : \"%(asctime)s %(name)s:%(lineno)s %(funcName)s [%(levelname)s]: %(message)s\" } }, \"handlers\" : { \"consoleHandler\" : { \"class\" : \"logging.StreamHandler\" , \"level\" : \"DEBUG\" , \"formatter\" : \"simple\" , \"stream\" : \"ext://sys.stdout\" }, \"fileHandler\" : { \"class\" : \"logging.FileHandler\" , \"level\" : \"DEBUG\" , \"formatter\" : \"simple\" , \"filename\" : \"app.log\" , \"encoding\" : \"utf-8\" } }, \"loggers\" : { \"hogeLogger\" : { \"level\" : \"DEBUG\" , \"handlers\" : [ \"consoleHandler\" , \"fileHandler\" ] }, \"fooLogger\" : { \"level\" : \"DEBUG\" , \"handlers\" : [ \"consoleHandler\" , \"fileHandler\" ] } }, \"root\" : { \"level\" : \"ERROR\" } } \u753b\u50cf\u7528\u30c7\u30d0\u30c3\u30b0 \u00b6 1 https : // github . com / dchaplinsky / visual - logging assert \u3092\u5229\u7528 \u00b6 https://realpython.com/python-assert-statement/ 1 2 3 4 5 6 number = 42 assert ( number > 0 and isinstance ( number , int ), f \"number greater than 0 expected, got: { number } \" ) 1 2 3 4 5 6 7 8 9 10 11 import math class Circle : def __init__ ( self , radius ): if radius < 0 : raise ValueError ( \"positive radius expected\" ) self . radius = radius def area ( self ): assert self . radius >= 0 , \"positive radius expected\" return math . pi * self . radius ** 2 \u30ab\u30b9\u30bf\u30e0\u4f8b\u5916\u30af\u30e9\u30b9\u3092\u4f5c\u6210 \u00b6 1 2 3 4 5 6 class CustomError ( Exception ): \"\"\"\u81ea\u4f5c\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30a8\u30e9\u30fc\u3092\u77e5\u3089\u305b\u308b\u4f8b\u5916\u30af\u30e9\u30b9\u3067\u3059\u3002\"\"\" pass # \u4f8b\u5916\u30af\u30e9\u30b9\u3092\u4f7f\u3044\u307e\u3059\u3002 raise CustomError ( 'error message' ) Sentry \u306e\u5229\u7528 \u00b6 pdb, breakpoint \u306e\u5229\u7528 \u00b6 VSCode \u3068 pytest \u3067 Python \u30b3\u30fc\u30c9\u3092\u30c6\u30b9\u30c8&\u30c7\u30d0\u30c3\u30b0\u3059\u308b \u30e6\u30cb\u30c3\u30c8\u30c6\u30b9\u30c8 \u00b6","title":"Python books"},{"location":"python_books_note/#python-books","text":"","title":"Python books"},{"location":"python_books_note/#efficient-pythonpython-python","text":"","title":"\u81ea\u8d70\u30d7\u30ed\u30b0\u30e9\u30de\u30fc/Efficient Python/Python \u30c8\u30ea\u30c3\u30af/Python \u30cf\u30c3\u30ab\u30fc\u30ac\u30a4\u30c9\u30d6\u30c3\u30af"},{"location":"python_books_note/#_1","text":"","title":"\u30b7\u30b9\u30c6\u30e0\u8a2d\u8a08"},{"location":"python_books_note/#pathlib","text":"1 2 3 4 from pathlib import Path root = Path ( __file__ ) . parent DATA_DIR = root / \"target.dat\" \u4f8b 1 2 3 4 file_path = Path ( __file__ ) . resolve () root_path = file_path . parent log_path = root_path / \"log\" log_path . mkdir ( parents = True , exist_ok = True )","title":"\u76f8\u5bfe\u30d1\u30b9\u3067\u306e\u7d44\u307f\u7acb\u3066/Pathlib"},{"location":"python_books_note/#tempfile","text":"1 2 3 4 5 import tempfile with tempfile . NamedTemporaryFile ( prefix = \"test-\" ) as f : f . write ( data ) send_file ( f . name )","title":"\u7af6\u5408\u3057\u306a\u3044\u4e00\u6642\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210/Tempfile"},{"location":"python_books_note/#hashlib","text":"1 2 3 4 5 6 import hashlib from pathlib import Path filename = \"somefile\" hash = hashlib . md5 ( filename . encode ()) . hexdigest () path = Path ( f \"/root/ { hash } /somefile.dat\" )","title":"\u30d5\u30a1\u30a4\u30eb\u3092\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5358\u4f4d\u3067\u5206\u6563/Hashlib"},{"location":"python_books_note/#_2","text":"","title":"\u74b0\u5883\u5909\u6570\u306e\u6709\u52b9\u6d3b\u7528"},{"location":"python_books_note/#osenvision","text":"1 2 import os DEBUG = bool ( os . envision . get ( \"DEBUG\" , False ))","title":"os.envision"},{"location":"python_books_note/#python_dotenv","text":"1 2 # .env DEBUG=True 1 pip install python-dotenv 1 2 3 4 5 6 7 8 9 10 # \u4f7f\u7528\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u30a4\u30f3\u30dd\u30fc\u30c8 from dotenv import load_dotenv import os # .env\u304b\u3089\u74b0\u5883\u5909\u6570\u3092\u8aad\u307f\u8fbc\u3080 load_dotenv () # \u74b0\u5883\u5909\u6570\u3092\u53c2\u7167 DATABASE_NAME = os . getenv ( 'DATABASE_NAME' ) print ( DATABASE_NAME ) # \u300csample.db\u300d\u304c\u8868\u793a\u3055\u308c\u308b","title":"python_dotenv"},{"location":"python_books_note/#_3","text":"","title":"\u30c7\u30d0\u30c3\u30b0"},{"location":"python_books_note/#loggerdebug","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import logging from logging import DEBUG , StreamHandler , getLogger from logging.handlers import RotatingFileHandler from pathlib import Path def setup_logger ( name ): file_path = Path ( __file__ ) . resolve () root_path = file_path . parent log_path = root_path / \"log\" log_path . mkdir ( parents = True , exist_ok = True ) logger = logging . getLogger ( name ) logger . setLevel ( DEBUG ) logger . propagate = False formatter = logging . Formatter ( \"[ %(asctime)s ][ %(name)s ][ %(funcName)s , %(lineno)d ] %(message)s \" ) fh = RotatingFileHandler ( log_path / f \" { file_path . name } .log\" , maxBytes = 100000 , backupCount = 10 ) fh . setLevel ( DEBUG ) fh . setFormatter ( formatter ) ch = logging . StreamHandler () ch . setLevel ( DEBUG ) ch . setFormatter ( formatter ) logger . addHandler ( fh ) logger . addHandler ( ch ) return logger logger = setup_logger ( __name__ )","title":"logger.debug \u3092\u4f7f\u3046"},{"location":"python_books_note/#logger-exc_infotrue","text":"1 2 3 4 try : do_something () except Exception as e : logger . warning ( \"something raised an exception: \" , exc_info = True )","title":"\u4f8b\u5916\uff0b logger + exc_info=True"},{"location":"python_books_note/#json","text":"https://zenn.dev/re24_1986/articles/32a92b75d2bbf7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 { \"version\" : 1 , \"disable_existing_loggers\" : false , \"formatters\" : { \"simple\" : { \"format\" : \"%(asctime)s %(name)s:%(lineno)s %(funcName)s [%(levelname)s]: %(message)s\" } }, \"handlers\" : { \"consoleHandler\" : { \"class\" : \"logging.StreamHandler\" , \"level\" : \"DEBUG\" , \"formatter\" : \"simple\" , \"stream\" : \"ext://sys.stdout\" }, \"fileHandler\" : { \"class\" : \"logging.FileHandler\" , \"level\" : \"DEBUG\" , \"formatter\" : \"simple\" , \"filename\" : \"app.log\" , \"encoding\" : \"utf-8\" } }, \"loggers\" : { \"hogeLogger\" : { \"level\" : \"DEBUG\" , \"handlers\" : [ \"consoleHandler\" , \"fileHandler\" ] }, \"fooLogger\" : { \"level\" : \"DEBUG\" , \"handlers\" : [ \"consoleHandler\" , \"fileHandler\" ] } }, \"root\" : { \"level\" : \"ERROR\" } }","title":"json \u30d5\u30a1\u30a4\u30eb\u3067\u7ba1\u7406"},{"location":"python_books_note/#_4","text":"1 https : // github . com / dchaplinsky / visual - logging","title":"\u753b\u50cf\u7528\u30c7\u30d0\u30c3\u30b0"},{"location":"python_books_note/#assert","text":"https://realpython.com/python-assert-statement/ 1 2 3 4 5 6 number = 42 assert ( number > 0 and isinstance ( number , int ), f \"number greater than 0 expected, got: { number } \" ) 1 2 3 4 5 6 7 8 9 10 11 import math class Circle : def __init__ ( self , radius ): if radius < 0 : raise ValueError ( \"positive radius expected\" ) self . radius = radius def area ( self ): assert self . radius >= 0 , \"positive radius expected\" return math . pi * self . radius ** 2","title":"assert \u3092\u5229\u7528"},{"location":"python_books_note/#_5","text":"1 2 3 4 5 6 class CustomError ( Exception ): \"\"\"\u81ea\u4f5c\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u30a8\u30e9\u30fc\u3092\u77e5\u3089\u305b\u308b\u4f8b\u5916\u30af\u30e9\u30b9\u3067\u3059\u3002\"\"\" pass # \u4f8b\u5916\u30af\u30e9\u30b9\u3092\u4f7f\u3044\u307e\u3059\u3002 raise CustomError ( 'error message' )","title":"\u30ab\u30b9\u30bf\u30e0\u4f8b\u5916\u30af\u30e9\u30b9\u3092\u4f5c\u6210"},{"location":"python_books_note/#sentry","text":"","title":"Sentry \u306e\u5229\u7528"},{"location":"python_books_note/#pdb-breakpoint","text":"VSCode \u3068 pytest \u3067 Python \u30b3\u30fc\u30c9\u3092\u30c6\u30b9\u30c8&\u30c7\u30d0\u30c3\u30b0\u3059\u308b","title":"pdb, breakpoint \u306e\u5229\u7528"},{"location":"python_books_note/#_6","text":"","title":"\u30e6\u30cb\u30c3\u30c8\u30c6\u30b9\u30c8"},{"location":"system_design/","text":"\u30b7\u30b9\u30c6\u30e0\u30c7\u30b6\u30a4\u30f3 interview \u30c7\u30fc\u30bf\u5fd7\u5411\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3 algoex, educ,","title":"System design"},{"location":"templates/","text":"TH \u5de6\u5bc4\u305b TH \u4e2d\u592e\u5bc4\u305b TH \u53f3\u5bc4\u305b TD TD TD TD TD TD Note \u3053\u308c\u306f\u30ce\u30fc\u30c8\u3067\u3059\u3002 Tip \u30d2\u30f3\u30c8\u3067\u3059\u3002 Warning \u3053\u308c\u306f\u8b66\u544a\u3067\u3059\u3002 Danger \u3053\u308c\u306f\u5371\u967a\u3067\u3059\u3002 Success \u3053\u308c\u306f\u6210\u529f\u3067\u3059\u3002 Failure \u3053\u308c\u306f\u5931\u6557\u3067\u3059\u3002 Bug \u3053\u308c\u306f\u30d0\u30b0\u3067\u3059\u3002 Summary \u3053\u308c\u306f\u6982\u8981\u3067\u3059\u3002 Mkdocs \u3068\u306f\u9759\u7684\u30b5\u30a4\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3067\u3059\u3002 \u30b3\u30f3\u30c6\u30f3\u30c4\u306f\u57fa\u672c\u7684\u306b markdown 1 \u5f62\u5f0f\u3067\u8a18\u8ff0\u3057\u305f\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u306b\u306a\u308a\u307e\u3059\u3002 \u5b9a\u7fa9\u8a9e \u3053\u3053\u306b\u8aac\u660e\u3092\u66f8\u304d\u307e\u3059 \u6587\u66f8\u3092\u8a18\u8ff0\u3059\u308b\u305f\u3081\u306e\u8efd\u91cf\u30de\u30fc\u30af\u30a2\u30c3\u30d7\u8a00\u8a9e\u306e\u3072\u3068\u3064 \u21a9","title":"Templates"},{"location":"util/","text":"python \u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea \u00b6 https://qiita.com/hiroyuki_mrp/items/8bbd9ab6c16601e87a9c \u30ea\u30f3\u30bf\u30fc \u00b6 \u3010VS Code\u3011Black \u3068 Flake8 \u3092\u4f7f\u3063\u3066\u304d\u308c\u3044\u306a Python \u30b3\u30fc\u30c9\u3092\u66f8\u304f\uff01\uff01 Python \u306e\u30b3\u30fc\u30c9\u3092\u30ad\u30ec\u30a4\u306b\u66f8\u304d\u305f\u3044\uff01(VSCode \u306b flake8 & black \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb) flake8\u3001black\u3001isort\u3001mypy \u3092 VS Code \u4e0a\u3067\u4f7f\u7528\u3059\u308b GCP \u306a\u3069\u306e\u5834\u5408\u306f\u500b\u5225\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff08pip install black\uff09,\u8a2d\u5b9a(ssh \u30ef\u30fc\u30af\u30b9\u30da\u30fc\u30b9)\u304c\u5fc5\u8981\u3001error lense \u306f 500 \u307e\u3067\u306e delay \u304c\u5fc5\u8981 util pyfile templates \u00b6 Pypy, pip \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Python \u3067\u81ea\u5206\u3060\u3051\u306e\u30af\u30bd\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f5c\u308b\u65b9\u6cd5 Python \u3067\u4f5c\u3063\u305f\u30b3\u30de\u30f3\u30c9\u3092 GitHub \u7d4c\u7531\u3067 pip \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u53ef\u80fd\u306b\u3059\u308b \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import glob import os import time from functools import wraps import argparse from tqdm import tqdm @stop_watch def func ( file_list , output_dir ): return None if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--input_dir\" , type = str , required = True , help = \"path for input dir\" , ) parser . add_argument ( \"--output_dir\" , type = str , required = True , help = \"path for output dir\" , ) args = parser . parse_args () check_create_dir ( args . output_dir ) file_list = get_file_list ( args . input_dir ) func ( file_list , args . output_dir ) \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u78ba\u8a8d\u3068\u4f5c\u6210 \u00b6 1 2 3 4 def check_create_dir ( path ): # check if there is the directory. if not create a new directory. if not os . path . isdir ( path ): os . makedirs ( path ) \u30d5\u30a1\u30a4\u30eb\u30ea\u30b9\u30c8\u306e\u53d6\u5f97 \u00b6 1 2 3 4 5 6 7 8 9 def get_file_list ( input_dir ): # get the list of files in input directory. return sorted ( [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pathlib import Path from typing import List def list_file_paths ( dir_path : str ) -> List [ str ]: \"\"\" List file paths in a directory. Parameters ---------- dir_path : str Path for the directory Returns ------- List[str] List of the file paths in the directory \"\"\" return sorted ([ str ( path ) for path in Path ( dir_path ) . rglob ( \"*\" ) if path . is_file ()]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def add_suffix_to_all_file_paths ( dir_path : str ) -> List [ str ]: \"\"\" Add suffix to all the file name Parameters ---------- dir_path : str Path for the directory Returns ------- List[str] List of the file paths in the directory \"\"\" return sorted ( [ str ( path . rename ( str ( path . parent / Path ( path . stem + \"_suffix\" + path . suffix )) ) ) for path in Path ( dir_path ) . rglob ( \"*\" ) if path . is_file () ] ) 1 2 3 4 5 6 7 8 9 10 11 ### def stop_watch ( func ) : @wraps ( func ) def wrapper ( * args , ** kargs ) : start = time . time () result = func ( * args , ** kargs ) elapsed_time = time . time () - start print ( f \"total time of { func . __name__ } : { elapsed_time } \" ) return result return wrapper 1 2 3 4 5 6 7 8 9 10 11 12 13 from time import time from contextlib import contextmanager import cudf @contextmanager def timer ( name ): start = time () yield print ( f '[ { name } ] done in { time () - start : .2f } s' ) with timer ( 'name' ): some_function () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @stop_watch def func ( file_list , output_dir ): return None if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--input_dir\" , type = str , required = True , help = \"path for input dir\" , ) parser . add_argument ( \"--output_dir\" , type = str , required = True , help = \"path for output dir\" , ) args = parser . parse_args () check_create_dir ( args . output_dir ) file_list = get_file_list ( args . input_dir ) func ( file_list , args . output_dir ) General util \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def load_pickle ( load_path ): # pd.read_pickle(load_path) with open ( load_path , mode = \"rb\" ) as f : return pickle . load ( f ) def save_pickle ( object , save_path ): # pd.to_pickle(object, save_path) with open ( save_path , mode = \"wb\" ) as f : pickle . dump ( object , f ) def check_create_dir ( path ): if not os . path . isdir ( path ): os . makedirs ( path , exist_ok = True ) def get_file_list ( input_dir ): return [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] def split_list ( l , n ): \"\"\" https://www.python.ambitious-engineer.com/archives/1843 Other: np.array_split(l, 3) \"\"\" for idx in range ( 0 , len ( l ), n ): yield l [ idx : idx + n ] def split_list ( l , n ): return np . array_split ( l , n ) def json2dataframe ( json_data ): return pd . json_normalize ( json_data , record_path = 'data' ) def flatten_dict ( dict ): df = pd . json_normalize ( dict [ \"data\" ][ 0 ], sep = \"_\" ) return df . to_dict ( orient = \"records\" )[ 0 ] def flatten_dict ( d , parent_key = \"\" , sep = \"_\" ): items = [] for k , v in d . items (): new_key = parent_key + sep + k if parent_key else k if isinstance ( v , collections . MutableMapping ): items . extend ( flatten ( v , new_key , sep = sep ) . items ()) else : items . append (( new_key , v )) return dict ( items ) from functools import wraps import time def stop_watch ( func ) : @wraps ( func ) def wrapper ( * args , ** kargs ) : start = time . time () result = func ( * args , ** kargs ) elapsed_time = time . time () - start print ( f \"total time of { func . __name__ } : { elapsed_time } \" ) return result return wrapper def seed_everything ( seed = 2021 ): random . seed ( seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False def git_commits ( rand ): def func_decorator ( my_func ): repo = git . Repo ( \"/work\" ) repo . config_writer () . set_value ( \"user\" , \"name\" , \"your_userID\" ) . release () repo . config_writer () . set_value ( \"user\" , \"email\" , \"your_email@gmail.com\" ) . release () repo . git . diff ( \"HEAD\" ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _running\" ) repo . git . push ( \"origin\" , \"HEAD\" ) logger . info ( f \"git pushed to the remote origin\" ) def decorator_wrapper ( * args , ** kwargs ): my_func ( * args , ** kwargs ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _done\" ) repo . git . push ( \"origin\" , \"HEAD\" ) logger . info ( f \"git pushed to the remote origin\" ) return decorator_wrapper return func_decorator def randomname ( n ): return \"\" . join ( random . choices ( string . ascii_letters + string . digits , k = n )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def save_mlflow_pickle ( obj , file_name ): with tempfile . TemporaryDirectory () as d : artifact_path = pathlib . Path ( d ) / file_name save_pickle ( obj , artifact_path ) mlflow . log_artifact ( artifact_path ) def save_mlflow_model ( original_path ): with tempfile . TemporaryDirectory () as d : artifact_path = pathlib . Path ( d ) / os . path . basename ( original_path ) shutil . copy ( original_path , artifact_path ) mlflow . log_artifact ( artifact_path ) def get_git_revision_short_hash (): short_hash = subprocess . check_output ([ 'git' , 'rev-parse' , '--short' , 'HEAD' ]) short_hash = str ( short_hash , \"utf-8\" ) . strip () return short_hash logger \u00b6 \u30ed\u30b0\u51fa\u529b\u306e\u305f\u3081\u306e print \u3068 import logging \u306f\u3084\u3081\u3066\u307b\u3057\u3044 Python \u3067 print \u3092\u5352\u696d\u3057\u3066\u30ed\u30b0\u51fa\u529b\u3092\u3044\u3044\u611f\u3058\u306b\u3059\u308b \u00b6 1 2 3 4 5 6 7 8 9 from logging import getLogger , StreamHandler , DEBUG logger = getLogger ( __name__ ) handler = StreamHandler () handler . setLevel ( DEBUG ) logger . setLevel ( DEBUG ) logger . addHandler ( handler ) logger . propagate = False logger . debug ( 'hello' ) https://qiita.com/shotakaha/items/0fa2db1dc8253c83e2bb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import logging import logging.handlers def setup_logger ( name , logfile = 'log.log' ): logger = logging . getLogger ( name ) logger . setLevel ( logging . DEBUG ) # create file handler which logs even DEBUG messages fh = logging . handlers . RotatingFileHandler ( logfile , maxBytes = 100000000 , backupCount = 10 ) fh . setLevel ( logging . DEBUG ) fh_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) fh . setFormatter ( fh_formatter ) # create console handler with a INFO log level ch = LoggingHandler () ch . setLevel ( logging . INFO ) ch_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) ch . setFormatter ( ch_formatter ) # add the handlers to the logger logger . addHandler ( fh ) logger . addHandler ( ch ) return logger def setup_logger ( name , logfile = 'log.log' ): logger = logging . getLogger ( name ) logger . setLevel ( logging . DEBUG ) # create file handler which logs even DEBUG messages fh = logging . FileHandler ( logfile ) fh . setLevel ( logging . DEBUG ) fh_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) fh . setFormatter ( fh_formatter ) # create console handler with a INFO log level ch = LoggingHandler () ch . setLevel ( logging . INFO ) ch_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) ch . setFormatter ( ch_formatter ) # add the handlers to the logger logger . addHandler ( fh ) logger . addHandler ( ch ) return logger \u53c2\u8003 https://github.com/tqdm/tqdm#redirecting-writing https://qiita.com/mino-38/items/f09251d18fe3181bfbfd https://waregawa-log.hatenablog.com/entry/2020/01/01/100000 https://stackoverflow.com/questions/384076/how-can-i-color-python-logging-output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 import contextlib import logging import logging.handlers import sys from time import sleep from tqdm import tqdm from tqdm.contrib import DummyTqdmFile class TqdmLoggingHandler ( logging . Handler ): colors = { \"INFO\" : \" \\033 [37m {} \\033 [0m\" } def __init__ ( self , level = logging . NOTSET ): super () . __init__ ( level ) def emit ( self , record ): try : record . msg = TqdmLoggingHandler . colors . get ( record . levelname , \" {} \" ) . format ( record . msg ) msg = self . format ( record ) tqdm . write ( msg , file = sys . stderr ) self . flush () except Exception : self . handleError ( record ) class CustomFormatter ( logging . Formatter ): grey = \" \\x1b [38;20m\" green = \" \\x1b [32;20m\" yellow = \" \\x1b [33;20m\" red = \" \\x1b [31;20m\" bold_red = \" \\x1b [31;1m\" reset = \" \\x1b [0m\" format = \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" FORMATS = { logging . DEBUG : grey + format + reset , logging . INFO : green + format + reset , logging . WARNING : yellow + format + reset , logging . ERROR : red + format + reset , logging . CRITICAL : bold_red + format + reset , } def format ( self , record ): log_fmt = self . FORMATS . get ( record . levelno ) formatter = logging . Formatter ( log_fmt ) return formatter . format ( record ) def setup_logger ( name , logfile = \"log.log\" ): logger = logging . getLogger ( name ) logger . setLevel ( logging . DEBUG ) # create file handler which logs even DEBUG messages fh = logging . handlers . RotatingFileHandler ( logfile , maxBytes = 100000000 , backupCount = 10 ) fh . setLevel ( logging . DEBUG ) fh_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) fh . setFormatter ( fh_formatter ) # create console handler with a INFO log level ch = TqdmLoggingHandler () ch . setLevel ( logging . INFO ) ch . setFormatter ( CustomFormatter ()) # add the handlers to the logger logger . addHandler ( fh ) logger . addHandler ( ch ) return logger logger = setup_logger ( __name__ ) @contextlib . contextmanager def std_out_err_redirect_tqdm (): orig_out_err = sys . stdout , sys . stderr try : sys . stdout , sys . stderr = map ( DummyTqdmFile , orig_out_err ) yield orig_out_err [ 0 ] # Relay exceptions except Exception as exc : raise exc # Always restore sys.stdout/err if necessary finally : sys . stdout , sys . stderr = orig_out_err def some_fun ( i ): logger . info ( \"Fee, fi, fo,\" . split ()[ 2 ]) print ( \"Fee, fi, fo,\" . split ()[ 2 ]) # Redirect stdout to tqdm.write() (don't forget the `as save_stdout`) with std_out_err_redirect_tqdm () as orig_stdout : # tqdm needs the original stdout # and dynamic_ncols=True to autodetect console width for i in tqdm ( range ( 3 ), file = orig_stdout , dynamic_ncols = True ): sleep ( 0.5 ) some_fun ( i ) # After the `with`, printing is restored print ( \"Done!\" ) Image \u00b6 png \u753b\u50cf\u3068 jpg \u753b\u50cf\u306e\u53d6\u308a\u6271\u3044\u306e\u6ce8\u610f\u70b9 https://qiita.com/pashango2/items/145d858eff3c505c100a https://note.nkmk.me/python-pillow-basic/ 1 2 3 4 def jpg2png ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = Image . open ( file_path ) . resize (( 256 , 256 )) . convert ( \"RGBA\" ) img . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .png\" )) \u900f\u660e png \u3092\u767d jpg \u306b\u5909\u63db \u00b6 1 2 3 4 5 6 7 def tra_png2white_jpg ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = Image . open ( file_path ) img . load () background = Image . new ( \"RGB\" , img . size , ( 255 , 255 , 255 )) background . paste ( img , mask = img . split ()[ 3 ]) background . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .jpg\" ), \"JPEG\" , quality = 95 ) \u900f\u660e png \u3092\u767d png \u306b\u5909\u63db \u00b6 1 2 3 4 5 6 def tra_png2white_jpg ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = cv2 . imread ( file_path ) index = np . where ( img [:, :, 3 ] == 0 ) img [ index ] = [ 255 , 255 , 255 , 255 ] cv2 . imwrite ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .png\" ), img ) \u65e9\u304f\u77e5\u3063\u3066\u304a\u304d\u305f\u304b\u3063\u305f matplotlib \u306e\u57fa\u790e\u77e5\u8b58\u3001\u3042\u308b\u3044\u306f\u898b\u305f\u76ee\u306e\u8abf\u6574\u304c\u6357\u308b Artist \u306e\u8a71 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def conv_base64_to_pillow ( img_base64 ): decoded = base64 . b64decode ( img_base64 ) img_io = io . BytesIO ( decoded ) img_pillow = Image . open ( img_io ) . convert ( 'RGB' ) return img_pillow def conv_pillow_to_base64 ( img_pillow ): buff = io . BytesIO () img_pillow . save ( buff , format = \"PNG\" ) img_binary = buff . getvalue () img_base64 = base64 . b64encode ( img_binary ) . decode ( 'utf-8' ) return img_base64 def conv_tensor_to_pillow ( img_tsr ): \"\"\" transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5) \u6b63\u898f\u5316\u3057\u305f Tensor \u3092 Pillow \u306b\u5909\u63db\u3059\u308b\u3002 \"\"\" img_tsr = ( img_tsr . clone () + 1 ) * 0.5 * 255 img_tsr = img_tsr . cpu () . clamp ( 0 , 255 ) img_np = img_tsr . detach () . numpy () . astype ( 'uint8' ) if img_np . shape [ 0 ] == 1 : img_np = img_np . squeeze ( 0 ) img_np = img_np . swapaxes ( 0 , 1 ) . swapaxes ( 1 , 2 ) elif img_np . shape [ 0 ] == 3 : img_np = img_np . swapaxes ( 0 , 1 ) . swapaxes ( 1 , 2 ) #Image.fromarray(img_np).save(save_img_paths) return Image . fromarray ( img_np ) \u753b\u50cf\u3092 Grid \u4e0a\u306b\u8868\u793a \u00b6 \u753b\u50cf\u3092\u305f\u3060\u4e26\u3079\u305f\u3044\u3068\u304d\u306b\u4f7f\u3048\u308b TorchVision PIL to CV \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def cv2pil ( image ): ''' OpenCV\u578b -> PIL\u578b ''' new_image = image . copy () if new_image . ndim == 2 : # \u30e2\u30ce\u30af\u30ed pass elif new_image . shape [ 2 ] == 3 : # \u30ab\u30e9\u30fc new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_BGR2RGB ) elif new_image . shape [ 2 ] == 4 : # \u900f\u904e new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_BGRA2RGBA ) new_image = Image . fromarray ( new_image ) return new_image def pil2cv ( image ): ''' PIL\u578b -> OpenCV\u578b ''' new_image = np . array ( image , dtype = np . uint8 ) if new_image . ndim == 2 : # \u30e2\u30ce\u30af\u30ed pass elif new_image . shape [ 2 ] == 3 : # \u30ab\u30e9\u30fc new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_RGB2BGR ) elif new_image . shape [ 2 ] == 4 : # \u900f\u904e new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_RGBA2BGRA ) return new_image torch \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_file_list ( input_dir ): return sorted ( [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] ) def create_image_array ( image_folder_path , number_of_images = 100 , size = 256 ): file_list = sorted ( get_file_list ( image_folder_path )) random . seed ( 0 ) # random_images = random.sample(file_list, number_of_images) random_images = file_list [: 100 ] image_array = np . zeros (( number_of_images , size , size , 3 ), np . uint8 ) for i , image_path in enumerate ( random_images ): im = Image . open ( image_path ) . resize (( size , size )) img = np . asarray ( im ) image_array [ i ] = img return image_array def torchvision_save ( image_array , save_path , nrows = 10 , padding = 2 ): # \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f images = image_array # save_image\u3067255\u639b\u3051\u308b\u305f\u3081[0,1]\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u3066\u304a\u304f images = ( images / 255.0 ) . astype ( np . float32 ) # 1\u679a\u306b\u7d50\u5408 images = np . transpose ( images , [ 0 , 3 , 1 , 2 ]) images_tensor = torch . as_tensor ( images ) torchvision . utils . save_image ( images_tensor , save_path , nrow = nrows , padding = padding ) 1 2 3 4 5 6 7 8 9 10 11 12 13 import seaborn as sns import matplotlib.pyplot as plt num_rows , num_cols = 10 , 10 f , axes = plt . subplots ( nrows = num_rows , ncols = num_cols , figsize = ( 14 , 14 )) #f.suptitle('Distribution of Features', fontsize=16) for index , ( key , ( a1 , a2 )) in enumerate ( dic_male_five . items ()): i , j = ( index // num_cols , index % num_cols ) axes [ i , j ] . tick_params ( labelbottom = False , labelleft = False , labelright = False , labeltop = False ) sns . histplot ( a1 , bins = 5 , ax = axes [ i , j ]) 1 2 3 4 5 6 7 8 9 10 def tra_png2white_jpg1 ( file_path , output_dir ): img = Image . open ( file_path ) img . load () background = Image . new ( \"RGB\" , img . size , ( 255 , 255 , 255 )) background . paste ( img , mask = img . split ()[ 3 ]) background . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .jpg\" ), \"JPEG\" , quality = 95 , ) \u753b\u50cf\u51e6\u7406\u3001\u30de\u30b9\u30af \u00b6 Python, OpenCV, NumPy \u3067\u753b\u50cf\u306e\u30a2\u30eb\u30d5\u30a1\u30d6\u30ec\u30f3\u30c9\u3068\u30de\u30b9\u30af\u51e6\u7406 \u753b\u50cf\u51e6\u7406\u5165\u9580\u8b1b\u5ea7 : OpenCV \u3068 Python \u3067\u59cb\u3081\u308b\u753b\u50cf\u51e6\u7406 Plot util \u00b6 1 2 3 4 5 def plot_pie ( data , labels ): plt . figure ( figsize = ( 12 , 8 )) plt . rcParams [ 'font.size' ] = 16.0 plt . pie ( data , labels = labels , counterclock = True , autopct = \" %1.1f%% \" ) pandas \u00b6 [Python3 / pandas] dataframe \u306b\u8f9e\u66f8\u578b\u30c7\u30fc\u30bf\u3092 1 \u884c\u305a\u3064\u8ffd\u52a0\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u304d\uff08\u901f\u5ea6\u6bd4\u8f03\uff09 pandas.DataFrame \u306b\uff11\u884c\u305a\u3064\u66f8\u304d\u8db3\u3059\u65e9\u3044\u65b9\u6cd5\u3092\u8abf\u3079\u305f 1 2 3 4 5 6 7 def from_dict_method (): some_df = pd . DataFrame ([], columns = some_dict . keys ()) dict_array = [] for i in range ( 3000 ): dict_array . append ( some_dict ) some_df = pd . concat ([ some_df , pd . DataFrame . from_dict ( dict_array )]) return some_df vectorize \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 def func_1 ( energy_kwh_0 , energy_kwh_1 , energy_kwh_2 ): if energy_kwh_0 > energy_kwh_1 : return 'a' elif energy_kwh_0 > energy_kwh_2 : return 'b' else : return 'c' df [ 'pattern_np_vectorize' ] = np . vectorize ( func_1 )( df [ \"energy_kwh_0\" ], df [ \"energy_kwh_1\" ], df [ \"energy_kwh_2\" ] ) argparse \u00b6 Python \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u89e3\u6790\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u6bd4\u8f03(argparse, click, fire) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 parser = argparse . ArgumentParser ( description = \"Image Annotation\" ) parser . add_argument ( \"--input_dir\" , type = str , default = \"./images\" , help = \"path for input data\" , ) parser . add_argument ( \"--output_dir\" , type = str , default = \"./ntt\" , help = \"path for output csv file\" , ) args = parser . parse_args () parallel, asyncronized \u00b6 subprocess \u00b6 1 2 3 4 5 6 7 8 def subprocess_popen ( max_process , loop_number , cmd ): for i in range ( loop_num ): proc_list = [] proc = subprocess . Popen ( cmd ) proc_list . append ( proc ) if ( i + 1 ) % max_process == 0 or ( i + 1 ) == loop_num : for subproc in proc_list : subproc . wait () joblib \u00b6 1 2 3 4 from joblib import Parallel , delayed def task ( file ): return None Parallel ( n_jobs =- 1 )( delayed ( task )( i ) for i in tqdm ( data )) mpire \u00b6 1 2 3 4 5 6 7 from mpire import WorkerPool def task ( file ): return None with WorkerPool ( n_jobs = 5 ) as pool : results = pool . map ( task , data , progress_bar = True ) multithread, multiprocess \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def task ( file ): return None def main (): with ThreadPoolExecutor () as executor : results = list ( executor . map ( task , files , ** kwargs ), total = len ( my_iter )) return list ( results ) def main (): with ProcessPoolExecutor () as executor : results = list ( executor . map ( task , files ), total = len ( my_iter )) def split_list ( l , n ): \"\"\" https://www.python.ambitious-engineer.com/archives/1843 Other: np.array_split(l, 3) \"\"\" for idx in range ( 0 , len ( l ), n ): yield l [ idx : idx + n ] batch_L = list ( split_list ( L , 32 )) for batch in tqdm ( batch_L ): with ProcessPoolExecutor () as executor : results = list ( executor . map ( task , batch )) # tqdm\u3092\u4f7f\u3046 # https://stackoverflow.com/questions/51601756/use-tqdm-with-concurrent-futures asyncio & multiprocess \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import asyncio import time import random import concurrent.futures def task ( one_task ): return None async def multi_process ( loop , task_list ): executor = concurrent . futures . ProcessPoolExecutor () queue = asyncio . Queue () [ queue . put_nowait ( x ) for x in task_list ] async def p ( q ): while not q . empty (): one_task = await q . get () future = loop . run_in_executor ( executor , task , one_task ) await future # 8\u30d7\u30ed\u30bb\u30b9\u3067\u51e6\u7406 tasks = [ asyncio . create_task ( p ( queue )) for i in range ( 8 )] return await asyncio . wait ( tasks ) def main ( task_list ): loop = asyncio . get_event_loop () loop . run_until_complete ( multi_process ( loop , task_list )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import asyncio from concurrent.futures import ProcessPoolExecutor , ThreadPoolExecutor async def with_processing (): with ProcessPoolExecutor () as executor : tasks = [ ... ] for task in asyncio . as_completed ( tasks ): result = await task ... async def with_threading (): with ThreadPoolExecutor () as executor : tasks = [ ... ] for task in asyncio . as_completed ( tasks ): result = await task ... async def main (): await asyncio . gather ( with_processing (), with_threading ()) asyncio . run ( main ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from concurrent.futures import ProcessPoolExecutor import asyncio import time async def mygen ( u : int = 2 ): i = 0 while i < u : yield i i += 1 def blocking ( delay ): time . sleep ( delay + 1 ) return ( 'EXECUTOR: Completed blocking task number ' + str ( delay + 1 )) async def run_blocking ( executor , task_no , delay ): print ( 'MASTER: Sending to executor blocking task number ' + str ( task_no )) result = await loop . run_in_executor ( executor , blocking , delay ) print ( result ) print ( 'MASTER: Well done executor - you seem to have completed ' 'blocking task number ' + str ( task_no )) async def non_blocking ( loop ): tasks = [] with ProcessPoolExecutor ( max_workers = 2 ) as executor : async for i in mygen (): # spawn the task and let it run in the background tasks . append ( asyncio . create_task ( run_blocking ( executor , i + 1 , i ))) # if there was an exception, retrieve it now await asyncio . gather ( * tasks ) loop = asyncio . get_event_loop () loop . run_until_complete ( non_blocking ( loop )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def task (): futures = [] def main (): with concurrent . futures . ProcessPoolExecutor ( NUM_CORES ) as executor : run = asyncio . run ( task ( num_pages , output_file )) for i in range ( NUM_CORES - 1 ): new_future = executor . submit ( start_scraping , num_pages = PAGES_PER_CORE , output_file = OUTPUT_FILE , i = i ) futures . append ( new_future ) concurrent . futures . wait ( futures ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 async def task (): await do_something () await do_something2 () async def batch_task ( task_batch ): task_list = [ task ( f ) for f in task_batch ] results = await asyncio . gather ( * task_batch ) return results def do_batch_task_async ( task_batch ): loop = asyncio . get_event_loop () results = loop . run_until_complete ( do_batch_task_async ( task_batch )) loop . close () # asyncio.run(do_batch_task_async(task_batch)) if __name__ == \"__main__\" : task_divided = [[], [], []] # task \u3092cpu\u306e\u6570\u306bdivide\u3057\u305f\u3082\u306e with concurrent . futures . ProcessPoolExecutor () as executor : results = list ( tqdm ( executor . map ( do_batch_task_async , task_divided ), total = len ( my_iter ))) asyncio & aiohttp \u00b6 python \u3067\u975e\u540c\u671f\u30ea\u30af\u30a8\u30b9\u30c8\u3059\u308b\u306a\u3089\u5927\u4eba\u3057\u304f aiohttp \u3092\u4f7f\u3044\u307e\u3057\u3087\u3046\u3068\u3044\u3046\u8a71 aiohttp \u3068 asyncio \u3092\u4f7f\u7528\u3057\u305f Python \u306e\u975e\u540c\u671f HTTP \u30ea\u30af\u30a8\u30b9\u30c8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import aiohttp import asyncio import time async def get_data ( session ): async with session . post ( url = url , params = params , headers = headers , data = open ( file_path , \"rb\" ) . read (), timeout = 10 , ) as resp : data = await resp . json () return data async def main ( total_number ): async with aiohttp . ClientSession () as session : tasks = [] tasks . append ( asyncio . ensure_future ( get_data ( session ))) all_data = await asyncio . gather ( * tasks ) \u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0 \u00b6 https://chusotsu-program.com/arimurakasumi-scraping/ https://itstudio.co/2018/12/28/8664/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from bs4 import BeautifulSoup import urllib.request , urllib.error , urllib.parse keyword = '\u30ac\u30c3\u30ad\u30fc' max_page = 3 # \u30da\u30fc\u30b8\u6570\uff0820\u679a/\u30da\u30fc\u30b8\uff09 dst_path = './img-kasumi/' headers = { \"User-Agent\" : \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\" , } cnt = 1 for i in range ( max_page ): cnt += 20 url = 'https://search.yahoo.co.jp/image/search?p= {} &ei=UTF-8&b= {} ' . format ( urllib . parse . quote ( keyword ), cnt ) req = urllib . request . Request ( url = url , headers = headers ) res = urllib . request . urlopen ( req ) soup = BeautifulSoup ( res ) div = soup . find ( 'div' , id = 'gridlist' ) imgs = div . find_all ( 'img' ) for j in range ( len ( imgs )): img = imgs [ j ][ 'src' ] tmp = urllib . request . urlopen ( img ) data = tmp . read () file_name = dst_path + 'page' + str ( i + 1 ) + '_img' + str ( j + 1 ) + '.jpg' with open ( file_name , 'wb' ) as save_img : save_img . write ( data ) subprocess \u00b6 \u51fa\u529b\u3092\u8a18\u9332\u3059\u308b \u00b6 1 2 3 4 5 6 7 proc = subprocess . run ([ \"ls\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) print ( proc . stdout . decode ( \"utf8\" )) # \u51fa\u529b\u3092\u8a18\u9332 p = subprocess . Popen ( mycmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) for line in iter ( p . stdout . readline , b '' ): print ( line . rstrip () . decode ( \"utf8\" )) gokart, luigi \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import logging import time import luigi import gokart class TaskA ( gokart . TaskOnKart ): param = luigi . Parameter () def run ( self ): output = \"Hi {} \" . format ( self . param ) time . sleep ( 10.0 ) self . dump ( output ) class TaskB ( gokart . TaskOnKart ): param = luigi . Parameter () def requires ( self ): return dict ( a = TaskA ( param = \"called by TaskB\" ), b = TaskA ( param = \"aaaaaa\" + self . param ) ) # return dict(a=TaskA(serialized_task_definition_check=True), b=TaskA()) def run ( self ): res = self . load ( \"a\" ) time . sleep ( 5.0 ) print ( \"I am waited\" ) # res = self.load('caaa') self . dump ( res ) class TaskC ( gokart . TaskOnKart ): param = luigi . Parameter () task = gokart . TaskInstanceParameter def requires ( self ): self . task = TaskB ( param = self . param ) return self . task def run ( self ): summary = gokart . tree . task_info . make_task_info_as_table ( self . task , []) show_columns = [ \"name\" , \"processing_time\" , \"is_complete\" ] print ( summary . columns ) print ( summary [ show_columns ]) string = \"aeeee\" task = TaskC ( param = string ) gokart . build ( TaskC ( param = string ), log_level = logging . DEBUG , return_value = False ) tqdm \u00b6 print \u3092\u4f7f\u3044\u3064\u3064\u3001tqdm \u3092\u30bf\u30fc\u30df\u30ca\u30eb\u306e\u5e45\u306b\u5408\u308f\u305b\u3066\u5909\u5316 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import contextlib import sys from time import sleep from tqdm import tqdm from tqdm.contrib import DummyTqdmFile @contextlib . contextmanager def std_out_err_redirect_tqdm (): orig_out_err = sys . stdout , sys . stderr try : sys . stdout , sys . stderr = map ( DummyTqdmFile , orig_out_err ) yield orig_out_err [ 0 ] # Relay exceptions except Exception as exc : raise exc # Always restore sys.stdout/err if necessary finally : sys . stdout , sys . stderr = orig_out_err def some_fun ( i ): print ( \"Fee, fi, fo,\" . split ()[ 2 ]) # Redirect stdout to tqdm.write() (don't forget the `as save_stdout`) with std_out_err_redirect_tqdm () as orig_stdout : # tqdm needs the original stdout # and dynamic_ncols=True to autodetect console width for i in tqdm ( range ( 3 ), file = orig_stdout , dynamic_ncols = True ): sleep ( 0.5 ) some_fun ( i ) \u540d\u524d\u53d6\u5f97 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def get_parameters_of_func ( offset = None ): \"\"\"Get a dictionary of paramteres of the function. Parameters ---------- offset : int default value is None Return ------ dictionary The dictionary includes pairs of paremeter's name and the corresponding values. References ---------- [1] https://tottoto.net/python3-get-args-of-current-function/ \"\"\" parent_frame = inspect . currentframe () . f_back info = inspect . getargvalues ( parent_frame ) return { key : info . locals [ key ] for key in info . args [ offset :]} 1 function_name = inspect . currentframe () . f_code . co_name","title":"Python util"},{"location":"util/#python","text":"https://qiita.com/hiroyuki_mrp/items/8bbd9ab6c16601e87a9c","title":"python \u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea"},{"location":"util/#_1","text":"\u3010VS Code\u3011Black \u3068 Flake8 \u3092\u4f7f\u3063\u3066\u304d\u308c\u3044\u306a Python \u30b3\u30fc\u30c9\u3092\u66f8\u304f\uff01\uff01 Python \u306e\u30b3\u30fc\u30c9\u3092\u30ad\u30ec\u30a4\u306b\u66f8\u304d\u305f\u3044\uff01(VSCode \u306b flake8 & black \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb) flake8\u3001black\u3001isort\u3001mypy \u3092 VS Code \u4e0a\u3067\u4f7f\u7528\u3059\u308b GCP \u306a\u3069\u306e\u5834\u5408\u306f\u500b\u5225\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\uff08pip install black\uff09,\u8a2d\u5b9a(ssh \u30ef\u30fc\u30af\u30b9\u30da\u30fc\u30b9)\u304c\u5fc5\u8981\u3001error lense \u306f 500 \u307e\u3067\u306e delay \u304c\u5fc5\u8981","title":"\u30ea\u30f3\u30bf\u30fc"},{"location":"util/#util-pyfile-templates","text":"Pypy, pip \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Python \u3067\u81ea\u5206\u3060\u3051\u306e\u30af\u30bd\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f5c\u308b\u65b9\u6cd5 Python \u3067\u4f5c\u3063\u305f\u30b3\u30de\u30f3\u30c9\u3092 GitHub \u7d4c\u7531\u3067 pip \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u53ef\u80fd\u306b\u3059\u308b","title":"util pyfile templates"},{"location":"util/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import glob import os import time from functools import wraps import argparse from tqdm import tqdm @stop_watch def func ( file_list , output_dir ): return None if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--input_dir\" , type = str , required = True , help = \"path for input dir\" , ) parser . add_argument ( \"--output_dir\" , type = str , required = True , help = \"path for output dir\" , ) args = parser . parse_args () check_create_dir ( args . output_dir ) file_list = get_file_list ( args . input_dir ) func ( file_list , args . output_dir )","title":"\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8"},{"location":"util/#_3","text":"1 2 3 4 def check_create_dir ( path ): # check if there is the directory. if not create a new directory. if not os . path . isdir ( path ): os . makedirs ( path )","title":"\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u78ba\u8a8d\u3068\u4f5c\u6210"},{"location":"util/#_4","text":"1 2 3 4 5 6 7 8 9 def get_file_list ( input_dir ): # get the list of files in input directory. return sorted ( [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pathlib import Path from typing import List def list_file_paths ( dir_path : str ) -> List [ str ]: \"\"\" List file paths in a directory. Parameters ---------- dir_path : str Path for the directory Returns ------- List[str] List of the file paths in the directory \"\"\" return sorted ([ str ( path ) for path in Path ( dir_path ) . rglob ( \"*\" ) if path . is_file ()]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def add_suffix_to_all_file_paths ( dir_path : str ) -> List [ str ]: \"\"\" Add suffix to all the file name Parameters ---------- dir_path : str Path for the directory Returns ------- List[str] List of the file paths in the directory \"\"\" return sorted ( [ str ( path . rename ( str ( path . parent / Path ( path . stem + \"_suffix\" + path . suffix )) ) ) for path in Path ( dir_path ) . rglob ( \"*\" ) if path . is_file () ] ) 1 2 3 4 5 6 7 8 9 10 11 ### def stop_watch ( func ) : @wraps ( func ) def wrapper ( * args , ** kargs ) : start = time . time () result = func ( * args , ** kargs ) elapsed_time = time . time () - start print ( f \"total time of { func . __name__ } : { elapsed_time } \" ) return result return wrapper 1 2 3 4 5 6 7 8 9 10 11 12 13 from time import time from contextlib import contextmanager import cudf @contextmanager def timer ( name ): start = time () yield print ( f '[ { name } ] done in { time () - start : .2f } s' ) with timer ( 'name' ): some_function () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @stop_watch def func ( file_list , output_dir ): return None if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--input_dir\" , type = str , required = True , help = \"path for input dir\" , ) parser . add_argument ( \"--output_dir\" , type = str , required = True , help = \"path for output dir\" , ) args = parser . parse_args () check_create_dir ( args . output_dir ) file_list = get_file_list ( args . input_dir ) func ( file_list , args . output_dir )","title":"\u30d5\u30a1\u30a4\u30eb\u30ea\u30b9\u30c8\u306e\u53d6\u5f97"},{"location":"util/#general-util","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def load_pickle ( load_path ): # pd.read_pickle(load_path) with open ( load_path , mode = \"rb\" ) as f : return pickle . load ( f ) def save_pickle ( object , save_path ): # pd.to_pickle(object, save_path) with open ( save_path , mode = \"wb\" ) as f : pickle . dump ( object , f ) def check_create_dir ( path ): if not os . path . isdir ( path ): os . makedirs ( path , exist_ok = True ) def get_file_list ( input_dir ): return [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] def split_list ( l , n ): \"\"\" https://www.python.ambitious-engineer.com/archives/1843 Other: np.array_split(l, 3) \"\"\" for idx in range ( 0 , len ( l ), n ): yield l [ idx : idx + n ] def split_list ( l , n ): return np . array_split ( l , n ) def json2dataframe ( json_data ): return pd . json_normalize ( json_data , record_path = 'data' ) def flatten_dict ( dict ): df = pd . json_normalize ( dict [ \"data\" ][ 0 ], sep = \"_\" ) return df . to_dict ( orient = \"records\" )[ 0 ] def flatten_dict ( d , parent_key = \"\" , sep = \"_\" ): items = [] for k , v in d . items (): new_key = parent_key + sep + k if parent_key else k if isinstance ( v , collections . MutableMapping ): items . extend ( flatten ( v , new_key , sep = sep ) . items ()) else : items . append (( new_key , v )) return dict ( items ) from functools import wraps import time def stop_watch ( func ) : @wraps ( func ) def wrapper ( * args , ** kargs ) : start = time . time () result = func ( * args , ** kargs ) elapsed_time = time . time () - start print ( f \"total time of { func . __name__ } : { elapsed_time } \" ) return result return wrapper def seed_everything ( seed = 2021 ): random . seed ( seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False def git_commits ( rand ): def func_decorator ( my_func ): repo = git . Repo ( \"/work\" ) repo . config_writer () . set_value ( \"user\" , \"name\" , \"your_userID\" ) . release () repo . config_writer () . set_value ( \"user\" , \"email\" , \"your_email@gmail.com\" ) . release () repo . git . diff ( \"HEAD\" ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _running\" ) repo . git . push ( \"origin\" , \"HEAD\" ) logger . info ( f \"git pushed to the remote origin\" ) def decorator_wrapper ( * args , ** kwargs ): my_func ( * args , ** kwargs ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _done\" ) repo . git . push ( \"origin\" , \"HEAD\" ) logger . info ( f \"git pushed to the remote origin\" ) return decorator_wrapper return func_decorator def randomname ( n ): return \"\" . join ( random . choices ( string . ascii_letters + string . digits , k = n )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def save_mlflow_pickle ( obj , file_name ): with tempfile . TemporaryDirectory () as d : artifact_path = pathlib . Path ( d ) / file_name save_pickle ( obj , artifact_path ) mlflow . log_artifact ( artifact_path ) def save_mlflow_model ( original_path ): with tempfile . TemporaryDirectory () as d : artifact_path = pathlib . Path ( d ) / os . path . basename ( original_path ) shutil . copy ( original_path , artifact_path ) mlflow . log_artifact ( artifact_path ) def get_git_revision_short_hash (): short_hash = subprocess . check_output ([ 'git' , 'rev-parse' , '--short' , 'HEAD' ]) short_hash = str ( short_hash , \"utf-8\" ) . strip () return short_hash","title":"General util"},{"location":"util/#logger","text":"\u30ed\u30b0\u51fa\u529b\u306e\u305f\u3081\u306e print \u3068 import logging \u306f\u3084\u3081\u3066\u307b\u3057\u3044","title":"logger"},{"location":"util/#python-print","text":"1 2 3 4 5 6 7 8 9 from logging import getLogger , StreamHandler , DEBUG logger = getLogger ( __name__ ) handler = StreamHandler () handler . setLevel ( DEBUG ) logger . setLevel ( DEBUG ) logger . addHandler ( handler ) logger . propagate = False logger . debug ( 'hello' ) https://qiita.com/shotakaha/items/0fa2db1dc8253c83e2bb 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import logging import logging.handlers def setup_logger ( name , logfile = 'log.log' ): logger = logging . getLogger ( name ) logger . setLevel ( logging . DEBUG ) # create file handler which logs even DEBUG messages fh = logging . handlers . RotatingFileHandler ( logfile , maxBytes = 100000000 , backupCount = 10 ) fh . setLevel ( logging . DEBUG ) fh_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) fh . setFormatter ( fh_formatter ) # create console handler with a INFO log level ch = LoggingHandler () ch . setLevel ( logging . INFO ) ch_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) ch . setFormatter ( ch_formatter ) # add the handlers to the logger logger . addHandler ( fh ) logger . addHandler ( ch ) return logger def setup_logger ( name , logfile = 'log.log' ): logger = logging . getLogger ( name ) logger . setLevel ( logging . DEBUG ) # create file handler which logs even DEBUG messages fh = logging . FileHandler ( logfile ) fh . setLevel ( logging . DEBUG ) fh_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) fh . setFormatter ( fh_formatter ) # create console handler with a INFO log level ch = LoggingHandler () ch . setLevel ( logging . INFO ) ch_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) ch . setFormatter ( ch_formatter ) # add the handlers to the logger logger . addHandler ( fh ) logger . addHandler ( ch ) return logger \u53c2\u8003 https://github.com/tqdm/tqdm#redirecting-writing https://qiita.com/mino-38/items/f09251d18fe3181bfbfd https://waregawa-log.hatenablog.com/entry/2020/01/01/100000 https://stackoverflow.com/questions/384076/how-can-i-color-python-logging-output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 import contextlib import logging import logging.handlers import sys from time import sleep from tqdm import tqdm from tqdm.contrib import DummyTqdmFile class TqdmLoggingHandler ( logging . Handler ): colors = { \"INFO\" : \" \\033 [37m {} \\033 [0m\" } def __init__ ( self , level = logging . NOTSET ): super () . __init__ ( level ) def emit ( self , record ): try : record . msg = TqdmLoggingHandler . colors . get ( record . levelname , \" {} \" ) . format ( record . msg ) msg = self . format ( record ) tqdm . write ( msg , file = sys . stderr ) self . flush () except Exception : self . handleError ( record ) class CustomFormatter ( logging . Formatter ): grey = \" \\x1b [38;20m\" green = \" \\x1b [32;20m\" yellow = \" \\x1b [33;20m\" red = \" \\x1b [31;20m\" bold_red = \" \\x1b [31;1m\" reset = \" \\x1b [0m\" format = \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" FORMATS = { logging . DEBUG : grey + format + reset , logging . INFO : green + format + reset , logging . WARNING : yellow + format + reset , logging . ERROR : red + format + reset , logging . CRITICAL : bold_red + format + reset , } def format ( self , record ): log_fmt = self . FORMATS . get ( record . levelno ) formatter = logging . Formatter ( log_fmt ) return formatter . format ( record ) def setup_logger ( name , logfile = \"log.log\" ): logger = logging . getLogger ( name ) logger . setLevel ( logging . DEBUG ) # create file handler which logs even DEBUG messages fh = logging . handlers . RotatingFileHandler ( logfile , maxBytes = 100000000 , backupCount = 10 ) fh . setLevel ( logging . DEBUG ) fh_formatter = logging . Formatter ( \"[ %(asctime)s ] [ %(levelname)s ] [ %(process)d ] [ %(name)s ] [ %(funcName)s ] [ %(lineno)d ] %(message)s \" ) fh . setFormatter ( fh_formatter ) # create console handler with a INFO log level ch = TqdmLoggingHandler () ch . setLevel ( logging . INFO ) ch . setFormatter ( CustomFormatter ()) # add the handlers to the logger logger . addHandler ( fh ) logger . addHandler ( ch ) return logger logger = setup_logger ( __name__ ) @contextlib . contextmanager def std_out_err_redirect_tqdm (): orig_out_err = sys . stdout , sys . stderr try : sys . stdout , sys . stderr = map ( DummyTqdmFile , orig_out_err ) yield orig_out_err [ 0 ] # Relay exceptions except Exception as exc : raise exc # Always restore sys.stdout/err if necessary finally : sys . stdout , sys . stderr = orig_out_err def some_fun ( i ): logger . info ( \"Fee, fi, fo,\" . split ()[ 2 ]) print ( \"Fee, fi, fo,\" . split ()[ 2 ]) # Redirect stdout to tqdm.write() (don't forget the `as save_stdout`) with std_out_err_redirect_tqdm () as orig_stdout : # tqdm needs the original stdout # and dynamic_ncols=True to autodetect console width for i in tqdm ( range ( 3 ), file = orig_stdout , dynamic_ncols = True ): sleep ( 0.5 ) some_fun ( i ) # After the `with`, printing is restored print ( \"Done!\" )","title":"Python \u3067 print \u3092\u5352\u696d\u3057\u3066\u30ed\u30b0\u51fa\u529b\u3092\u3044\u3044\u611f\u3058\u306b\u3059\u308b"},{"location":"util/#image","text":"png \u753b\u50cf\u3068 jpg \u753b\u50cf\u306e\u53d6\u308a\u6271\u3044\u306e\u6ce8\u610f\u70b9 https://qiita.com/pashango2/items/145d858eff3c505c100a https://note.nkmk.me/python-pillow-basic/ 1 2 3 4 def jpg2png ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = Image . open ( file_path ) . resize (( 256 , 256 )) . convert ( \"RGBA\" ) img . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .png\" ))","title":"Image"},{"location":"util/#png-jpg","text":"1 2 3 4 5 6 7 def tra_png2white_jpg ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = Image . open ( file_path ) img . load () background = Image . new ( \"RGB\" , img . size , ( 255 , 255 , 255 )) background . paste ( img , mask = img . split ()[ 3 ]) background . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .jpg\" ), \"JPEG\" , quality = 95 )","title":"\u900f\u660e png \u3092\u767d jpg \u306b\u5909\u63db"},{"location":"util/#png-png","text":"1 2 3 4 5 6 def tra_png2white_jpg ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = cv2 . imread ( file_path ) index = np . where ( img [:, :, 3 ] == 0 ) img [ index ] = [ 255 , 255 , 255 , 255 ] cv2 . imwrite ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .png\" ), img ) \u65e9\u304f\u77e5\u3063\u3066\u304a\u304d\u305f\u304b\u3063\u305f matplotlib \u306e\u57fa\u790e\u77e5\u8b58\u3001\u3042\u308b\u3044\u306f\u898b\u305f\u76ee\u306e\u8abf\u6574\u304c\u6357\u308b Artist \u306e\u8a71 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def conv_base64_to_pillow ( img_base64 ): decoded = base64 . b64decode ( img_base64 ) img_io = io . BytesIO ( decoded ) img_pillow = Image . open ( img_io ) . convert ( 'RGB' ) return img_pillow def conv_pillow_to_base64 ( img_pillow ): buff = io . BytesIO () img_pillow . save ( buff , format = \"PNG\" ) img_binary = buff . getvalue () img_base64 = base64 . b64encode ( img_binary ) . decode ( 'utf-8' ) return img_base64 def conv_tensor_to_pillow ( img_tsr ): \"\"\" transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5) \u6b63\u898f\u5316\u3057\u305f Tensor \u3092 Pillow \u306b\u5909\u63db\u3059\u308b\u3002 \"\"\" img_tsr = ( img_tsr . clone () + 1 ) * 0.5 * 255 img_tsr = img_tsr . cpu () . clamp ( 0 , 255 ) img_np = img_tsr . detach () . numpy () . astype ( 'uint8' ) if img_np . shape [ 0 ] == 1 : img_np = img_np . squeeze ( 0 ) img_np = img_np . swapaxes ( 0 , 1 ) . swapaxes ( 1 , 2 ) elif img_np . shape [ 0 ] == 3 : img_np = img_np . swapaxes ( 0 , 1 ) . swapaxes ( 1 , 2 ) #Image.fromarray(img_np).save(save_img_paths) return Image . fromarray ( img_np )","title":"\u900f\u660e png \u3092\u767d png \u306b\u5909\u63db"},{"location":"util/#grid","text":"\u753b\u50cf\u3092\u305f\u3060\u4e26\u3079\u305f\u3044\u3068\u304d\u306b\u4f7f\u3048\u308b TorchVision","title":"\u753b\u50cf\u3092 Grid \u4e0a\u306b\u8868\u793a"},{"location":"util/#pil-to-cv","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def cv2pil ( image ): ''' OpenCV\u578b -> PIL\u578b ''' new_image = image . copy () if new_image . ndim == 2 : # \u30e2\u30ce\u30af\u30ed pass elif new_image . shape [ 2 ] == 3 : # \u30ab\u30e9\u30fc new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_BGR2RGB ) elif new_image . shape [ 2 ] == 4 : # \u900f\u904e new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_BGRA2RGBA ) new_image = Image . fromarray ( new_image ) return new_image def pil2cv ( image ): ''' PIL\u578b -> OpenCV\u578b ''' new_image = np . array ( image , dtype = np . uint8 ) if new_image . ndim == 2 : # \u30e2\u30ce\u30af\u30ed pass elif new_image . shape [ 2 ] == 3 : # \u30ab\u30e9\u30fc new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_RGB2BGR ) elif new_image . shape [ 2 ] == 4 : # \u900f\u904e new_image = cv2 . cvtColor ( new_image , cv2 . COLOR_RGBA2BGRA ) return new_image","title":"PIL to CV"},{"location":"util/#torch","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def get_file_list ( input_dir ): return sorted ( [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] ) def create_image_array ( image_folder_path , number_of_images = 100 , size = 256 ): file_list = sorted ( get_file_list ( image_folder_path )) random . seed ( 0 ) # random_images = random.sample(file_list, number_of_images) random_images = file_list [: 100 ] image_array = np . zeros (( number_of_images , size , size , 3 ), np . uint8 ) for i , image_path in enumerate ( random_images ): im = Image . open ( image_path ) . resize (( size , size )) img = np . asarray ( im ) image_array [ i ] = img return image_array def torchvision_save ( image_array , save_path , nrows = 10 , padding = 2 ): # \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f images = image_array # save_image\u3067255\u639b\u3051\u308b\u305f\u3081[0,1]\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u3066\u304a\u304f images = ( images / 255.0 ) . astype ( np . float32 ) # 1\u679a\u306b\u7d50\u5408 images = np . transpose ( images , [ 0 , 3 , 1 , 2 ]) images_tensor = torch . as_tensor ( images ) torchvision . utils . save_image ( images_tensor , save_path , nrow = nrows , padding = padding ) 1 2 3 4 5 6 7 8 9 10 11 12 13 import seaborn as sns import matplotlib.pyplot as plt num_rows , num_cols = 10 , 10 f , axes = plt . subplots ( nrows = num_rows , ncols = num_cols , figsize = ( 14 , 14 )) #f.suptitle('Distribution of Features', fontsize=16) for index , ( key , ( a1 , a2 )) in enumerate ( dic_male_five . items ()): i , j = ( index // num_cols , index % num_cols ) axes [ i , j ] . tick_params ( labelbottom = False , labelleft = False , labelright = False , labeltop = False ) sns . histplot ( a1 , bins = 5 , ax = axes [ i , j ]) 1 2 3 4 5 6 7 8 9 10 def tra_png2white_jpg1 ( file_path , output_dir ): img = Image . open ( file_path ) img . load () background = Image . new ( \"RGB\" , img . size , ( 255 , 255 , 255 )) background . paste ( img , mask = img . split ()[ 3 ]) background . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .jpg\" ), \"JPEG\" , quality = 95 , )","title":"torch"},{"location":"util/#_5","text":"Python, OpenCV, NumPy \u3067\u753b\u50cf\u306e\u30a2\u30eb\u30d5\u30a1\u30d6\u30ec\u30f3\u30c9\u3068\u30de\u30b9\u30af\u51e6\u7406 \u753b\u50cf\u51e6\u7406\u5165\u9580\u8b1b\u5ea7 : OpenCV \u3068 Python \u3067\u59cb\u3081\u308b\u753b\u50cf\u51e6\u7406","title":"\u753b\u50cf\u51e6\u7406\u3001\u30de\u30b9\u30af"},{"location":"util/#plot-util","text":"1 2 3 4 5 def plot_pie ( data , labels ): plt . figure ( figsize = ( 12 , 8 )) plt . rcParams [ 'font.size' ] = 16.0 plt . pie ( data , labels = labels , counterclock = True , autopct = \" %1.1f%% \" )","title":"Plot util"},{"location":"util/#pandas","text":"[Python3 / pandas] dataframe \u306b\u8f9e\u66f8\u578b\u30c7\u30fc\u30bf\u3092 1 \u884c\u305a\u3064\u8ffd\u52a0\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u304d\uff08\u901f\u5ea6\u6bd4\u8f03\uff09 pandas.DataFrame \u306b\uff11\u884c\u305a\u3064\u66f8\u304d\u8db3\u3059\u65e9\u3044\u65b9\u6cd5\u3092\u8abf\u3079\u305f 1 2 3 4 5 6 7 def from_dict_method (): some_df = pd . DataFrame ([], columns = some_dict . keys ()) dict_array = [] for i in range ( 3000 ): dict_array . append ( some_dict ) some_df = pd . concat ([ some_df , pd . DataFrame . from_dict ( dict_array )]) return some_df","title":"pandas"},{"location":"util/#vectorize","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 def func_1 ( energy_kwh_0 , energy_kwh_1 , energy_kwh_2 ): if energy_kwh_0 > energy_kwh_1 : return 'a' elif energy_kwh_0 > energy_kwh_2 : return 'b' else : return 'c' df [ 'pattern_np_vectorize' ] = np . vectorize ( func_1 )( df [ \"energy_kwh_0\" ], df [ \"energy_kwh_1\" ], df [ \"energy_kwh_2\" ] )","title":"vectorize"},{"location":"util/#argparse","text":"Python \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u89e3\u6790\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u6bd4\u8f03(argparse, click, fire) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 parser = argparse . ArgumentParser ( description = \"Image Annotation\" ) parser . add_argument ( \"--input_dir\" , type = str , default = \"./images\" , help = \"path for input data\" , ) parser . add_argument ( \"--output_dir\" , type = str , default = \"./ntt\" , help = \"path for output csv file\" , ) args = parser . parse_args ()","title":"argparse"},{"location":"util/#parallel-asyncronized","text":"","title":"parallel, asyncronized"},{"location":"util/#subprocess","text":"1 2 3 4 5 6 7 8 def subprocess_popen ( max_process , loop_number , cmd ): for i in range ( loop_num ): proc_list = [] proc = subprocess . Popen ( cmd ) proc_list . append ( proc ) if ( i + 1 ) % max_process == 0 or ( i + 1 ) == loop_num : for subproc in proc_list : subproc . wait ()","title":"subprocess"},{"location":"util/#joblib","text":"1 2 3 4 from joblib import Parallel , delayed def task ( file ): return None Parallel ( n_jobs =- 1 )( delayed ( task )( i ) for i in tqdm ( data ))","title":"joblib"},{"location":"util/#mpire","text":"1 2 3 4 5 6 7 from mpire import WorkerPool def task ( file ): return None with WorkerPool ( n_jobs = 5 ) as pool : results = pool . map ( task , data , progress_bar = True )","title":"mpire"},{"location":"util/#multithread-multiprocess","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def task ( file ): return None def main (): with ThreadPoolExecutor () as executor : results = list ( executor . map ( task , files , ** kwargs ), total = len ( my_iter )) return list ( results ) def main (): with ProcessPoolExecutor () as executor : results = list ( executor . map ( task , files ), total = len ( my_iter )) def split_list ( l , n ): \"\"\" https://www.python.ambitious-engineer.com/archives/1843 Other: np.array_split(l, 3) \"\"\" for idx in range ( 0 , len ( l ), n ): yield l [ idx : idx + n ] batch_L = list ( split_list ( L , 32 )) for batch in tqdm ( batch_L ): with ProcessPoolExecutor () as executor : results = list ( executor . map ( task , batch )) # tqdm\u3092\u4f7f\u3046 # https://stackoverflow.com/questions/51601756/use-tqdm-with-concurrent-futures","title":"multithread, multiprocess"},{"location":"util/#asyncio-multiprocess","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import asyncio import time import random import concurrent.futures def task ( one_task ): return None async def multi_process ( loop , task_list ): executor = concurrent . futures . ProcessPoolExecutor () queue = asyncio . Queue () [ queue . put_nowait ( x ) for x in task_list ] async def p ( q ): while not q . empty (): one_task = await q . get () future = loop . run_in_executor ( executor , task , one_task ) await future # 8\u30d7\u30ed\u30bb\u30b9\u3067\u51e6\u7406 tasks = [ asyncio . create_task ( p ( queue )) for i in range ( 8 )] return await asyncio . wait ( tasks ) def main ( task_list ): loop = asyncio . get_event_loop () loop . run_until_complete ( multi_process ( loop , task_list )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import asyncio from concurrent.futures import ProcessPoolExecutor , ThreadPoolExecutor async def with_processing (): with ProcessPoolExecutor () as executor : tasks = [ ... ] for task in asyncio . as_completed ( tasks ): result = await task ... async def with_threading (): with ThreadPoolExecutor () as executor : tasks = [ ... ] for task in asyncio . as_completed ( tasks ): result = await task ... async def main (): await asyncio . gather ( with_processing (), with_threading ()) asyncio . run ( main ()) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from concurrent.futures import ProcessPoolExecutor import asyncio import time async def mygen ( u : int = 2 ): i = 0 while i < u : yield i i += 1 def blocking ( delay ): time . sleep ( delay + 1 ) return ( 'EXECUTOR: Completed blocking task number ' + str ( delay + 1 )) async def run_blocking ( executor , task_no , delay ): print ( 'MASTER: Sending to executor blocking task number ' + str ( task_no )) result = await loop . run_in_executor ( executor , blocking , delay ) print ( result ) print ( 'MASTER: Well done executor - you seem to have completed ' 'blocking task number ' + str ( task_no )) async def non_blocking ( loop ): tasks = [] with ProcessPoolExecutor ( max_workers = 2 ) as executor : async for i in mygen (): # spawn the task and let it run in the background tasks . append ( asyncio . create_task ( run_blocking ( executor , i + 1 , i ))) # if there was an exception, retrieve it now await asyncio . gather ( * tasks ) loop = asyncio . get_event_loop () loop . run_until_complete ( non_blocking ( loop )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def task (): futures = [] def main (): with concurrent . futures . ProcessPoolExecutor ( NUM_CORES ) as executor : run = asyncio . run ( task ( num_pages , output_file )) for i in range ( NUM_CORES - 1 ): new_future = executor . submit ( start_scraping , num_pages = PAGES_PER_CORE , output_file = OUTPUT_FILE , i = i ) futures . append ( new_future ) concurrent . futures . wait ( futures ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 async def task (): await do_something () await do_something2 () async def batch_task ( task_batch ): task_list = [ task ( f ) for f in task_batch ] results = await asyncio . gather ( * task_batch ) return results def do_batch_task_async ( task_batch ): loop = asyncio . get_event_loop () results = loop . run_until_complete ( do_batch_task_async ( task_batch )) loop . close () # asyncio.run(do_batch_task_async(task_batch)) if __name__ == \"__main__\" : task_divided = [[], [], []] # task \u3092cpu\u306e\u6570\u306bdivide\u3057\u305f\u3082\u306e with concurrent . futures . ProcessPoolExecutor () as executor : results = list ( tqdm ( executor . map ( do_batch_task_async , task_divided ), total = len ( my_iter )))","title":"asyncio &amp; multiprocess"},{"location":"util/#asyncio-aiohttp","text":"python \u3067\u975e\u540c\u671f\u30ea\u30af\u30a8\u30b9\u30c8\u3059\u308b\u306a\u3089\u5927\u4eba\u3057\u304f aiohttp \u3092\u4f7f\u3044\u307e\u3057\u3087\u3046\u3068\u3044\u3046\u8a71 aiohttp \u3068 asyncio \u3092\u4f7f\u7528\u3057\u305f Python \u306e\u975e\u540c\u671f HTTP \u30ea\u30af\u30a8\u30b9\u30c8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import aiohttp import asyncio import time async def get_data ( session ): async with session . post ( url = url , params = params , headers = headers , data = open ( file_path , \"rb\" ) . read (), timeout = 10 , ) as resp : data = await resp . json () return data async def main ( total_number ): async with aiohttp . ClientSession () as session : tasks = [] tasks . append ( asyncio . ensure_future ( get_data ( session ))) all_data = await asyncio . gather ( * tasks )","title":"asyncio &amp; aiohttp"},{"location":"util/#_6","text":"https://chusotsu-program.com/arimurakasumi-scraping/ https://itstudio.co/2018/12/28/8664/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from bs4 import BeautifulSoup import urllib.request , urllib.error , urllib.parse keyword = '\u30ac\u30c3\u30ad\u30fc' max_page = 3 # \u30da\u30fc\u30b8\u6570\uff0820\u679a/\u30da\u30fc\u30b8\uff09 dst_path = './img-kasumi/' headers = { \"User-Agent\" : \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\" , } cnt = 1 for i in range ( max_page ): cnt += 20 url = 'https://search.yahoo.co.jp/image/search?p= {} &ei=UTF-8&b= {} ' . format ( urllib . parse . quote ( keyword ), cnt ) req = urllib . request . Request ( url = url , headers = headers ) res = urllib . request . urlopen ( req ) soup = BeautifulSoup ( res ) div = soup . find ( 'div' , id = 'gridlist' ) imgs = div . find_all ( 'img' ) for j in range ( len ( imgs )): img = imgs [ j ][ 'src' ] tmp = urllib . request . urlopen ( img ) data = tmp . read () file_name = dst_path + 'page' + str ( i + 1 ) + '_img' + str ( j + 1 ) + '.jpg' with open ( file_name , 'wb' ) as save_img : save_img . write ( data )","title":"\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0"},{"location":"util/#subprocess_1","text":"","title":"subprocess"},{"location":"util/#_7","text":"1 2 3 4 5 6 7 proc = subprocess . run ([ \"ls\" ], stdout = subprocess . PIPE , stderr = subprocess . PIPE ) print ( proc . stdout . decode ( \"utf8\" )) # \u51fa\u529b\u3092\u8a18\u9332 p = subprocess . Popen ( mycmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) for line in iter ( p . stdout . readline , b '' ): print ( line . rstrip () . decode ( \"utf8\" ))","title":"\u51fa\u529b\u3092\u8a18\u9332\u3059\u308b"},{"location":"util/#gokart-luigi","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import logging import time import luigi import gokart class TaskA ( gokart . TaskOnKart ): param = luigi . Parameter () def run ( self ): output = \"Hi {} \" . format ( self . param ) time . sleep ( 10.0 ) self . dump ( output ) class TaskB ( gokart . TaskOnKart ): param = luigi . Parameter () def requires ( self ): return dict ( a = TaskA ( param = \"called by TaskB\" ), b = TaskA ( param = \"aaaaaa\" + self . param ) ) # return dict(a=TaskA(serialized_task_definition_check=True), b=TaskA()) def run ( self ): res = self . load ( \"a\" ) time . sleep ( 5.0 ) print ( \"I am waited\" ) # res = self.load('caaa') self . dump ( res ) class TaskC ( gokart . TaskOnKart ): param = luigi . Parameter () task = gokart . TaskInstanceParameter def requires ( self ): self . task = TaskB ( param = self . param ) return self . task def run ( self ): summary = gokart . tree . task_info . make_task_info_as_table ( self . task , []) show_columns = [ \"name\" , \"processing_time\" , \"is_complete\" ] print ( summary . columns ) print ( summary [ show_columns ]) string = \"aeeee\" task = TaskC ( param = string ) gokart . build ( TaskC ( param = string ), log_level = logging . DEBUG , return_value = False )","title":"gokart, luigi"},{"location":"util/#tqdm","text":"print \u3092\u4f7f\u3044\u3064\u3064\u3001tqdm \u3092\u30bf\u30fc\u30df\u30ca\u30eb\u306e\u5e45\u306b\u5408\u308f\u305b\u3066\u5909\u5316 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import contextlib import sys from time import sleep from tqdm import tqdm from tqdm.contrib import DummyTqdmFile @contextlib . contextmanager def std_out_err_redirect_tqdm (): orig_out_err = sys . stdout , sys . stderr try : sys . stdout , sys . stderr = map ( DummyTqdmFile , orig_out_err ) yield orig_out_err [ 0 ] # Relay exceptions except Exception as exc : raise exc # Always restore sys.stdout/err if necessary finally : sys . stdout , sys . stderr = orig_out_err def some_fun ( i ): print ( \"Fee, fi, fo,\" . split ()[ 2 ]) # Redirect stdout to tqdm.write() (don't forget the `as save_stdout`) with std_out_err_redirect_tqdm () as orig_stdout : # tqdm needs the original stdout # and dynamic_ncols=True to autodetect console width for i in tqdm ( range ( 3 ), file = orig_stdout , dynamic_ncols = True ): sleep ( 0.5 ) some_fun ( i )","title":"tqdm"},{"location":"util/#_8","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def get_parameters_of_func ( offset = None ): \"\"\"Get a dictionary of paramteres of the function. Parameters ---------- offset : int default value is None Return ------ dictionary The dictionary includes pairs of paremeter's name and the corresponding values. References ---------- [1] https://tottoto.net/python3-get-args-of-current-function/ \"\"\" parent_frame = inspect . currentframe () . f_back info = inspect . getargvalues ( parent_frame ) return { key : info . locals [ key ] for key in info . args [ offset :]} 1 function_name = inspect . currentframe () . f_code . co_name","title":"\u540d\u524d\u53d6\u5f97"},{"location":"util_template/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 FROM python:3.9 ENV DEBIAN_FRONTEND = noninteractive ENV TZ = Asia/Tokyo RUN ln -snf /usr/share/zoneinfo/ $TZ /etc/localtime && echo $TZ > /etc/timezone RUN apt-get update && apt-get upgrade -y \\ && apt-get install -y --no-install-recommends \\ apt-utils \\ gcc \\ sudo \\ wget \\ make \\ cmake \\ libgl1-mesa-dev \\ && apt-get autoremove -y \\ && apt-get clean \\ && rm -rf \\ /var/lib/apt/lists/* \\ /var/cache/apt/* \\ /usr/local/src/* \\ /tmp/* RUN pip install --no-cache-dir \\ numpy \\ pandas \\ scipy \\ scikit-learn \\ matplotlib \\ seaborn \\ japanize-matplotlib \\ lightgbm \\ opencv-python \\ opencv-contrib-python \\ fastapi> = 0 .65.2 \\ future> = 0 .18.2 \\ gunicorn> = 20 .0.4 \\ importlib-metadata> = 1 .7.0 \\ joblib> = 0 .15.1 \\ Pillow> = 8 .3.2 \\ PyYAML> = 5 .3.1 \\ redis> = 3 .5.3 \\ typing> = 3 .7.4.1 \\ uvicorn> = 0 .11.7 \\ requests> = 2 .25.1 \\ tqdm \\ && rm -rf ~/.cache/pip CMD [ \"/bin/bash\" ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #!/bin/bash set -eu SCRIPT_DIR = $( cd $( dirname $0 ) ; pwd ) cd $SCRIPT_DIR cd .. DOCKER_WORK_DIR = \"/work\" WORK_DIR = ${ PWD } SCRIPT_NAME = \"script.py\" SCRIPT_DIR_DOCKER = \" ${ DOCKER_WORK_DIR } /src/ ${ SCRIPT_NAME } \" INPUT_DIR = \" ${ DOCKER_WORK_DIR } /data/figures\" OUTPUT_DIR = \" ${ DOCKER_WORK_DIR } /data/output\" LOG_DIR = \" ${ WORK_DIR } /log\" DOCKER_FILE_DIR = \" ${ WORK_DIR } /docker\" SHM_SIZE = \"4g\" DOCKER_IMAGE_NAME = \"pytorch\" DOCKER_CONTAINER_NAME = \"sample-container\" if [ -d ${ LOG_DIR } ] ; then echo \"Log directory exists.\" else mkdir ${ LOG_DIR } fi if [ ! \" $( docker images | grep ${ DOCKER_IMAGE_NAME } ) \" ] ; then docker build ${ DOCKER_FILE_DIR } -t ${ DOCKER_IMAGE_NAME } 2 > & 1 | tee \" ${ LOG_DIR } /docker_building.log\" | tail -n 20 fi echo \"docker image exists.\" if [ \" $( docker ps -a | grep ${ DOCKER_CONTAINER_NAME } ) \" ] ; then echo \" ${ DOCKER_CONTAINER_NAME } already exist.\" docker rm --force ${ DOCKER_CONTAINER_NAME } fi docker run --name ${ DOCKER_CONTAINER_NAME } \\ --init \\ --rm \\ -v ${ WORK_DIR } : ${ DOCKER_WORK_DIR } \\ -w ${ DOCKER_WORK_DIR } \\ --gpus all \\ --shm-size ${ SHM_SIZE } \\ ${ DOCKER_IMAGE_NAME } \\ python ${ SCRIPT_DIR_DOCKER } \\ --debug \\ --input_dir ${ INPUT_DIR } \\ --output_dir ${ OUTPUT_DIR } \\ 2 > & 1 | tee \" ${ LOG_DIR } / ${ SCRIPT_NAME } .log\" | less +F 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 import argparse import time from concurrent.futures import ProcessPoolExecutor from functools import partial , wraps from pathlib import Path from typing import Iterator , List import requests from fastapi import FastAPI from PIL import Image from tqdm import tqdm app = FastAPI () def get_script_dir (): return Path ( __file__ ) . parent def get_file_paths ( dir_path : str ) -> List [ Path ]: \"\"\" List file paths in a directory. Parameters ---------- dir_path : str Path for the directory Returns ------- List[str] List of the file paths in the directory \"\"\" return sorted ([ path for path in Path ( dir_path ) . rglob ( \"*\" ) if path . is_file ()]) def create_dir ( dir_path : str ): \"\"\" Create a directry if it does not exists. Parameters ---------- dir_path : str Path of the directory. \"\"\" if not Path ( dir_path ) . is_dir (): Path ( dir_path ) . mkdir ( parents = True ) def split_list ( list : List [ str ], num_per_batch : int ) -> Iterator [ List [ str ]]: \"\"\" Split the list Parameters ---------- list : List[str] input list num_per_batch : int the number per batch Yields ------ Iterator[List[str]] \"\"\" for idx in range ( 0 , len ( list ), num_per_batch ): yield list [ idx : idx + num_per_batch ] def stop_watch ( func ): @wraps ( func ) def wrapper ( * args , ** kargs ): start = time . time () result = func ( * args , ** kargs ) elapsed_time = time . time () - start print ( f \"total time of { func . __name__ } : { elapsed_time } \" ) print ( f \"expected { elapsed_time * 100 } / 100 loop, { elapsed_time * 1000 } / 1000 loop, { elapsed_time * 10000 } sec / 10000 loop\" ) return result return wrapper def task ( input_path , output_path ): img = Image . open ( str ( input_path )) . resize (( 100 , 100 )) img . save ( str ( output_path )) return None def task_only_input ( input_path , output_dir ): output_path = Path ( output_dir ) / input_path . name task ( input_path , output_path ) @stop_watch def task_only_input_stopwatch ( input_path , output_dir ): task_only_input ( input_path , output_dir ) def task_all ( file_list , output_dir ): for file_path in tqdm ( file_list ): task_only_input ( file_path , output_dir ) def task_all_multiprocess ( file_list , output_dir , batch_size ): file_batch_list = list ( split_list ( file_list , batch_size )) for file_batch in tqdm ( file_batch_list ): with ProcessPoolExecutor () as executor : task_partial = partial ( task_only_input , output_dir = output_dir ) list ( executor . map ( task_partial , file_batch )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import argparse from general_utils import ( create_dir , get_file_paths , task_all , task_all_multiprocess , task_only_input_stopwatch , ) if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--input_dir\" , type = str , required = True , help = \"path for input dir\" , ) parser . add_argument ( \"--output_dir\" , type = str , required = True , help = \"path for output dir\" , ) parser . add_argument ( \"--debug\" , action = \"store_true\" , help = \"just check one file\" , ) parser . add_argument ( \"--multiprocess\" , action = \"store_true\" , help = \"use multiprocess\" , ) parser . add_argument ( \"--batch_size\" , type = int , default = 32 , help = \"This value should depend on the available max memory in using multiprocess option\" , ) args = parser . parse_args () file_list = get_file_paths ( args . input_dir ) output_dir = args . output_dir batch_size = args . batch_size create_dir ( args . output_dir ) if args . debug : task_only_input_stopwatch ( file_list [ 0 ], output_dir ) else : if args . multiprocess : task_all_multiprocess ( file_list , output_dir , batch_size ) else : task_all ( file_list , output_dir ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import logging from logging import DEBUG , StreamHandler , getLogger from pathlib import Path from fastapi import FastAPI from general_utils import ( create_dir , get_file_paths , task_all , task_all_multiprocess , task_only_input , task_only_input_stopwatch , ) logger = getLogger ( __name__ ) handler = StreamHandler () handler . setLevel ( DEBUG ) logger . setLevel ( DEBUG ) logger . addHandler ( handler ) formatter = logging . Formatter ( \"[ %(asctime)s ][ %(name)s ][ %(funcName)s , %(lineno)d ] %(message)s \" ) handler . setFormatter ( formatter ) logger . propagate = False app = FastAPI () @app . post ( \"/do_task\" ) def do_task ( input_path : str , output_dir : str ): _input_path = Path ( input_path ) task_only_input ( _input_path , output_dir ) logger . info ( \"task is done\" ) return { \"do_task\" : \"task_done\" } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 import argparse from pathlib import Path import requests from general_utils import ( create_dir , get_file_paths , task_all , task_all_multiprocess , task_only_input , task_only_input_stopwatch , ) if __name__ == \"__main__\" : parser = argparse . ArgumentParser () parser . add_argument ( \"--input_dir\" , type = str , required = True , help = \"path for input dir\" , ) parser . add_argument ( \"--output_dir\" , type = str , required = True , help = \"path for output dir\" , ) parser . add_argument ( \"--debug\" , action = \"store_true\" , help = \"just check one file\" , ) parser . add_argument ( \"--multiprocess\" , action = \"store_true\" , help = \"use multiprocess\" , ) parser . add_argument ( \"--batch_size\" , type = int , default = 32 , help = \"This value should depend on the available max memory in using multiprocess option\" , ) parser . add_argument ( \"--api_server_url\" , type = str , required = True , help = \"api_server_url\" , ) args = parser . parse_args () file_list = get_file_paths ( args . input_dir ) output_dir = args . output_dir batch_size = args . batch_size create_dir ( args . output_dir ) api_server_url = args . api_server_url params = { \"input_path\" : str ( file_list [ 0 ]), \"output_dir\" : str ( output_dir ), } try : api_response = requests . post ( api_server_url , params = params ) api_response = api_response . json () except Exception as e : print ( e ) pass","title":"Util template"},{"location":"webserver_request/","text":"curl \u30b3\u30de\u30f3\u30c9 \u00b6 https://weblabo.oscasierra.net/curl-post/ https://qiita.com/att55/items/04e8080d1c441837ad42 https://www.setouchino.cloud/blogs/99 https://hydrocul.github.io/wiki/commands/curl.html https://qiita.com/shtnkgm/items/45b4cd274fa813d29539 https://qiita.com/ryuichi1208/items/e4e1b27ff7d54a66dcd9 https://www.y-hakopro.com/entry/curl-post-options https://blog.kyanny.me/entry/20110427/1303838381 https://qiita.com/aosho235/items/d89bb027db0c5662d8c5 \u57fa\u672c \u00b6 1 curl -X POST https://xxxxx.net/xxxxxx \u30c7\u30fc\u30bf\u9001\u4fe1 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # -X POST\u306f\u7701\u7565\u53ef\u80fd (Note: Unnecessary use of -X or \u2013request, POST is already inferred.\u3068\u3044\u3046\u8b66\u544a\u6587\u304c\u51fa\u308b\u3053\u3068\u3082\u3042\u308b) # curl -d \u5358\u4e00\u306e\u30c7\u30fc\u30bf\uff1aMIME type \u306f application/x-www-form-urlencoded #\u3000-F (--Form) \u306f\u8907\u6570\u30bf\u30a4\u30d7\u306e\u30c7\u30fc\u30bf\uff1aMIME type \u306f multipart/form-data # https://qiita.com/aosho235/items/d89bb027db0c5662d8c5 curl --data-urlencode \"name=John Doe (Junior)\" -d 'age=10' https://xxxxx.com/xxxxxx curl -X POST -F 'age=30' -F file1 = @/var/tmp/sample.jpg https://xxxxx.net/xxxxxx # \u30ea\u30af\u30a8\u30b9\u30c8\u30d8\u30c3\u30c0\u3092\u8ffd\u52a0\u3059\u308b curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"\u3059\u3044\u304b\", \"age\":\"30\"}' https://xxxxx.net/xxxxxx curl -X POST -H \"Content-Type: application/json\" -d '@request.json' https://xxxxx.net/xxxxxx cat foo.txt | curl http://www.example.com/ -X POST -d @- ## \u30b5\u30fc\u30d0\u306b\u30af\u30c3\u30ad\u30fc\u3092\u9001\u4fe1 $ curl -b cookie.txt http://www.example.com/ ## \u30b5\u30fc\u30d0\u304b\u3089\u53d7\u4fe1\u3057\u305f\u30af\u30c3\u30ad\u30fc\u3092\u4fdd\u5b58 $ curl -c cookie.txt http://www.example.com/ ## \u30b5\u30fc\u30d0\u306b\u30af\u30c3\u30ad\u30fc\u3092\u9001\u4fe1\u3057\u3001\u53d7\u4fe1\u3057\u305f\u30af\u30c3\u30ad\u30fc\u3092\u518d\u3073\u4fdd\u5b58 $ curl -b cookie.txt -c cookie.txt http://www.example.com/ # User-Agent\u3092\u4ed8\u4e0e\uff08-A\uff09 $ curl -A \"USER_AGENT\" http://www.example.com \u52d5\u4f5c\u306e\u6307\u5b9a \u00b6 1 2 3 4 5 6 7 8 9 # HTTP\u30e1\u30bd\u30c3\u30c9\u306e\u6307\u5b9a\uff08-X\uff09 $ curl -X PUT http://www.example.com/ # SSL\u306e\u30a8\u30e9\u30fc\u3092\u7121\u8996\u3057\u3066\u51e6\u7406\u3092\u7d99\u7d9a\uff08-k\uff09 # \uff08\u30b5\u30fc\u30d0\u30fc\u5074\u8a3c\u660e\u66f8\u304c\u4e0d\u6b63\u3001\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5074\u306e\u30eb\u30fc\u30c8\u8a3c\u660e\u66f8\u304c\u4e0d\u6b63\u306a\u3069\uff09 $ curl -k http://www.example.com/ # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8\u3055\u305b\u308b\uff08-L\uff09 $ curl -L http://www.example.com/ \u30c7\u30d0\u30c3\u30b0 \u00b6 1 2 3 4 5 6 7 8 9 # HTTP\u30ec\u30b9\u30dd\u30f3\u30b9\u30d8\u30c3\u30c0\u30fc\u306e\u53d6\u5f97\uff08-I\uff09 $ curl -I http://www.example.com/ # \u8a73\u7d30\u3092\u30ed\u30b0\u51fa\u529b\uff08-v\u3082\u3057\u304f\u306f--verbose\uff09 $ curl -v http://www.example.com/ # \u9032\u6357\u3084\u30a8\u30e9\u30fc\u3092\u8868\u793a\u3057\u306a\u3044\uff08-s\u3082\u3057\u304f\u306f--silent\uff09 # \uff08curl\u306e\u7d50\u679c\u3092\u30d1\u30a4\u30d7\u3067\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u306b\u6e21\u3059\u969b\u306b\u3001\u901a\u4fe1\u6642\u9593\u306e\u30c7\u30fc\u30bf\u306a\u3069\u304c\u90aa\u9b54\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3059\u308b\uff09 $ curl -s http://www.example.com/ \u30b3\u30de\u30f3\u30c9\u3092 chrome \u306e developper \u30c4\u30fc\u30eb\u304b\u3089\u3068\u3063\u3066\u304f\u308b \u00b6 Developper \u30c4\u30fc\u30eb\u3000=> Network => \u9078\u629e\u3057\u3066\uff08\u53f3\u30af\u30ea\u30c3\u30af\uff09>= copy cURL https://qiita.com/ueokande/items/a580e9d9f17dbf82f382 requests \u00b6 https://qiita.com/moonwalkerpoday/items/0c31c35588df49a5ca57 https://qiita.com/hththt/items/14bfc2bf23192b020371 https://qiita.com/danishi/items/07dd1b2f2a28255f7a85 https://note.nkmk.me/python-requests-usage/ https://curlconverter.com/ https://intellectual-curiosity.tokyo/2019/08/31/python%E3%81%A7curl%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%81%A8%E5%90%8C%E7%AD%89%E3%81%AE%E5%87%A6%E7%90%86%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/ 1 2 3 4 5 6 7 8 9 import requests headers = { 'Content-Type' : 'application/json' , } data = '{\"key1\": \"value1\", \"key2\": \"value2\"}' response = requests . post ( 'https://example.com' , headers = headers , data = data ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 requests . get ( url , params = None , ** kwargs ) requests . post ( url , data = None , json = None , ** kwargs ) requests . put ( url , data = None , ** kwargs ) requests . patch ( url , data = None , ** kwargs ) requests . delete ( url , ** kwargs ) requests . options ( url , ** kwargs ) requests . head ( url , ** kwargs ) def request ( self , method , url , params = None , data = None , headers = None , cookies = None , files = None , auth = None , timeout = None , allow_redirects = True , proxies = None , hooks = None , stream = None , verify = None , cert = None , json = None ) response . url response . status_code response . headers response . text response . content \u30c7\u30fc\u30bf\u306e\u53d6\u6271 \u00b6 1 2 3 r = requests.get(url, params=params) data = r.json() print(json.dumps(data, indent=2)) 1 2 3 4 5 6 r = requests.get(url) print(r.headers['Content-Type']) filename= os.path.basename(url) with open('data/temp/' + filename, 'wb') as f: f.write(r.content) fastAPI \u00b6 https://qiita.com/uezo/items/847e1911ac486f5a89c4 https://qiita.com/bee2/items/75d9c0d7ba20e7a4a0e9 https://blog.jicoman.info/2021/01/how-to-logging-request-and-response-body-by-fastapi/ https://zenn.dev/panyoriokome/scraps/819941ebf161ee 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import uvicorn from fastapi import FastAPI app = FastAPI () class Item ( BaseModel ): name : str price : float is_offer : bool = None @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {color} / {item_id} \" ) async def read_item ( color : Color , item_id : int ): return { \"color\" : color , \"item_id\" : item_id } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : str = None ): return { \"item_id\" : item_id , \"q\" : q } @app . post ( \"/items/\" ) def post_item ( item : Item ): return { \"item_name\" : item . name , \"item_price\" : item . price } @app . put ( \"/items/ {item_id} \" ) def update_item ( item_id : int , item : Item ): return { \"item_name\" : item . name , \"item_id\" : item_id if __name__ == \"__main__\" : uvicorn . run ( \"run:app\" , host = \"127.0.0.1\" , port = 1234 , log_level = \"info\" ) 1 gunicorn -w 4 -k uvicorn.workers.UvicornWorker run:app --bind localhost:1234 japonto \u00b6 https://qiita.com/t-iguchi/items/4bf3bf24a4f9d6f734b3 https://qiita.com/bee2/items/0ad260ab9835a2087dae#japronto","title":"Webserver request"},{"location":"webserver_request/#curl","text":"https://weblabo.oscasierra.net/curl-post/ https://qiita.com/att55/items/04e8080d1c441837ad42 https://www.setouchino.cloud/blogs/99 https://hydrocul.github.io/wiki/commands/curl.html https://qiita.com/shtnkgm/items/45b4cd274fa813d29539 https://qiita.com/ryuichi1208/items/e4e1b27ff7d54a66dcd9 https://www.y-hakopro.com/entry/curl-post-options https://blog.kyanny.me/entry/20110427/1303838381 https://qiita.com/aosho235/items/d89bb027db0c5662d8c5","title":"curl \u30b3\u30de\u30f3\u30c9"},{"location":"webserver_request/#_1","text":"1 curl -X POST https://xxxxx.net/xxxxxx","title":"\u57fa\u672c"},{"location":"webserver_request/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # -X POST\u306f\u7701\u7565\u53ef\u80fd (Note: Unnecessary use of -X or \u2013request, POST is already inferred.\u3068\u3044\u3046\u8b66\u544a\u6587\u304c\u51fa\u308b\u3053\u3068\u3082\u3042\u308b) # curl -d \u5358\u4e00\u306e\u30c7\u30fc\u30bf\uff1aMIME type \u306f application/x-www-form-urlencoded #\u3000-F (--Form) \u306f\u8907\u6570\u30bf\u30a4\u30d7\u306e\u30c7\u30fc\u30bf\uff1aMIME type \u306f multipart/form-data # https://qiita.com/aosho235/items/d89bb027db0c5662d8c5 curl --data-urlencode \"name=John Doe (Junior)\" -d 'age=10' https://xxxxx.com/xxxxxx curl -X POST -F 'age=30' -F file1 = @/var/tmp/sample.jpg https://xxxxx.net/xxxxxx # \u30ea\u30af\u30a8\u30b9\u30c8\u30d8\u30c3\u30c0\u3092\u8ffd\u52a0\u3059\u308b curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"\u3059\u3044\u304b\", \"age\":\"30\"}' https://xxxxx.net/xxxxxx curl -X POST -H \"Content-Type: application/json\" -d '@request.json' https://xxxxx.net/xxxxxx cat foo.txt | curl http://www.example.com/ -X POST -d @- ## \u30b5\u30fc\u30d0\u306b\u30af\u30c3\u30ad\u30fc\u3092\u9001\u4fe1 $ curl -b cookie.txt http://www.example.com/ ## \u30b5\u30fc\u30d0\u304b\u3089\u53d7\u4fe1\u3057\u305f\u30af\u30c3\u30ad\u30fc\u3092\u4fdd\u5b58 $ curl -c cookie.txt http://www.example.com/ ## \u30b5\u30fc\u30d0\u306b\u30af\u30c3\u30ad\u30fc\u3092\u9001\u4fe1\u3057\u3001\u53d7\u4fe1\u3057\u305f\u30af\u30c3\u30ad\u30fc\u3092\u518d\u3073\u4fdd\u5b58 $ curl -b cookie.txt -c cookie.txt http://www.example.com/ # User-Agent\u3092\u4ed8\u4e0e\uff08-A\uff09 $ curl -A \"USER_AGENT\" http://www.example.com","title":"\u30c7\u30fc\u30bf\u9001\u4fe1"},{"location":"webserver_request/#_3","text":"1 2 3 4 5 6 7 8 9 # HTTP\u30e1\u30bd\u30c3\u30c9\u306e\u6307\u5b9a\uff08-X\uff09 $ curl -X PUT http://www.example.com/ # SSL\u306e\u30a8\u30e9\u30fc\u3092\u7121\u8996\u3057\u3066\u51e6\u7406\u3092\u7d99\u7d9a\uff08-k\uff09 # \uff08\u30b5\u30fc\u30d0\u30fc\u5074\u8a3c\u660e\u66f8\u304c\u4e0d\u6b63\u3001\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5074\u306e\u30eb\u30fc\u30c8\u8a3c\u660e\u66f8\u304c\u4e0d\u6b63\u306a\u3069\uff09 $ curl -k http://www.example.com/ # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8\u3055\u305b\u308b\uff08-L\uff09 $ curl -L http://www.example.com/","title":"\u52d5\u4f5c\u306e\u6307\u5b9a"},{"location":"webserver_request/#_4","text":"1 2 3 4 5 6 7 8 9 # HTTP\u30ec\u30b9\u30dd\u30f3\u30b9\u30d8\u30c3\u30c0\u30fc\u306e\u53d6\u5f97\uff08-I\uff09 $ curl -I http://www.example.com/ # \u8a73\u7d30\u3092\u30ed\u30b0\u51fa\u529b\uff08-v\u3082\u3057\u304f\u306f--verbose\uff09 $ curl -v http://www.example.com/ # \u9032\u6357\u3084\u30a8\u30e9\u30fc\u3092\u8868\u793a\u3057\u306a\u3044\uff08-s\u3082\u3057\u304f\u306f--silent\uff09 # \uff08curl\u306e\u7d50\u679c\u3092\u30d1\u30a4\u30d7\u3067\u6b21\u306e\u30b3\u30de\u30f3\u30c9\u306b\u6e21\u3059\u969b\u306b\u3001\u901a\u4fe1\u6642\u9593\u306e\u30c7\u30fc\u30bf\u306a\u3069\u304c\u90aa\u9b54\u306b\u306a\u3089\u306a\u3044\u3088\u3046\u306b\u3059\u308b\uff09 $ curl -s http://www.example.com/","title":"\u30c7\u30d0\u30c3\u30b0"},{"location":"webserver_request/#chrome-developper","text":"Developper \u30c4\u30fc\u30eb\u3000=> Network => \u9078\u629e\u3057\u3066\uff08\u53f3\u30af\u30ea\u30c3\u30af\uff09>= copy cURL https://qiita.com/ueokande/items/a580e9d9f17dbf82f382","title":"\u30b3\u30de\u30f3\u30c9\u3092 chrome \u306e developper \u30c4\u30fc\u30eb\u304b\u3089\u3068\u3063\u3066\u304f\u308b"},{"location":"webserver_request/#requests","text":"https://qiita.com/moonwalkerpoday/items/0c31c35588df49a5ca57 https://qiita.com/hththt/items/14bfc2bf23192b020371 https://qiita.com/danishi/items/07dd1b2f2a28255f7a85 https://note.nkmk.me/python-requests-usage/ https://curlconverter.com/ https://intellectual-curiosity.tokyo/2019/08/31/python%E3%81%A7curl%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%81%A8%E5%90%8C%E7%AD%89%E3%81%AE%E5%87%A6%E7%90%86%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/ 1 2 3 4 5 6 7 8 9 import requests headers = { 'Content-Type' : 'application/json' , } data = '{\"key1\": \"value1\", \"key2\": \"value2\"}' response = requests . post ( 'https://example.com' , headers = headers , data = data ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 requests . get ( url , params = None , ** kwargs ) requests . post ( url , data = None , json = None , ** kwargs ) requests . put ( url , data = None , ** kwargs ) requests . patch ( url , data = None , ** kwargs ) requests . delete ( url , ** kwargs ) requests . options ( url , ** kwargs ) requests . head ( url , ** kwargs ) def request ( self , method , url , params = None , data = None , headers = None , cookies = None , files = None , auth = None , timeout = None , allow_redirects = True , proxies = None , hooks = None , stream = None , verify = None , cert = None , json = None ) response . url response . status_code response . headers response . text response . content","title":"requests"},{"location":"webserver_request/#_5","text":"1 2 3 r = requests.get(url, params=params) data = r.json() print(json.dumps(data, indent=2)) 1 2 3 4 5 6 r = requests.get(url) print(r.headers['Content-Type']) filename= os.path.basename(url) with open('data/temp/' + filename, 'wb') as f: f.write(r.content)","title":"\u30c7\u30fc\u30bf\u306e\u53d6\u6271"},{"location":"webserver_request/#fastapi","text":"https://qiita.com/uezo/items/847e1911ac486f5a89c4 https://qiita.com/bee2/items/75d9c0d7ba20e7a4a0e9 https://blog.jicoman.info/2021/01/how-to-logging-request-and-response-body-by-fastapi/ https://zenn.dev/panyoriokome/scraps/819941ebf161ee 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import uvicorn from fastapi import FastAPI app = FastAPI () class Item ( BaseModel ): name : str price : float is_offer : bool = None @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {color} / {item_id} \" ) async def read_item ( color : Color , item_id : int ): return { \"color\" : color , \"item_id\" : item_id } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : str = None ): return { \"item_id\" : item_id , \"q\" : q } @app . post ( \"/items/\" ) def post_item ( item : Item ): return { \"item_name\" : item . name , \"item_price\" : item . price } @app . put ( \"/items/ {item_id} \" ) def update_item ( item_id : int , item : Item ): return { \"item_name\" : item . name , \"item_id\" : item_id if __name__ == \"__main__\" : uvicorn . run ( \"run:app\" , host = \"127.0.0.1\" , port = 1234 , log_level = \"info\" ) 1 gunicorn -w 4 -k uvicorn.workers.UvicornWorker run:app --bind localhost:1234","title":"fastAPI"},{"location":"webserver_request/#japonto","text":"https://qiita.com/t-iguchi/items/4bf3bf24a4f9d6f734b3 https://qiita.com/bee2/items/0ad260ab9835a2087dae#japronto","title":"japonto"},{"location":"algo/al/","text":"alala","title":"Al"}]}