{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Test \u00b6 test1 \u00b6 test2 \u00b6","title":"DFS"},{"location":"#test","text":"","title":"Test"},{"location":"#test1","text":"","title":"test1"},{"location":"#test2","text":"","title":"test2"},{"location":"albumentations/","text":"Image Augumentations \u00b6 \u30b5\u30f3\u30d7\u30eb\u5199\u771f\u306e\u8868\u793a \u00b6 \u30e9\u30a4\u30d6\u30e9\u30ea\u306eimport \u00b6 1 2 3 4 5 import matplotlib.pyplot as plt import albumentations as A import numpy as np import cv2 from PIL import Image Pillow \u3068OpenCV\u305d\u308c\u305e\u308c\u3067\u753b\u50cf\u3092\u8868\u793a \u00b6 \u753b\u50cf\u30c7\u30fc\u30bf\u306fKaggle\u306e Flowers Recognition \u304b\u3089\u53d6\u5f97\u3002Pillow\u3092\u4f7f\u3046\u5834\u5408\u306f\u3001\u8aad\u307f\u8fbc\u3093\u3060\u3068\u304d\u306bJpegImageFile\u306a\u306e\u3067openCV\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 image_path = '../input/flowers-recognition/flowers/daisy/10140303196_b88d3d6cec.jpg' Pillow\u306f\u5358\u306a\u308b\u753b\u50cf\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308a\u3001OpenCV\u306f\u300c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u300d\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u78ba\u304b\u306b\u6a5f\u80fd\u304c\u91cd\u8907\u3059\u308b\u90e8\u5206\u306f\u591a\u3044\uff08\u3064\u307e\u308aOpenCV\u306b\u306f\u304b\u306a\u308a\u306e\u753b\u50cf\u51e6\u7406\u6a5f\u80fd\u304c\u542b\u307e\u308c\u3066\u3044\u308b\uff09\u304c\u3001 \u305d\u306e\u6271\u3046\u5185\u5bb9\u306f\u5927\u304d\u304f\u7570\u306a\u308a\u307e\u3059\u3002\u6975\u7aef\u306a\u8a71\u3001\u753b\u50cf\u3092\u30ab\u30c3\u30c8\u3084\u30ea\u30b5\u30a4\u30ba\u3057\u305f\u3044\u6642\u3084\u3001\u5c11\u3057\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u3044\u5834\u5408\u306f Pillow \u3092\u4f7f\u3044\u3001\u7269\u4e8b\u3092\u300c\u898b\u3088\u3046\u300d\u3068\u601d\u3063\u3066\u3044\u308b\u30ed\u30dc\u30c3\u30c8\u3092\u7d44\u307f\u305f\u3044\u6642\u306b\u306f OpenCV \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u5f15\u7528\u5143\uff1a https://teratail.com/questions/71851 Pillow 1 2 3 4 5 6 img = Image . open ( image_path ) # img: JpegImageFile img = np . asarray ( img ) # \u3082\u3068\u306e\u753b\u50cf\u306b\u623b\u3059\u5834\u5408 # im = Image.fromarray(np.uint8(myarray*255)) plt . imshow ( img ) OpenCV 1 2 3 4 5 6 img = cv2 . imread ( image_path ) # img : ndarray (N-dimensional array, np.array\u306b\u3088\u3063\u3066\u751f\u6210) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # \u4e0b\u8a18\u3082RGB\u753b\u50cf\u2192BGR\u753b\u50cf\u3078\u306e\u5909\u63db #img = img[:,:,::-1] plt . imshow ( img ) \u53c2\u8003 - https://note.nkmk.me/python-image-processing-pillow-numpy-opencv/ - https://nixeneko.hatenablog.com/entry/2017/09/01/000000 - https://tomomai.com/python-opencv-pillow/ - https://www.codexa.net/opencv_python_introduction/ (open CV\u306b\u95a2\u3057\u3066) Note \u4e0b\u8a18\u306e\u753b\u50cf\u8868\u793a\u30b3\u30fc\u30c9\u306f\u3001 https://github.com/tkuri/albumentations_test/blob/master/albumentations_test.ipynb \u3000\u3092\u53c2\u8003\u306b\u3057\u305f\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 aug = [] n = 3 param1 = ( 1 , 20 ) param2 = ( 16 , 16 ) aug . append ( A . Compose ([ A . Blur ( p = 1 )])) aug . append ( A . Compose ([ A . MedianBlur ( p = 1 )]) aug . append ( A . Compose ([ A . GaussianBlur ( p = 1 )]) aug_img = [ aug [ i ]( image = img ) for i in range ( n )] fig , ax = plt . subplots ( 1 , 1 + n , figsize = ( 5 + 5 * n , 5 )) plt . subplots_adjust ( wspace = 0 ) plt . rcParams [ \"font.size\" ] = 18 [ ax [ i ] . tick_params ( bottom = False , left = False , right = False , top = False , labelbottom = False , labelleft = False , labelright = False , labeltop = False ) for i in range ( 1 + n )] ax [ 0 ] . set_xlabel ( \"Original\" ) ax [ 1 ] . set_xlabel ( \"Default Augmentation\" ) ax [ 2 ] . set_xlabel ( \"blur_limit= {} \" . format ( param1 )) ax [ 3 ] . set_xlabel ( \"blur_limit= {} \" . format ( param2 )) ax [ 0 ] . imshow ( img ) [ ax [ i + 1 ] . imshow ( aug_img [ i ][ 'image' ]) for i in range ( n )] Albumentations \u00b6 \u53c2\u8003\uff1a https://qiita.com/kurilab/items/b69e1be8d0224ae139ad Flip, Crop, Rotate etc.\uff08\u30d5\u30ea\u30c3\u30d7\u3001\u5207\u308a\u53d6\u308a\u3001\u56de\u8ee2\u306a\u3069\uff09 \u00b6 \u30d5\u30ea\u30c3\u30d7 \u00b6 \u5207\u308a\u53d6\u308a \u00b6 Blur, Noise\uff08\u307c\u304b\u3057\uff09 \u00b6 Blur \u00b6 \u9ad8\u5ea6\u5e7e\u4f55\u5909\u63db\u7cfb (Affine, Distortion) \u00b6","title":"albumentations"},{"location":"albumentations/#image-augumentations","text":"","title":"Image Augumentations"},{"location":"albumentations/#_1","text":"","title":"\u30b5\u30f3\u30d7\u30eb\u5199\u771f\u306e\u8868\u793a"},{"location":"albumentations/#import","text":"1 2 3 4 5 import matplotlib.pyplot as plt import albumentations as A import numpy as np import cv2 from PIL import Image","title":"\u30e9\u30a4\u30d6\u30e9\u30ea\u306eimport"},{"location":"albumentations/#pillow-opencv","text":"\u753b\u50cf\u30c7\u30fc\u30bf\u306fKaggle\u306e Flowers Recognition \u304b\u3089\u53d6\u5f97\u3002Pillow\u3092\u4f7f\u3046\u5834\u5408\u306f\u3001\u8aad\u307f\u8fbc\u3093\u3060\u3068\u304d\u306bJpegImageFile\u306a\u306e\u3067openCV\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 image_path = '../input/flowers-recognition/flowers/daisy/10140303196_b88d3d6cec.jpg' Pillow\u306f\u5358\u306a\u308b\u753b\u50cf\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308a\u3001OpenCV\u306f\u300c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u300d\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u78ba\u304b\u306b\u6a5f\u80fd\u304c\u91cd\u8907\u3059\u308b\u90e8\u5206\u306f\u591a\u3044\uff08\u3064\u307e\u308aOpenCV\u306b\u306f\u304b\u306a\u308a\u306e\u753b\u50cf\u51e6\u7406\u6a5f\u80fd\u304c\u542b\u307e\u308c\u3066\u3044\u308b\uff09\u304c\u3001 \u305d\u306e\u6271\u3046\u5185\u5bb9\u306f\u5927\u304d\u304f\u7570\u306a\u308a\u307e\u3059\u3002\u6975\u7aef\u306a\u8a71\u3001\u753b\u50cf\u3092\u30ab\u30c3\u30c8\u3084\u30ea\u30b5\u30a4\u30ba\u3057\u305f\u3044\u6642\u3084\u3001\u5c11\u3057\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u3044\u5834\u5408\u306f Pillow \u3092\u4f7f\u3044\u3001\u7269\u4e8b\u3092\u300c\u898b\u3088\u3046\u300d\u3068\u601d\u3063\u3066\u3044\u308b\u30ed\u30dc\u30c3\u30c8\u3092\u7d44\u307f\u305f\u3044\u6642\u306b\u306f OpenCV \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u5f15\u7528\u5143\uff1a https://teratail.com/questions/71851 Pillow 1 2 3 4 5 6 img = Image . open ( image_path ) # img: JpegImageFile img = np . asarray ( img ) # \u3082\u3068\u306e\u753b\u50cf\u306b\u623b\u3059\u5834\u5408 # im = Image.fromarray(np.uint8(myarray*255)) plt . imshow ( img ) OpenCV 1 2 3 4 5 6 img = cv2 . imread ( image_path ) # img : ndarray (N-dimensional array, np.array\u306b\u3088\u3063\u3066\u751f\u6210) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # \u4e0b\u8a18\u3082RGB\u753b\u50cf\u2192BGR\u753b\u50cf\u3078\u306e\u5909\u63db #img = img[:,:,::-1] plt . imshow ( img ) \u53c2\u8003 - https://note.nkmk.me/python-image-processing-pillow-numpy-opencv/ - https://nixeneko.hatenablog.com/entry/2017/09/01/000000 - https://tomomai.com/python-opencv-pillow/ - https://www.codexa.net/opencv_python_introduction/ (open CV\u306b\u95a2\u3057\u3066) Note \u4e0b\u8a18\u306e\u753b\u50cf\u8868\u793a\u30b3\u30fc\u30c9\u306f\u3001 https://github.com/tkuri/albumentations_test/blob/master/albumentations_test.ipynb \u3000\u3092\u53c2\u8003\u306b\u3057\u305f\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 aug = [] n = 3 param1 = ( 1 , 20 ) param2 = ( 16 , 16 ) aug . append ( A . Compose ([ A . Blur ( p = 1 )])) aug . append ( A . Compose ([ A . MedianBlur ( p = 1 )]) aug . append ( A . Compose ([ A . GaussianBlur ( p = 1 )]) aug_img = [ aug [ i ]( image = img ) for i in range ( n )] fig , ax = plt . subplots ( 1 , 1 + n , figsize = ( 5 + 5 * n , 5 )) plt . subplots_adjust ( wspace = 0 ) plt . rcParams [ \"font.size\" ] = 18 [ ax [ i ] . tick_params ( bottom = False , left = False , right = False , top = False , labelbottom = False , labelleft = False , labelright = False , labeltop = False ) for i in range ( 1 + n )] ax [ 0 ] . set_xlabel ( \"Original\" ) ax [ 1 ] . set_xlabel ( \"Default Augmentation\" ) ax [ 2 ] . set_xlabel ( \"blur_limit= {} \" . format ( param1 )) ax [ 3 ] . set_xlabel ( \"blur_limit= {} \" . format ( param2 )) ax [ 0 ] . imshow ( img ) [ ax [ i + 1 ] . imshow ( aug_img [ i ][ 'image' ]) for i in range ( n )]","title":"Pillow \u3068OpenCV\u305d\u308c\u305e\u308c\u3067\u753b\u50cf\u3092\u8868\u793a"},{"location":"albumentations/#albumentations","text":"\u53c2\u8003\uff1a https://qiita.com/kurilab/items/b69e1be8d0224ae139ad","title":"Albumentations"},{"location":"albumentations/#flip-crop-rotate-etc","text":"","title":"Flip, Crop, Rotate etc.\uff08\u30d5\u30ea\u30c3\u30d7\u3001\u5207\u308a\u53d6\u308a\u3001\u56de\u8ee2\u306a\u3069\uff09"},{"location":"albumentations/#_2","text":"","title":"\u30d5\u30ea\u30c3\u30d7"},{"location":"albumentations/#_3","text":"","title":"\u5207\u308a\u53d6\u308a"},{"location":"albumentations/#blur-noise","text":"","title":"Blur, Noise\uff08\u307c\u304b\u3057\uff09"},{"location":"albumentations/#blur","text":"","title":"Blur"},{"location":"albumentations/#affine-distortion","text":"","title":"\u9ad8\u5ea6\u5e7e\u4f55\u5909\u63db\u7cfb (Affine, Distortion)"},{"location":"dcgan/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 28 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval betwen image samples\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , z ): img = self . model ( z ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . model = nn . Sequential ( nn . Linear ( int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 256 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 256 , 1 ), nn . Sigmoid (), ) def forward ( self , img ): img_flat = img . view ( img . size ( 0 ), - 1 ) validity = self . model ( img_flat ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . size ( 0 ), 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . size ( 0 ), 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) DCGAN \u00b6 G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084\u8ee2\u79fb\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092tanh\u95a2\u6570\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092leaky relu\u306b\u4ee3\u7528 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) cuda = True if torch . cuda . is_available () else False def weights_init_normal ( m ): classname = m . __class__ . __name__ if classname . find ( \"Conv\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 0.0 , 0.02 ) elif classname . find ( \"BatchNorm2d\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 1.0 , 0.02 ) torch . nn . init . constant_ ( m . bias . data , 0.0 ) class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . init_size = opt . img_size // 4 self . l1 = nn . Sequential ( nn . Linear ( opt . latent_dim , 128 * self . init_size ** 2 )) self . conv_blocks = nn . Sequential ( nn . BatchNorm2d ( 128 ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 64 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 64 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Conv2d ( 64 , opt . channels , 3 , stride = 1 , padding = 1 ), nn . Tanh (), ) def forward ( self , z ): out = self . l1 ( z ) out = out . view ( out . shape [ 0 ], 128 , self . init_size , self . init_size ) img = self . conv_blocks ( out ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () def discriminator_block ( in_filters , out_filters , bn = True ): block = [ nn . Conv2d ( in_filters , out_filters , 3 , 2 , 1 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Dropout2d ( 0.25 )] if bn : block . append ( nn . BatchNorm2d ( out_filters , 0.8 )) return block self . model = nn . Sequential ( * discriminator_block ( opt . channels , 16 , bn = False ), * discriminator_block ( 16 , 32 ), * discriminator_block ( 32 , 64 ), * discriminator_block ( 64 , 128 ), ) # The height and width of downsampled image ds_size = opt . img_size // 2 ** 4 self . adv_layer = nn . Sequential ( nn . Linear ( 128 * ds_size ** 2 , 1 ), nn . Sigmoid ()) def forward ( self , img ): out = self . model ( img ) out = out . view ( out . shape [ 0 ], - 1 ) validity = self . adv_layer ( out ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Initialize weights generator . apply ( weights_init_normal ) discriminator . apply ( weights_init_normal ) # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) Conditional GAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) WGAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) Cycle GAN Star GAN Progressive GAN Pix2Pix","title":"other"},{"location":"dcgan/#dcgan","text":"G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084\u8ee2\u79fb\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092tanh\u95a2\u6570\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092leaky relu\u306b\u4ee3\u7528 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) cuda = True if torch . cuda . is_available () else False def weights_init_normal ( m ): classname = m . __class__ . __name__ if classname . find ( \"Conv\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 0.0 , 0.02 ) elif classname . find ( \"BatchNorm2d\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 1.0 , 0.02 ) torch . nn . init . constant_ ( m . bias . data , 0.0 ) class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . init_size = opt . img_size // 4 self . l1 = nn . Sequential ( nn . Linear ( opt . latent_dim , 128 * self . init_size ** 2 )) self . conv_blocks = nn . Sequential ( nn . BatchNorm2d ( 128 ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 64 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 64 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Conv2d ( 64 , opt . channels , 3 , stride = 1 , padding = 1 ), nn . Tanh (), ) def forward ( self , z ): out = self . l1 ( z ) out = out . view ( out . shape [ 0 ], 128 , self . init_size , self . init_size ) img = self . conv_blocks ( out ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () def discriminator_block ( in_filters , out_filters , bn = True ): block = [ nn . Conv2d ( in_filters , out_filters , 3 , 2 , 1 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Dropout2d ( 0.25 )] if bn : block . append ( nn . BatchNorm2d ( out_filters , 0.8 )) return block self . model = nn . Sequential ( * discriminator_block ( opt . channels , 16 , bn = False ), * discriminator_block ( 16 , 32 ), * discriminator_block ( 32 , 64 ), * discriminator_block ( 64 , 128 ), ) # The height and width of downsampled image ds_size = opt . img_size // 2 ** 4 self . adv_layer = nn . Sequential ( nn . Linear ( 128 * ds_size ** 2 , 1 ), nn . Sigmoid ()) def forward ( self , img ): out = self . model ( img ) out = out . view ( out . shape [ 0 ], - 1 ) validity = self . adv_layer ( out ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Initialize weights generator . apply ( weights_init_normal ) discriminator . apply ( weights_init_normal ) # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) Conditional GAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) WGAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) Cycle GAN Star GAN Progressive GAN Pix2Pix","title":"DCGAN"},{"location":"docker/","text":"Dockerhub\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Dockerfile\u304b\u3089DockerImage, DockerImage\u304b\u3089\u30b3\u30f3\u30c6\u30ca docker login : dockerhub\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b docker pull <image> dockerhub \u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u3068\u3063\u3066\u304f\u308b docker images dockerimage\u306e\u4e00\u89a7\u3092\u8868\u793a docker run <image> create + start, docker\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u30b3\u30f3\u30c6\u30ca\u3092\u4f5c\u6210\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u304c\u5b9f\u884c\u3055\u308c\u308b\u3002 docker ps -a \u30b3\u30f3\u30c6\u30ca\u30a4\u30e1\u30fc\u30b8\u306e\u4e00\u89a7\u3092\u8868\u793a docker run -it ubuntu bash :docker \u3067ubuntu\u306ebash\u3092\u8d77\u52d5\uff08bash\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30de\u30f3\u30c9\u4e0a\u66f8\u304d\uff09, -it \u306fbash\u3092\u8d77\u52d5\u72b6\u614b\uff08up\uff09\u306b\u4fdd\u6301\u3059\u308b\u3002-it\u304c\u306a\u3044\u3068exit\u72b6\u614b\u306b\u5909\u308f\u308b\u3002-i:\u6a19\u6e96\u5165\u529b\u3092\u958b\u304f\u3001-t:\u51fa\u529b\u304c\u304d\u308c\u3044\u306b\u306a\u308b\u3002 ctri + p + q :detach \u7d42\u4e86 docker attach <container> attach\u3067up\u72b6\u614b\u306econtainer\u306b\u5165\u308b\u3002 exit :\u7d42\u4e86 docker restart <container> docker exec -it <container> <command> docker commit <container> <new image> \u30b3\u30f3\u30c6\u30ca\u304b\u3089new image\u3068\u3057\u3066\u4fdd\u5b58 docker commit <container> ubuntu:updated \u30bb\u30df\u30b3\u30ed\u30f3\u3067tag\u540d\u306b\u306a\u308b image\u540d\u306frepostitory\u540d\uff0btag\u540d docker tag <source> <target> docker tag ubuntu:updated <username>/my-repo :\u540d\u524d\u306e\u5909\u66f4 library/ubuntu\u306f, \u6b63\u5f0f\u306b\u306fregistry-1.docker.io/library/ubuntu:latest docker push <image> docker pull <image> docker rmi <image> docker image \u3092\u524a\u9664 docker rm <container> \u30b3\u30f3\u30c6\u30ca\u306e\u524a\u9664 docker stop <container> \u30b3\u30f3\u30c6\u30ca\u3092\u6b62\u3081\u308b docker system prune :\u30b3\u30f3\u30c6\u30ca\u5168\u524a\u9664 docker run --name <name> <image> :\u30b3\u30f3\u30c6\u30ca\u306e\u540d\u524d\u3092\u3064\u3051\u308b\u3002 docker run -d <image> :\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5\u5f8c\u306bdetach\u3059\u308b\uff08host\u306b\u623b\u308b\uff09 docker run -rm <image> :\u30b3\u30f3\u30c6\u30ca\u3092exit\u5f8c\u306b\u524a\u9664\u3059\u308b\u3002 docker file\u306e\u4f5c\u6210 \u00b6 1 2 3 4 5 6 7 8 9 10 11 FROM ubuntu:latest ADD copressed.tar / COPY something /new_directory/ ENV key1 value RUN apt update && apt install -y \\ aaa \\ bbb \\ ccc WORKDIR /sample_folder RUN touch something CMD [ \"executable\" , \"param1\" , \"param2\" ] docker build <directory> docker build -t <name> <directory> docker build -f <dockerfilename> <build context> \u540d\u524d\u306f\u30c9\u30c3\u30c8\u3067\u3064\u306a\u304c\u308b\u3053\u3068\u304c\u591a\u3044\u3002Dockerfile\u3068\u3044\u3046\u540d\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u30d3\u30eb\u30c9\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u5165\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3002 FROM :\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30a4\u30e1\u30fc\u30b8\u3092\u6c7a\u5b9a RUN :Linux\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3002RUN\u6bce\u306bLayer\u304c\u4f5c\u3089\u308c\u308b\u3002Layer\u6570\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002&&\u3067\u3064\u306a\u3052\u308b\u3002(\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u3067)\\\u30d0\u30c3\u30af\u30b9\u30e9\u30c3\u30b7\u30e5\u3067\u6539\u884c\u3059\u308b\u3002 \u6700\u521d\u306fLayer\u3092\u7d30\u304b\u304f\u5206\u3051\u3066\u901a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\u6700\u5f8c\u306bLayer\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 CMD :\u30b3\u30f3\u30c6\u30ca\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u3092\u6307\u5b9a\u3002CMD [\"command\", \"param1\", \"paramn2\"] ex. CMD [/bin/bash], CMD\u306f\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u3089\u306a\u3044\u3002 Docker\u30b3\u30de\u30f3\u30c9\u3067Docker Daemon\u306b\u547d\u4ee4\u3092\u51fa\u3059 * COPY: \u5358\u7d14\u306b\u30d5\u30a1\u30a4\u30eb\u3084\u30d5\u30a9\u30eb\u30c0\u3092\u30b3\u30d4\u30fc\u3059\u308b\u5834\u5408 * ADD: tar\u306e\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u3092\u89e3\u7b54\u3059\u308b ENTRYPOINT \u306f\u4e0a\u66f8\u304d\u3067\u304d\u306a\u3044\uff08CMD\u306f\u4e0a\u66f8\u304d\u3067\u304d\u308b\uff09\u3002ENTRYPOINT\u304c\u3042\u308b\u3068\u304d\u306fCMD\u306fparams\u306e\u307f\u3092\u66f8\u304f\u3002 ENTRYPOINT\u306f\u30b3\u30f3\u30c6\u30ca\u3092\u30b3\u30de\u30f3\u30c9\u306e\u3088\u3046\u306b\u4f7f\u3044\u305f\u3044\u3068\u304d\u3002 ENV :\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u3002 WORKDIR \u5b9f\u884c\u74b0\u5883\u3092\u5909\u66f4\u3059\u308b\u3002 \u30db\u30b9\u30c8\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u3064\u306a\u3050 \u00b6 docker run -it -v <host>:<container> <image bash> docker run -it -u $(id -u):$(id -g) -v ~/mouted_folder:/new_dir <image> bash -u <uder id>:<group id>: \u30e6\u30fc\u30b6ID\u3068\u30b0\u30eb\u30fc\u30d7ID\u3092\u6307\u5b9a\u3059\u308b -p <host_port>:<container_port> docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash docker run -it --rm --cpus 4 --memory 2g ubuntu bash docker inspect <container> | grep -i cpu \u30ed\u30fc\u30ab\u30eb\u3067\u74b0\u5883\u69cb\u7bc9 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 FROM ubuntu:latest RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ] docker run -p 8888:8888 -v ~/Desktop/ds-pyhton:/work --name my-lab <container> GPU\u74b0\u5883\u4f8b \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip && pip install \\ keras == 2 .3 \\ scipy == 1 .4.1 \\ tensorflow-gpu == 2 .1 WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ]","title":"Docker"},{"location":"docker/#docker-file","text":"1 2 3 4 5 6 7 8 9 10 11 FROM ubuntu:latest ADD copressed.tar / COPY something /new_directory/ ENV key1 value RUN apt update && apt install -y \\ aaa \\ bbb \\ ccc WORKDIR /sample_folder RUN touch something CMD [ \"executable\" , \"param1\" , \"param2\" ] docker build <directory> docker build -t <name> <directory> docker build -f <dockerfilename> <build context> \u540d\u524d\u306f\u30c9\u30c3\u30c8\u3067\u3064\u306a\u304c\u308b\u3053\u3068\u304c\u591a\u3044\u3002Dockerfile\u3068\u3044\u3046\u540d\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u30d3\u30eb\u30c9\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u5165\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3002 FROM :\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30a4\u30e1\u30fc\u30b8\u3092\u6c7a\u5b9a RUN :Linux\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3002RUN\u6bce\u306bLayer\u304c\u4f5c\u3089\u308c\u308b\u3002Layer\u6570\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002&&\u3067\u3064\u306a\u3052\u308b\u3002(\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u3067)\\\u30d0\u30c3\u30af\u30b9\u30e9\u30c3\u30b7\u30e5\u3067\u6539\u884c\u3059\u308b\u3002 \u6700\u521d\u306fLayer\u3092\u7d30\u304b\u304f\u5206\u3051\u3066\u901a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\u6700\u5f8c\u306bLayer\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 CMD :\u30b3\u30f3\u30c6\u30ca\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u3092\u6307\u5b9a\u3002CMD [\"command\", \"param1\", \"paramn2\"] ex. CMD [/bin/bash], CMD\u306f\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u3089\u306a\u3044\u3002 Docker\u30b3\u30de\u30f3\u30c9\u3067Docker Daemon\u306b\u547d\u4ee4\u3092\u51fa\u3059 * COPY: \u5358\u7d14\u306b\u30d5\u30a1\u30a4\u30eb\u3084\u30d5\u30a9\u30eb\u30c0\u3092\u30b3\u30d4\u30fc\u3059\u308b\u5834\u5408 * ADD: tar\u306e\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u3092\u89e3\u7b54\u3059\u308b ENTRYPOINT \u306f\u4e0a\u66f8\u304d\u3067\u304d\u306a\u3044\uff08CMD\u306f\u4e0a\u66f8\u304d\u3067\u304d\u308b\uff09\u3002ENTRYPOINT\u304c\u3042\u308b\u3068\u304d\u306fCMD\u306fparams\u306e\u307f\u3092\u66f8\u304f\u3002 ENTRYPOINT\u306f\u30b3\u30f3\u30c6\u30ca\u3092\u30b3\u30de\u30f3\u30c9\u306e\u3088\u3046\u306b\u4f7f\u3044\u305f\u3044\u3068\u304d\u3002 ENV :\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u3002 WORKDIR \u5b9f\u884c\u74b0\u5883\u3092\u5909\u66f4\u3059\u308b\u3002","title":"docker file\u306e\u4f5c\u6210"},{"location":"docker/#_1","text":"docker run -it -v <host>:<container> <image bash> docker run -it -u $(id -u):$(id -g) -v ~/mouted_folder:/new_dir <image> bash -u <uder id>:<group id>: \u30e6\u30fc\u30b6ID\u3068\u30b0\u30eb\u30fc\u30d7ID\u3092\u6307\u5b9a\u3059\u308b -p <host_port>:<container_port> docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash docker run -it --rm --cpus 4 --memory 2g ubuntu bash docker inspect <container> | grep -i cpu","title":"\u30db\u30b9\u30c8\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u3064\u306a\u3050"},{"location":"docker/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 FROM ubuntu:latest RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ] docker run -p 8888:8888 -v ~/Desktop/ds-pyhton:/work --name my-lab <container>","title":"\u30ed\u30fc\u30ab\u30eb\u3067\u74b0\u5883\u69cb\u7bc9"},{"location":"docker/#gpu","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip && pip install \\ keras == 2 .3 \\ scipy == 1 .4.1 \\ tensorflow-gpu == 2 .1 WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ]","title":"GPU\u74b0\u5883\u4f8b"},{"location":"gan/","text":"\u751f\u6210\u30e2\u30c7\u30eb \u00b6 \u751f\u6210\u30e2\u30c7\u30eb\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3069\u306e\u3088\u3046\u306b\u751f\u6210\u3055\u308c\u308b\u304b\u78ba\u7387\u30e2\u30c7\u30eb\u306e\u89b3\u70b9\u304b\u3089\u8a18\u8ff0\u3059\u308b\u3002\u6f5c\u5728\u7a7a\u9593\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3002 Note \u8b58\u5225\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(y|\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u306e\u30e9\u30d9\u30eb \\(y\\) \u306e\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\uff09 \u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u89b3\u6e2c\u3055\u308c\u308b\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\uff09 \u6f5c\u5728\u5909\u6570\u3068\u306f\u751f\u6210\u753b\u50cf\u306e\u5143\u306b\u306a\u308b\u6b21\u5143\u524a\u6e1b\u3055\u308c\u305f\u7279\u5fb4\u91cf VAE\u306f\u6f5c\u5728\u5909\u6570\u3092\u6b63\u898f\u5206\u5e03\u3068\u4eee\u5b9a \u5909\u5206\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc \u00b6 \u6f5c\u5728\u7a7a\u9593\u304c\u6b63\u898f\u5206\u5e03 GAN \u00b6 \u8b58\u5225\u5668\u306e\u8a13\u7df4 \u00b6 \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30ebx\u3092\u53d6\u308a\u51fa\u3059 \u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5927\u5316 \\(p_G(x_i|\\textbf{z})\\) \u3092 \\(p_{r}(x)\\) \u306b\u8fd1\u3065\u3051\u3066\u3044\u304f\u305f\u3081\u306e\u6307\u6a19\u3068\u3057\u3066Kullback\u2013Leibler divergence\uff08\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306e\u8ddd\u96e2\u306e\u5c3a\u5ea6\uff09\u3068Jensen\u2013Shannon (JS)divergence\u304c\u3042\u308b\u3002GAN\u306e\u640d\u5931\u95a2\u6570\u306f\u751f\u6210\u5668\u306eJS divergence\u306e\u6700\u5c0f\u5316\uff08\u8b58\u5225\u5668\u304b\u3089\u898b\u3066\u6700\u5927\u5316\uff09\u304b\u3089\u5c0e\u304b\u308c\u308b\u3002 GAN\u306e\u640d\u5931\u95a2\u6570 \u00b6 \\[\\min_{G}\\max_{D} E_{x\\sim p_r} [\\log D(z)] + E_{x\\sim p_z} [\\log (1-D(G(z)))]\\] \u751f\u6210\u5668\u306e\u8a13\u7df4 \u00b6 \u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\uff08- \u30c7\u30fc\u30bf\u304c\u5f93\u3046\u78ba\u7387\u5206\u5e03 \\(p_{r}(x)\\) \u305d\u306e\u3082\u306e\u306f\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u751f\u6210\u5668\u306e\u78ba\u7387 \\(p_G(x_i|\\textbf{z})\\) \u3067\u8fd1\u4f3c\u3059\u308b\u3002\uff09 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u3066x*\u304c\u672c\u7269\u304b\u63a8\u5b9a \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5c0f\u5316 https://www.iangoodfellow.com/slides/2019-05-07.pdf GAN\u306e\u53ce\u675f\u6761\u4ef6 \u00b6 \u30ca\u30c3\u30b7\u30e5\u5747\u8861 -\u3000\u751f\u6210\u5668\u304c\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4e2d\u306b\u3042\u308b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30eb\u3068\u898b\u5206\u3051\u304c\u3064\u304b\u306a\u3044\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\u3059\u308b \u8b58\u5225\u6a5f\u306e\u6b63\u7b54\u7387\u304c50%\uff08\u30e9\u30f3\u30c0\u30e0\u306b\u3057\u304b\u751f\u6210\u3067\u304d\u306a\u3044\uff09 GAN\u306e\u6b20\u70b9 \u00b6 \u5b66\u7fd2\u6642\u9593\u306e\u9577\u3055 \u30e2\u30fc\u30c9\u5d29\u58ca\uff1a\u3044\u304f\u3064\u304b\u306e\u30e2\u30fc\u30c9\u304c\u751f\u6210\u3055\u308c\u308b\u30b5\u30f3\u30d7\u30eb\u306b\u542b\u307e\u308c\u306a\u304f\u306a\u308b \u751f\u6210\u5668\u3068\u8b58\u5225\u5668\u306e\u30d0\u30e9\u30f3\u30b9\uff1a\u8b58\u5225\u5668\u304c\u5f37\u3059\u304e\u308b\uff1d\uff1e\u52fe\u914d\u6d88\u5931\u3001\u8b58\u5225\u5668\u304c\u5b66\u7fd2\u3057\u306a\u3044\uff1d\uff1e\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u4e0a\u304c\u3089\u306a\u3044 \u751f\u6210\u753b\u50cf\u306b\u7d30\u304b\u306a\u30ce\u30a4\u30ba\u304c\u5165\u308b \u6bd4\u8f03\u53ef\u80fd\u306a\u578b\u306e\u30c7\u30fc\u30bf\u3067\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044 \u640d\u5931\u95a2\u6570\u306e\u5024\u3068\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u5fc5\u305a\u3057\u3082\u76f8\u95a2\u3057\u306a\u3044\u3002 \u6539\u5584\u6cd5 \u00b6 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u6df1\u304f\u3059\u308b\u3002\uff08Progressive GAN\uff09 \u30b2\u30fc\u30e0\u306e\u8a2d\u5b9a\u3092\u5909\u3048\u308b\u3002 Min-Max\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 \u975e\u98fd\u548c\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 WassertsteinGAN \u30cf\u30c3\u30af \u00b6 \u5165\u529b\u306e\u6b63\u898f\u5316 \u52fe\u914d\u306e\u5236\u7d04 \u8b58\u5225\u5668\u3092\u3088\u308a\u591a\u304f\u8a13\u7df4\u3059\u308b \u758e\u306a\u52fe\u914d\u3092\u907f\u3051\u308b \u30bd\u30d5\u30c8\u306a\u3042\u308b\u3044\u306f\u30ce\u30a4\u30ba\u4ed8\u304d\u306e\u30e9\u30d9\u30eb\u306b\u5207\u308a\u66ff\u3048\u308b \u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , z ): img = self . model ( z ) img = img . view ( img . size ( 0 ), * img_shape ) return img \u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . model = nn . Sequential ( nn . Linear ( int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 256 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 256 , 1 ), nn . Sigmoid (), ) def forward ( self , img ): img_flat = img . view ( img . size ( 0 ), - 1 ) validity = self . model ( img_flat ) return validity GAN\u306e\u5b66\u7fd2\u306e1 step \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizer_G = optim . Adam ( netG . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) optimizer_D = optim . Adam ( netD . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) shape = ( batch_size , 1 , 1 , 1 ) labels_real = torch . ones ( shape ) . to ( device ) labels_fake = torch . zeros ( shape ) . to ( device ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss DCGAN (Deep Convolutional GAN) \u00b6 \u30ce\u30a4\u30ba\u30d9\u30af\u30c8\u30eb\u3092\u5165\u529b\u3057\u3066\u3001\u5e45\u3068\u9ad8\u3055\u3092\u62e1\u5927\u3057\u3064\u3064\u3001\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u6e1b\u3089\u3057\u3066\u3044\u304f\u3001\u6700\u7d42\u7684\u306b\uff08H x W x C\uff09\u3092\u51fa\u529b\u3002 G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084 \u8ee2\u7f6e\u7573\u307f\u8fbc\u307f \u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528 \u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Generator ( nn . Module ): def __init__ ( self , z_dim = 100 , ngf = 128 , nc = 1 ): super () . __init__ () self . convt1 = self . conv_trans_layers ( z_dim , 4 * ngf , 3 , 1 , 0 ) self . convt2 = self . conv_trans_layers ( 4 * ngf , 2 * ngf , 3 , 2 , 0 ) self . convt3 = self . conv_trans_layers ( 2 * ngf , ngf , 4 , 2 , 1 ) self . convt4 = nn . Sequential ( nn . ConvTranspose2d ( ngf , nc , 4 , 2 , 1 ), nn . Tanh () ) @staticmethod def conv_trans_layers ( in_channels , out_channels , kernel_size , stride , padding ): net = nn . Sequential ( nn . ConvTranspose2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ) ) return net def forward ( self , x ): out = self . convt1 ( x ) out = self . convt2 ( out ) out = self . convt3 ( out ) out = self . convt4 ( out ) return out \u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Discrimnator ( nn . Module ): def __init__ ( self , nc = 1 , ndf = 128 ): super () . __init__ () self . conv1 = self . conv_layers ( nc , ndf , has_batch_norm = False ) self . conv2 = self . conv_layers ( ndf , 2 * ndf ) self . conv3 = self . conv_layers ( 2 * ndf , 4 * ndf , 3 , 2 , 0 ) self . conv4 = nn . Sequential ( nn . Conv2d ( 4 * ndf , 1 , 3 , 1 , 0 ), nn . Sigmoid () ) @staticmethod def conv_layers ( in_channels , out_channels , kernel_size = 4 , stride = 2 , padding = 1 , has_batch_norm = True ): layers = [ nn . Conv2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ) ] if has_batch_norm : layers . append ( nn . BatchNorm2d ( out_channels )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) net = nn . Sequential ( * layers ) return net def forward ( self , x ): out = self . conv1 ( x ) out = self . conv2 ( out ) out = self . conv3 ( out ) out = self . conv4 ( out ) return out Conditional GAN \u00b6 \u30ce\u30a4\u30ba\u3084\u753b\u50cf\u306b\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\u3059\u308b\u3053\u3068\u3067\u7279\u5b9a\u306e\u753b\u50cf\u3092\u751f\u6210 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_noise_with_label ( noise , labels , device , n_class = 10 ): one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) . to ( device ) concat_noise = torch . cat (( noise , one_hot_vec ), dim = 1 ) return concat_noise def get_img_with_label ( imgs , labels , device , n_class = 10 ): B , _ , H , W = imgs . size () one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) one_hot_vec = one_hot_vec . expand ( B , n_class , H , W ) . to ( device ) concat_img = torch . cat (( imgs , one_hot_vec ), dim = 1 ) return concat_img def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # Get the noise with label noise_with_label = get_noise_with_label ( noise , labels , device ) # Get the real images with label real_imgs_with_label = get_img_with_label ( real_imgs , labels , device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss Wassersteing GAN \u00b6 \u8a13\u7df4\u306e\u5b89\u5b9a\u5316\u3068\u5224\u65ad\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u3001\u640d\u5931\u95a2\u6570\u306b Wasserstein\u640d\u5931 \u3092\u5c0e\u5165\u3002\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002 \u8b58\u5225\u5668\u306b 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3057\u305f\u3002 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3059\u305f\u3081\u306bWeight\u3092\u3042\u308b\u7bc4\u56f2\u3067\u30af\u30ea\u30c3\u30d7\u3057\u3001\u52fe\u914d\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u5247\u5316\u9805\u3092\u5897\u3084\u3059\u3002 \u8b58\u5225\u5668\u3092\u591a\u304f\u8a13\u7df4\u3059\u308b\u3002 optimizer\u306b RMSProp \u3092\u4f7f\u3046\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) for p in netD . parameters (): p . data . clamp_ ( opt . c_lower , opt . c_upper ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss Wassersteing GAN (Gradient penalty) \u00b6 -\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092False\u306b\u3059\u308b\u3002 -RMSprop\u306bweight_decay = 1e-4\u3092\u5165\u308c\u308b. 1 2 3 4 5 6 7 8 9 10 11 def gradient_penalty ( real_imgs , fake_img , gp_weight , netD , device ): batch_size = real_imgs . size ()[ 0 ] alpha = torch . randn ( batch_size , 1 , 1 , 1 ) alpha = alpha . expand_as ( real_imgs ) . to ( device ) interpolated_imgs = ( alpha * real_imgs . data + ( 1 - alpha ) * fake_img . data ) . requires_grad_ () grad_outputs = torch . autograd . grad ( inyerpolated_out , interpolated_imgs , grad_outputs = grad_outputs , retain_graph = True )[ 0 ] gradients = gradients . view ( batch_size , - 1 ) gradients_nrom = torch . sqrt ( torch . sum ( gradients ** 2 , dim = 1 ) + eps ) gp = gp_weight * (( gradients_norm - 1 ) ** 2 ) . mean () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr , weight_decay = 1e-4 ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr , weight_decay = 1e-4 ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # for p in netD.parameters(): # p.data.clamp_(opt.c_lower, opt.c_upper) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) gp_loss = gradient_penalty ( real_imgs , fake_imgs , opt . gp_weight , netD , device ) d_loss = real_loss + fake_loss + gp_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss Cycke GAN \u00b6 \u57fa\u672c\u7684\u306bEncoder-Decoder\u69cb\u9020 Instance Normalization : \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3067\u306f\u753b\u50cf\u5168\u4f53\u306e\u307f\u3067\u6b63\u898f\u5316\u3092\u884c\u3046\u3002\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba = 1\u306eBN\u3068\u540c\u3058 \u30ea\u30d5\u30ec\u30af\u30b7\u30e7\u30f3\u30d1\u30c3\u30c9\uff1a\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u3068\u306f\u7570\u306a\u308a\u30a8\u30c3\u30b8\u90e8\u5206\u3092\u7af6\u6cf3\u9762\u3068\u3057\u3066\u53cd\u5c04\u3055\u305b\u305f\u30d1\u30c7\u30a3\u30f3\u30b0\u65b9\u6cd5\u3002\u6298\u308a\u8fd4\u3057\u3066\u3064\u306a\u3052\u308b\u3053\u3068\u3067\u753b\u50cf\u306e\u4e2d\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u30a8\u30c3\u30b8\u5468\u8fba\u3067\u4fdd\u3064\u3002 \u30b5\u30a4\u30af\u30eb\u4e00\u8cab\u6027\u640d\u5931 \u540c\u4e00\u6027\u640d\u5931 Replay Buffer","title":"GAN basis"},{"location":"gan/#_1","text":"\u751f\u6210\u30e2\u30c7\u30eb\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3069\u306e\u3088\u3046\u306b\u751f\u6210\u3055\u308c\u308b\u304b\u78ba\u7387\u30e2\u30c7\u30eb\u306e\u89b3\u70b9\u304b\u3089\u8a18\u8ff0\u3059\u308b\u3002\u6f5c\u5728\u7a7a\u9593\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3002 Note \u8b58\u5225\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(y|\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u306e\u30e9\u30d9\u30eb \\(y\\) \u306e\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\uff09 \u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u89b3\u6e2c\u3055\u308c\u308b\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\uff09 \u6f5c\u5728\u5909\u6570\u3068\u306f\u751f\u6210\u753b\u50cf\u306e\u5143\u306b\u306a\u308b\u6b21\u5143\u524a\u6e1b\u3055\u308c\u305f\u7279\u5fb4\u91cf VAE\u306f\u6f5c\u5728\u5909\u6570\u3092\u6b63\u898f\u5206\u5e03\u3068\u4eee\u5b9a","title":"\u751f\u6210\u30e2\u30c7\u30eb"},{"location":"gan/#_2","text":"\u6f5c\u5728\u7a7a\u9593\u304c\u6b63\u898f\u5206\u5e03","title":"\u5909\u5206\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc"},{"location":"gan/#gan","text":"","title":"GAN"},{"location":"gan/#_3","text":"\u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30ebx\u3092\u53d6\u308a\u51fa\u3059 \u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5927\u5316 \\(p_G(x_i|\\textbf{z})\\) \u3092 \\(p_{r}(x)\\) \u306b\u8fd1\u3065\u3051\u3066\u3044\u304f\u305f\u3081\u306e\u6307\u6a19\u3068\u3057\u3066Kullback\u2013Leibler divergence\uff08\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306e\u8ddd\u96e2\u306e\u5c3a\u5ea6\uff09\u3068Jensen\u2013Shannon (JS)divergence\u304c\u3042\u308b\u3002GAN\u306e\u640d\u5931\u95a2\u6570\u306f\u751f\u6210\u5668\u306eJS divergence\u306e\u6700\u5c0f\u5316\uff08\u8b58\u5225\u5668\u304b\u3089\u898b\u3066\u6700\u5927\u5316\uff09\u304b\u3089\u5c0e\u304b\u308c\u308b\u3002","title":"\u8b58\u5225\u5668\u306e\u8a13\u7df4"},{"location":"gan/#gan_1","text":"\\[\\min_{G}\\max_{D} E_{x\\sim p_r} [\\log D(z)] + E_{x\\sim p_z} [\\log (1-D(G(z)))]\\]","title":"GAN\u306e\u640d\u5931\u95a2\u6570"},{"location":"gan/#_4","text":"\u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\uff08- \u30c7\u30fc\u30bf\u304c\u5f93\u3046\u78ba\u7387\u5206\u5e03 \\(p_{r}(x)\\) \u305d\u306e\u3082\u306e\u306f\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u751f\u6210\u5668\u306e\u78ba\u7387 \\(p_G(x_i|\\textbf{z})\\) \u3067\u8fd1\u4f3c\u3059\u308b\u3002\uff09 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u3066x*\u304c\u672c\u7269\u304b\u63a8\u5b9a \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5c0f\u5316 https://www.iangoodfellow.com/slides/2019-05-07.pdf","title":"\u751f\u6210\u5668\u306e\u8a13\u7df4"},{"location":"gan/#gan_2","text":"\u30ca\u30c3\u30b7\u30e5\u5747\u8861 -\u3000\u751f\u6210\u5668\u304c\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4e2d\u306b\u3042\u308b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30eb\u3068\u898b\u5206\u3051\u304c\u3064\u304b\u306a\u3044\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\u3059\u308b \u8b58\u5225\u6a5f\u306e\u6b63\u7b54\u7387\u304c50%\uff08\u30e9\u30f3\u30c0\u30e0\u306b\u3057\u304b\u751f\u6210\u3067\u304d\u306a\u3044\uff09","title":"GAN\u306e\u53ce\u675f\u6761\u4ef6"},{"location":"gan/#gan_3","text":"\u5b66\u7fd2\u6642\u9593\u306e\u9577\u3055 \u30e2\u30fc\u30c9\u5d29\u58ca\uff1a\u3044\u304f\u3064\u304b\u306e\u30e2\u30fc\u30c9\u304c\u751f\u6210\u3055\u308c\u308b\u30b5\u30f3\u30d7\u30eb\u306b\u542b\u307e\u308c\u306a\u304f\u306a\u308b \u751f\u6210\u5668\u3068\u8b58\u5225\u5668\u306e\u30d0\u30e9\u30f3\u30b9\uff1a\u8b58\u5225\u5668\u304c\u5f37\u3059\u304e\u308b\uff1d\uff1e\u52fe\u914d\u6d88\u5931\u3001\u8b58\u5225\u5668\u304c\u5b66\u7fd2\u3057\u306a\u3044\uff1d\uff1e\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u4e0a\u304c\u3089\u306a\u3044 \u751f\u6210\u753b\u50cf\u306b\u7d30\u304b\u306a\u30ce\u30a4\u30ba\u304c\u5165\u308b \u6bd4\u8f03\u53ef\u80fd\u306a\u578b\u306e\u30c7\u30fc\u30bf\u3067\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044 \u640d\u5931\u95a2\u6570\u306e\u5024\u3068\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u5fc5\u305a\u3057\u3082\u76f8\u95a2\u3057\u306a\u3044\u3002","title":"GAN\u306e\u6b20\u70b9"},{"location":"gan/#_5","text":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u6df1\u304f\u3059\u308b\u3002\uff08Progressive GAN\uff09 \u30b2\u30fc\u30e0\u306e\u8a2d\u5b9a\u3092\u5909\u3048\u308b\u3002 Min-Max\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 \u975e\u98fd\u548c\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 WassertsteinGAN","title":"\u6539\u5584\u6cd5"},{"location":"gan/#_6","text":"\u5165\u529b\u306e\u6b63\u898f\u5316 \u52fe\u914d\u306e\u5236\u7d04 \u8b58\u5225\u5668\u3092\u3088\u308a\u591a\u304f\u8a13\u7df4\u3059\u308b \u758e\u306a\u52fe\u914d\u3092\u907f\u3051\u308b \u30bd\u30d5\u30c8\u306a\u3042\u308b\u3044\u306f\u30ce\u30a4\u30ba\u4ed8\u304d\u306e\u30e9\u30d9\u30eb\u306b\u5207\u308a\u66ff\u3048\u308b","title":"\u30cf\u30c3\u30af"},{"location":"gan/#_7","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , z ): img = self . model ( z ) img = img . view ( img . size ( 0 ), * img_shape ) return img","title":"\u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#_8","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . model = nn . Sequential ( nn . Linear ( int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 256 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 256 , 1 ), nn . Sigmoid (), ) def forward ( self , img ): img_flat = img . view ( img . size ( 0 ), - 1 ) validity = self . model ( img_flat ) return validity","title":"\u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#gan1-step","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizer_G = optim . Adam ( netG . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) optimizer_D = optim . Adam ( netD . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) shape = ( batch_size , 1 , 1 , 1 ) labels_real = torch . ones ( shape ) . to ( device ) labels_fake = torch . zeros ( shape ) . to ( device ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"GAN\u306e\u5b66\u7fd2\u306e1 step"},{"location":"gan/#dcgan-deep-convolutional-gan","text":"\u30ce\u30a4\u30ba\u30d9\u30af\u30c8\u30eb\u3092\u5165\u529b\u3057\u3066\u3001\u5e45\u3068\u9ad8\u3055\u3092\u62e1\u5927\u3057\u3064\u3064\u3001\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u6e1b\u3089\u3057\u3066\u3044\u304f\u3001\u6700\u7d42\u7684\u306b\uff08H x W x C\uff09\u3092\u51fa\u529b\u3002 G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084 \u8ee2\u7f6e\u7573\u307f\u8fbc\u307f \u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528","title":"DCGAN (Deep Convolutional GAN)"},{"location":"gan/#_9","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Generator ( nn . Module ): def __init__ ( self , z_dim = 100 , ngf = 128 , nc = 1 ): super () . __init__ () self . convt1 = self . conv_trans_layers ( z_dim , 4 * ngf , 3 , 1 , 0 ) self . convt2 = self . conv_trans_layers ( 4 * ngf , 2 * ngf , 3 , 2 , 0 ) self . convt3 = self . conv_trans_layers ( 2 * ngf , ngf , 4 , 2 , 1 ) self . convt4 = nn . Sequential ( nn . ConvTranspose2d ( ngf , nc , 4 , 2 , 1 ), nn . Tanh () ) @staticmethod def conv_trans_layers ( in_channels , out_channels , kernel_size , stride , padding ): net = nn . Sequential ( nn . ConvTranspose2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ) ) return net def forward ( self , x ): out = self . convt1 ( x ) out = self . convt2 ( out ) out = self . convt3 ( out ) out = self . convt4 ( out ) return out","title":"\u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#_10","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Discrimnator ( nn . Module ): def __init__ ( self , nc = 1 , ndf = 128 ): super () . __init__ () self . conv1 = self . conv_layers ( nc , ndf , has_batch_norm = False ) self . conv2 = self . conv_layers ( ndf , 2 * ndf ) self . conv3 = self . conv_layers ( 2 * ndf , 4 * ndf , 3 , 2 , 0 ) self . conv4 = nn . Sequential ( nn . Conv2d ( 4 * ndf , 1 , 3 , 1 , 0 ), nn . Sigmoid () ) @staticmethod def conv_layers ( in_channels , out_channels , kernel_size = 4 , stride = 2 , padding = 1 , has_batch_norm = True ): layers = [ nn . Conv2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ) ] if has_batch_norm : layers . append ( nn . BatchNorm2d ( out_channels )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) net = nn . Sequential ( * layers ) return net def forward ( self , x ): out = self . conv1 ( x ) out = self . conv2 ( out ) out = self . conv3 ( out ) out = self . conv4 ( out ) return out","title":"\u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#conditional-gan","text":"\u30ce\u30a4\u30ba\u3084\u753b\u50cf\u306b\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\u3059\u308b\u3053\u3068\u3067\u7279\u5b9a\u306e\u753b\u50cf\u3092\u751f\u6210 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_noise_with_label ( noise , labels , device , n_class = 10 ): one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) . to ( device ) concat_noise = torch . cat (( noise , one_hot_vec ), dim = 1 ) return concat_noise def get_img_with_label ( imgs , labels , device , n_class = 10 ): B , _ , H , W = imgs . size () one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) one_hot_vec = one_hot_vec . expand ( B , n_class , H , W ) . to ( device ) concat_img = torch . cat (( imgs , one_hot_vec ), dim = 1 ) return concat_img def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # Get the noise with label noise_with_label = get_noise_with_label ( noise , labels , device ) # Get the real images with label real_imgs_with_label = get_img_with_label ( real_imgs , labels , device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"Conditional GAN"},{"location":"gan/#wassersteing-gan","text":"\u8a13\u7df4\u306e\u5b89\u5b9a\u5316\u3068\u5224\u65ad\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u3001\u640d\u5931\u95a2\u6570\u306b Wasserstein\u640d\u5931 \u3092\u5c0e\u5165\u3002\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002 \u8b58\u5225\u5668\u306b 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3057\u305f\u3002 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3059\u305f\u3081\u306bWeight\u3092\u3042\u308b\u7bc4\u56f2\u3067\u30af\u30ea\u30c3\u30d7\u3057\u3001\u52fe\u914d\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u5247\u5316\u9805\u3092\u5897\u3084\u3059\u3002 \u8b58\u5225\u5668\u3092\u591a\u304f\u8a13\u7df4\u3059\u308b\u3002 optimizer\u306b RMSProp \u3092\u4f7f\u3046\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) for p in netD . parameters (): p . data . clamp_ ( opt . c_lower , opt . c_upper ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"Wassersteing GAN"},{"location":"gan/#wassersteing-gan-gradient-penalty","text":"-\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092False\u306b\u3059\u308b\u3002 -RMSprop\u306bweight_decay = 1e-4\u3092\u5165\u308c\u308b. 1 2 3 4 5 6 7 8 9 10 11 def gradient_penalty ( real_imgs , fake_img , gp_weight , netD , device ): batch_size = real_imgs . size ()[ 0 ] alpha = torch . randn ( batch_size , 1 , 1 , 1 ) alpha = alpha . expand_as ( real_imgs ) . to ( device ) interpolated_imgs = ( alpha * real_imgs . data + ( 1 - alpha ) * fake_img . data ) . requires_grad_ () grad_outputs = torch . autograd . grad ( inyerpolated_out , interpolated_imgs , grad_outputs = grad_outputs , retain_graph = True )[ 0 ] gradients = gradients . view ( batch_size , - 1 ) gradients_nrom = torch . sqrt ( torch . sum ( gradients ** 2 , dim = 1 ) + eps ) gp = gp_weight * (( gradients_norm - 1 ) ** 2 ) . mean () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr , weight_decay = 1e-4 ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr , weight_decay = 1e-4 ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # for p in netD.parameters(): # p.data.clamp_(opt.c_lower, opt.c_upper) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) gp_loss = gradient_penalty ( real_imgs , fake_imgs , opt . gp_weight , netD , device ) d_loss = real_loss + fake_loss + gp_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"Wassersteing GAN (Gradient penalty)"},{"location":"gan/#cycke-gan","text":"\u57fa\u672c\u7684\u306bEncoder-Decoder\u69cb\u9020 Instance Normalization : \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3067\u306f\u753b\u50cf\u5168\u4f53\u306e\u307f\u3067\u6b63\u898f\u5316\u3092\u884c\u3046\u3002\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba = 1\u306eBN\u3068\u540c\u3058 \u30ea\u30d5\u30ec\u30af\u30b7\u30e7\u30f3\u30d1\u30c3\u30c9\uff1a\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u3068\u306f\u7570\u306a\u308a\u30a8\u30c3\u30b8\u90e8\u5206\u3092\u7af6\u6cf3\u9762\u3068\u3057\u3066\u53cd\u5c04\u3055\u305b\u305f\u30d1\u30c7\u30a3\u30f3\u30b0\u65b9\u6cd5\u3002\u6298\u308a\u8fd4\u3057\u3066\u3064\u306a\u3052\u308b\u3053\u3068\u3067\u753b\u50cf\u306e\u4e2d\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u30a8\u30c3\u30b8\u5468\u8fba\u3067\u4fdd\u3064\u3002 \u30b5\u30a4\u30af\u30eb\u4e00\u8cab\u6027\u640d\u5931 \u540c\u4e00\u6027\u640d\u5931 Replay Buffer","title":"Cycke GAN"},{"location":"git/","text":"Git \u00b6 Git\u3068\u306f \u00b6 Git\u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306e1\u3064\uff08\u5206\u6563\u7ba1\u7406\u65b9\u5f0f\uff09\u3002\u7279\u5b9a\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5dee\u5206\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3001\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3059\u308b\u3002 \u7528\u8a9e \u00b6 \u30ea\u30dd\u30b8\u30c8\u30ea\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5168\u3066\u306e\u30d5\u30a1\u30a4\u30eb\uff08\u5909\u66f4\u5c65\u6b74\u3084\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3082\u3059\u3079\u3066\uff09\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 \u30b3\u30df\u30c3\u30c8\uff1a\u89aa\u5b50\u95a2\u4fc2\u3092\u6301\u3064\u30b0\u30e9\u30d5\u3002\u30d5\u30a1\u30a4\u30eb\u306e\u72b6\u614b\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3053\u3068\u3002Working directory => staging area \uff08\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3082\u547c\u3070\u308c\u308b\uff09=> \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b3\u30df\u30c3\u30c8 \u30d6\u30e9\u30f3\u30c1\uff1a\u30b3\u30df\u30c3\u30c8\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002HEAD\u306f\u4eca\u81ea\u5206\u304c\u4f5c\u696d\u3057\u3066\u3044\u308b\u30d6\u30e9\u30f3\u30c1\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002\u30b3\u30df\u30c3\u30c8\u524d\u306b\u5206\u5c90\u3055\u305b\u308b\u3002\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u3057\u3066\u30de\u30fc\u30b8\u3055\u305b\u308b\u3002 Github \u00b6 \u30cf\u30a4\u30d5\u30f3\u3067\u540d\u524d\u3092\u533a\u5207\u308b\u306e\u304c\u4e00\u822c\u7684 ssh\u3067\u306e\u8a8d\u8a3c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 \u57fa\u672c\u7684\u306a\u6d41\u308c \u00b6 \u30ed\u30fc\u30ab\u30eb\u306b\u30e6\u30fc\u30b6\u60c5\u5831\u3092\u30bb\u30c3\u30c8\u78ba\u8a8d git config --global user.name \"<username, github\u306eusername>\" git config --global user.email \"<email>\" git config --global --list git config --global --replace-all core.pager \"less -F -X\" git config --global pull.rebase true \u30ea\u30dd\u30b8\u30c8\u30ea\u3092clone git clone <remote_repo_url> git remote -v : \u767b\u9332\u3057\u3066\u3042\u308b\u30ea\u30e2\u30fc\u30c8\u30ea\u30dc\u3092\u78ba\u8a8d git clone \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f origin \u304c\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b\u7d10\u4ed8\u3044\u3066\u3044\u308b \u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\uff08\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308b\uff09 git branch \u30d6\u30e9\u30f3\u30c1\u4e00\u89a7 git branch -a \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u542b\u3080\u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u306e\u4e00\u89a7 git branch <branch name> branch name\u3068\u3044\u3046branch\u3092\u4f5c\u6210, HEAD\u306e\u30dd\u30a4\u30f3\u30bf\u5148\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u3002 git branch -m <old name> <new name> git branch -d <branch-name> \u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664 git checkout <branch name> branch name\u306b\u79fb\u52d5\u3002HEAD\u30dd\u30a4\u30f3\u30bf\u306e\u5207\u308a\u66ff\u3048 git checkout -b <branch name> \uff08\u5b9f\u7528\u4e0a\uff09branch \u4f5c\u6210\u3057\u3066branch name\u306b\u79fb\u52d5 \u30d6\u30e9\u30f3\u30c1\u540d\u306f\u30cf\u30a4\u30d5\u30f3\u3067\u533a\u5207\u308b \u30ed\u30fc\u30ab\u30eb\u30d5\u30a1\u30a4\u30eb\uff08at working directory, working tree\uff09\u3092\u66f4\u65b0\u3057\u3066Staging\u30a8\u30ea\u30a2\u306b\u3042\u3052\u308b git diff --<filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD --<filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD --<filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ --<filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main --<filename> git add <file name> git add . git status \u72b6\u6cc1\u3092\u78ba\u8a8d \u30b3\u30df\u30c3\u30c8\u3059\u308b git commit -m \"commit message\" git tag <tagname> git tag --list git log --oneline --all --graph \u30b3\u30df\u30c3\u30c8\u3057\u305f\u5c65\u6b74\u3092\u78ba\u8a8d \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092pull\u3057\u3066\u304b\u3089\u3001\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306bpush\u3002pull\u306ffetch + merge git pull <remote ref> <branch name> git pull --rebase <remote ref> <branch name> : pull\u3059\u308b\u3068\u304d\u306brebase\u3059\u308b git pull origin main git push <remote ref> <branch name> git push origin new-branch git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b OSS\u306a\u3069\u306e\u5834\u5408\u3067\u306f\u3001\u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3059\u308b\u3002 git remote add upstream <repourl> git pull upstream main git push origin new-branch push\u3057\u305f\u30d6\u30e9\u30f3\u30c1\u3092pull request\u3092\u4f5c\u3063\u3066\u30ea\u30e2\u30fc\u30c8\u306emain\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8 Github\u3067\u4f5c\u696d\u3002 pull request \u3092\u30af\u30ea\u30c3\u30af => base (main)\u3068 compare (new branch)\u3092\u6307\u5b9a,\u81ea\u5206\u306e\u30ea\u30dd\u304b\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u304b\u3092\u78ba\u8a8d => create pull request => Merge pull request \u3092\u62bc\u3059\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306emain\u306e\u53cd\u6620\u3092\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u306emain\u306b\u53cd\u6620\uff08pull\uff09 git checkout main git pull origin main \u4e0d\u8981\u306a\u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664\u3059\u308b\u3002 git branch -d <branch-name> Gighub\u3067 branches \u304b\u3089\u524a\u9664 \u57fa\u672c\u64cd\u4f5c \u00b6 \u30b9\u30af\u30e9\u30c3\u30c1\u304b\u3089\u4f5c\u6210\uff08.git\u306e\u4f5c\u6210\uff09 git init <project-name> .git \u306e\u524a\u9664 rm -rf .git \u65e2\u5b58\u306e\u30d5\u30a9\u30eb\u30c0\u3092git\u30ea\u30dd\u306b\u3059\u308b\u3002 git init \u3000\u305d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u304b\u3089.git\u306e\u4f5c\u6210 \u65e2\u5b58\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u81ea\u5206\u306e\u30ea\u30dd\u3092\u30d5\u30a9\u30fc\u30af\u3057\u3066clone git clone <httsps or ssh> track\u30d5\u30a1\u30a4\u30eb\u3068untrack\u30d5\u30a1\u30a4\u30eb git ls-files \u3067track\u3057\u3066\u3044\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u89a7\u3092\u78ba\u8a8d Staging area\u3078\u306eadd\u3092\u30ad\u30e3\u30f3\u30bb\u30eb(git\u306e\u5185\u90e8\u3067\u306f\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5185\u5bb9\u3092staging area\u306b\u4e0a\u66f8\u304d) git reset HEAD <filename> Working directory\u306e\u5185\u5bb9\u3092\u7121\u3057\u306b\u3059\u308b\u3002(git\u306e\u5185\u90e8\u3067\u306fworking directory\u306e\u5185\u5bb9\u3092staging area \u3067\u4e0a\u66f8\u304d\u3057\u3066\u3044\u308b\u3002) git checkout -- <file name> \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5909\u66f4\u3092git\u3067\u7ba1\u7406 git mv <filename1> <filename2> (\u30b7\u30a7\u30eb\u306emv\u3067\u5909\u66f4\u3057\u305f\u5834\u5408\u306f git add -A ) \u30d5\u30a1\u30a4\u30eb\u306e\u524a\u9664\u3092Git\u3067\u7ba1\u7406\u3059\u308b\u3002 git rm <filename> (\u30b3\u30df\u30c3\u30c8\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044) git commit -m \"deleted\" \u524a\u9664\u5185\u5bb9\u306e\u53d6\u308a\u6d88\u3057 git reset HEAD <filename> git checkout -- <file name> \u30b3\u30df\u30c3\u30c8\u306e\u5c65\u6b74\u3092\u78ba\u8a8d\u3059\u308b git log --oneline, --graph, --<filename>, --follow <filename> git show <commitID> Git\u306e\u7ba1\u7406\u304b\u3089\u5916\u3059\u3002 .gitignore \u30d5\u30a1\u30a4\u30eb \u30b5\u30a4\u30ba\u304c\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u3084\u30d0\u30a4\u30ca\u30ea\u30fc\u30d5\u30a1\u30a4\u30eb\u3001\u4e2d\u9593\u30d5\u30a1\u30a4\u30eb\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u542b\u3080\u30d5\u30a1\u30a4\u30eb\u3001\u304f\u30a2\u30c3\u30b7\u30e5\u30d5\u30a1\u30a4\u30eb\u306a\u3069 \u30d6\u30e9\u30f3\u30c1\u3068\u30ed\u30fc\u30ab\u30eb\u3067\u30de\u30fc\u30b8,\u30ed\u30fc\u30ab\u30eb\u3067\u306e\u307frebase\u3059\u308b \u00b6 git merge <branchname> \uff1abranchname\u3092\u4eca\u3044\u308b\u30d6\u30e9\u30f3\u30c1\uff08\u666e\u901a\u306fmain\u30d6\u30e9\u30f3\u30c1\uff09\u306b\u53cd\u6620\u3002 git diff <base> <compare> \uff1abase\uff08main\uff09\u3068compare\uff08\u30d6\u30e9\u30f3\u30c1\uff09\u3092\u4f5c\u6210 conflict\u304c\u8d77\u304d\u3066\u3044\u308b\u5834\u5408\u306f\u30a8\u30c7\u30a3\u30bf\u3067\u958b\u3044\u3066\u30a2\u30ce\u30fc\u30c6\u30b7\u30e7\u30f3\u7b87\u6240\u3092\u6d88\u3059\u3002 git rebase main : main \u30d6\u30e9\u30f3\u30c1\u3092rebase\u3059\u308b\u3002rebase\u306f\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea \u00b6 git fetch <remote_ref> :\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092\u3068\u3063\u3066\u304f\u308b\u3002 git pull <remote_ref> <branchname> : git pull \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u304c\u3042\u308b\u5834\u5408\u306f\u5bfe\u51e6\u3059\u308b\u3002 Github\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30d5\u30a9\u30fc\u30af\u5143\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3078\u306epull request\u3092\u51fa\u3059\u3002 git remote add upstream <repourl> : \u30ed\u30fc\u30ab\u30eb\u306b\u306forigin\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd \u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3057\u3066\u304b\u3089\u81ea\u5206\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3057\u3066pull request\u3092\u4f5c\u6210\u3002 \u5dee\u5206diff\u3092\u898b\u308b \u00b6 p4merge \u3092\u5c0e\u5165\u3059\u308b\u3002 git diff \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git HEAD \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -- <filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD -- <filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD -- <filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ -- <filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main -- <filename> Stash\u3092\u4f7f\u3046\u3002 \u00b6 \u4f5c\u696d\u5185\u5bb9\u306e\u4e00\u6642\u56de\u907f - git stash git stash -a git stash list git stash apply git stash drop git stash show stash @{<i>} conflict\u304c\u3042\u308b\u5834\u5408 git mergetool \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306b\u5bfe\u51e6\u3059\u308b\u3002 ## Commit\u306btag\u3092\u4f7f\u3046\u3002 - \u30de\u30a4\u30eb\u30b9\u30c8\u30fc\u30f3\u306btag\u3092\u4f7f\u3063\u3066version\u3092\u7ba1\u7406\u3059\u308b - git tag <tagname> - git tag --list - git tag --delete <tagname> - git tag -a <tagname> tag\u3092\u3064\u3051\u308b\u3002 - git diff <tagname1> <tagname2> - git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 - git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b - git push <remote_ref> :<tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8push\u304b\u3089\u524a\u9664 - git checkout tags/<tagname> - git fetch --tgas --all submodule \u00b6 git submodule add <submodule_url> git submodule update git -recurse-submodule update submodule\u306e\u4e2d\u3067git pull\u3059\u308b\u3002 git submodule foreach 'git pull origin main' others \u00b6 convertio.io wiki\u3092\u4f7f\u3046 octotree zenhub\u3092\u4f7f\u3046\uff1a\u30a2\u30b8\u30e3\u30a4\u30eb\u958b\u767a\u306e\u30ab\u30f3\u30d0\u30f3 git revert <commitID> git reset --hard git reset --sorf HEAD \u30d5\u30a1\u30a4\u30eb\u540d \u9593\u9055\u3063\u3066add \u3057\u305f\u3068\u304d git reset \u2013soft HEAD^ \u9593\u9055\u3063\u3066commit\u3057\u305f\u3068\u304d","title":"Git"},{"location":"git/#git","text":"","title":"Git"},{"location":"git/#git_1","text":"Git\u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306e1\u3064\uff08\u5206\u6563\u7ba1\u7406\u65b9\u5f0f\uff09\u3002\u7279\u5b9a\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5dee\u5206\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3001\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3059\u308b\u3002","title":"Git\u3068\u306f"},{"location":"git/#_1","text":"\u30ea\u30dd\u30b8\u30c8\u30ea\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5168\u3066\u306e\u30d5\u30a1\u30a4\u30eb\uff08\u5909\u66f4\u5c65\u6b74\u3084\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3082\u3059\u3079\u3066\uff09\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 \u30b3\u30df\u30c3\u30c8\uff1a\u89aa\u5b50\u95a2\u4fc2\u3092\u6301\u3064\u30b0\u30e9\u30d5\u3002\u30d5\u30a1\u30a4\u30eb\u306e\u72b6\u614b\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3053\u3068\u3002Working directory => staging area \uff08\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3082\u547c\u3070\u308c\u308b\uff09=> \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b3\u30df\u30c3\u30c8 \u30d6\u30e9\u30f3\u30c1\uff1a\u30b3\u30df\u30c3\u30c8\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002HEAD\u306f\u4eca\u81ea\u5206\u304c\u4f5c\u696d\u3057\u3066\u3044\u308b\u30d6\u30e9\u30f3\u30c1\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002\u30b3\u30df\u30c3\u30c8\u524d\u306b\u5206\u5c90\u3055\u305b\u308b\u3002\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u3057\u3066\u30de\u30fc\u30b8\u3055\u305b\u308b\u3002","title":"\u7528\u8a9e"},{"location":"git/#github","text":"\u30cf\u30a4\u30d5\u30f3\u3067\u540d\u524d\u3092\u533a\u5207\u308b\u306e\u304c\u4e00\u822c\u7684 ssh\u3067\u306e\u8a8d\u8a3c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002","title":"Github"},{"location":"git/#_2","text":"\u30ed\u30fc\u30ab\u30eb\u306b\u30e6\u30fc\u30b6\u60c5\u5831\u3092\u30bb\u30c3\u30c8\u78ba\u8a8d git config --global user.name \"<username, github\u306eusername>\" git config --global user.email \"<email>\" git config --global --list git config --global --replace-all core.pager \"less -F -X\" git config --global pull.rebase true \u30ea\u30dd\u30b8\u30c8\u30ea\u3092clone git clone <remote_repo_url> git remote -v : \u767b\u9332\u3057\u3066\u3042\u308b\u30ea\u30e2\u30fc\u30c8\u30ea\u30dc\u3092\u78ba\u8a8d git clone \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f origin \u304c\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b\u7d10\u4ed8\u3044\u3066\u3044\u308b \u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\uff08\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308b\uff09 git branch \u30d6\u30e9\u30f3\u30c1\u4e00\u89a7 git branch -a \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u542b\u3080\u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u306e\u4e00\u89a7 git branch <branch name> branch name\u3068\u3044\u3046branch\u3092\u4f5c\u6210, HEAD\u306e\u30dd\u30a4\u30f3\u30bf\u5148\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u3002 git branch -m <old name> <new name> git branch -d <branch-name> \u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664 git checkout <branch name> branch name\u306b\u79fb\u52d5\u3002HEAD\u30dd\u30a4\u30f3\u30bf\u306e\u5207\u308a\u66ff\u3048 git checkout -b <branch name> \uff08\u5b9f\u7528\u4e0a\uff09branch \u4f5c\u6210\u3057\u3066branch name\u306b\u79fb\u52d5 \u30d6\u30e9\u30f3\u30c1\u540d\u306f\u30cf\u30a4\u30d5\u30f3\u3067\u533a\u5207\u308b \u30ed\u30fc\u30ab\u30eb\u30d5\u30a1\u30a4\u30eb\uff08at working directory, working tree\uff09\u3092\u66f4\u65b0\u3057\u3066Staging\u30a8\u30ea\u30a2\u306b\u3042\u3052\u308b git diff --<filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD --<filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD --<filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ --<filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main --<filename> git add <file name> git add . git status \u72b6\u6cc1\u3092\u78ba\u8a8d \u30b3\u30df\u30c3\u30c8\u3059\u308b git commit -m \"commit message\" git tag <tagname> git tag --list git log --oneline --all --graph \u30b3\u30df\u30c3\u30c8\u3057\u305f\u5c65\u6b74\u3092\u78ba\u8a8d \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092pull\u3057\u3066\u304b\u3089\u3001\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306bpush\u3002pull\u306ffetch + merge git pull <remote ref> <branch name> git pull --rebase <remote ref> <branch name> : pull\u3059\u308b\u3068\u304d\u306brebase\u3059\u308b git pull origin main git push <remote ref> <branch name> git push origin new-branch git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b OSS\u306a\u3069\u306e\u5834\u5408\u3067\u306f\u3001\u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3059\u308b\u3002 git remote add upstream <repourl> git pull upstream main git push origin new-branch push\u3057\u305f\u30d6\u30e9\u30f3\u30c1\u3092pull request\u3092\u4f5c\u3063\u3066\u30ea\u30e2\u30fc\u30c8\u306emain\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8 Github\u3067\u4f5c\u696d\u3002 pull request \u3092\u30af\u30ea\u30c3\u30af => base (main)\u3068 compare (new branch)\u3092\u6307\u5b9a,\u81ea\u5206\u306e\u30ea\u30dd\u304b\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u304b\u3092\u78ba\u8a8d => create pull request => Merge pull request \u3092\u62bc\u3059\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306emain\u306e\u53cd\u6620\u3092\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u306emain\u306b\u53cd\u6620\uff08pull\uff09 git checkout main git pull origin main \u4e0d\u8981\u306a\u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664\u3059\u308b\u3002 git branch -d <branch-name> Gighub\u3067 branches \u304b\u3089\u524a\u9664","title":"\u57fa\u672c\u7684\u306a\u6d41\u308c"},{"location":"git/#_3","text":"\u30b9\u30af\u30e9\u30c3\u30c1\u304b\u3089\u4f5c\u6210\uff08.git\u306e\u4f5c\u6210\uff09 git init <project-name> .git \u306e\u524a\u9664 rm -rf .git \u65e2\u5b58\u306e\u30d5\u30a9\u30eb\u30c0\u3092git\u30ea\u30dd\u306b\u3059\u308b\u3002 git init \u3000\u305d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u304b\u3089.git\u306e\u4f5c\u6210 \u65e2\u5b58\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u81ea\u5206\u306e\u30ea\u30dd\u3092\u30d5\u30a9\u30fc\u30af\u3057\u3066clone git clone <httsps or ssh> track\u30d5\u30a1\u30a4\u30eb\u3068untrack\u30d5\u30a1\u30a4\u30eb git ls-files \u3067track\u3057\u3066\u3044\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u89a7\u3092\u78ba\u8a8d Staging area\u3078\u306eadd\u3092\u30ad\u30e3\u30f3\u30bb\u30eb(git\u306e\u5185\u90e8\u3067\u306f\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5185\u5bb9\u3092staging area\u306b\u4e0a\u66f8\u304d) git reset HEAD <filename> Working directory\u306e\u5185\u5bb9\u3092\u7121\u3057\u306b\u3059\u308b\u3002(git\u306e\u5185\u90e8\u3067\u306fworking directory\u306e\u5185\u5bb9\u3092staging area \u3067\u4e0a\u66f8\u304d\u3057\u3066\u3044\u308b\u3002) git checkout -- <file name> \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5909\u66f4\u3092git\u3067\u7ba1\u7406 git mv <filename1> <filename2> (\u30b7\u30a7\u30eb\u306emv\u3067\u5909\u66f4\u3057\u305f\u5834\u5408\u306f git add -A ) \u30d5\u30a1\u30a4\u30eb\u306e\u524a\u9664\u3092Git\u3067\u7ba1\u7406\u3059\u308b\u3002 git rm <filename> (\u30b3\u30df\u30c3\u30c8\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044) git commit -m \"deleted\" \u524a\u9664\u5185\u5bb9\u306e\u53d6\u308a\u6d88\u3057 git reset HEAD <filename> git checkout -- <file name> \u30b3\u30df\u30c3\u30c8\u306e\u5c65\u6b74\u3092\u78ba\u8a8d\u3059\u308b git log --oneline, --graph, --<filename>, --follow <filename> git show <commitID> Git\u306e\u7ba1\u7406\u304b\u3089\u5916\u3059\u3002 .gitignore \u30d5\u30a1\u30a4\u30eb \u30b5\u30a4\u30ba\u304c\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u3084\u30d0\u30a4\u30ca\u30ea\u30fc\u30d5\u30a1\u30a4\u30eb\u3001\u4e2d\u9593\u30d5\u30a1\u30a4\u30eb\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u542b\u3080\u30d5\u30a1\u30a4\u30eb\u3001\u304f\u30a2\u30c3\u30b7\u30e5\u30d5\u30a1\u30a4\u30eb\u306a\u3069","title":"\u57fa\u672c\u64cd\u4f5c"},{"location":"git/#rebase","text":"git merge <branchname> \uff1abranchname\u3092\u4eca\u3044\u308b\u30d6\u30e9\u30f3\u30c1\uff08\u666e\u901a\u306fmain\u30d6\u30e9\u30f3\u30c1\uff09\u306b\u53cd\u6620\u3002 git diff <base> <compare> \uff1abase\uff08main\uff09\u3068compare\uff08\u30d6\u30e9\u30f3\u30c1\uff09\u3092\u4f5c\u6210 conflict\u304c\u8d77\u304d\u3066\u3044\u308b\u5834\u5408\u306f\u30a8\u30c7\u30a3\u30bf\u3067\u958b\u3044\u3066\u30a2\u30ce\u30fc\u30c6\u30b7\u30e7\u30f3\u7b87\u6240\u3092\u6d88\u3059\u3002 git rebase main : main \u30d6\u30e9\u30f3\u30c1\u3092rebase\u3059\u308b\u3002rebase\u306f\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3002","title":"\u30d6\u30e9\u30f3\u30c1\u3068\u30ed\u30fc\u30ab\u30eb\u3067\u30de\u30fc\u30b8,\u30ed\u30fc\u30ab\u30eb\u3067\u306e\u307frebase\u3059\u308b"},{"location":"git/#_4","text":"git fetch <remote_ref> :\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092\u3068\u3063\u3066\u304f\u308b\u3002 git pull <remote_ref> <branchname> : git pull \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u304c\u3042\u308b\u5834\u5408\u306f\u5bfe\u51e6\u3059\u308b\u3002 Github\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30d5\u30a9\u30fc\u30af\u5143\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3078\u306epull request\u3092\u51fa\u3059\u3002 git remote add upstream <repourl> : \u30ed\u30fc\u30ab\u30eb\u306b\u306forigin\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd \u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3057\u3066\u304b\u3089\u81ea\u5206\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3057\u3066pull request\u3092\u4f5c\u6210\u3002","title":"\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea"},{"location":"git/#diff","text":"p4merge \u3092\u5c0e\u5165\u3059\u308b\u3002 git diff \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git HEAD \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -- <filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD -- <filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD -- <filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ -- <filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main -- <filename>","title":"\u5dee\u5206diff\u3092\u898b\u308b"},{"location":"git/#stash","text":"\u4f5c\u696d\u5185\u5bb9\u306e\u4e00\u6642\u56de\u907f - git stash git stash -a git stash list git stash apply git stash drop git stash show stash @{<i>} conflict\u304c\u3042\u308b\u5834\u5408 git mergetool \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306b\u5bfe\u51e6\u3059\u308b\u3002 ## Commit\u306btag\u3092\u4f7f\u3046\u3002 - \u30de\u30a4\u30eb\u30b9\u30c8\u30fc\u30f3\u306btag\u3092\u4f7f\u3063\u3066version\u3092\u7ba1\u7406\u3059\u308b - git tag <tagname> - git tag --list - git tag --delete <tagname> - git tag -a <tagname> tag\u3092\u3064\u3051\u308b\u3002 - git diff <tagname1> <tagname2> - git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 - git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b - git push <remote_ref> :<tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8push\u304b\u3089\u524a\u9664 - git checkout tags/<tagname> - git fetch --tgas --all","title":"Stash\u3092\u4f7f\u3046\u3002"},{"location":"git/#submodule","text":"git submodule add <submodule_url> git submodule update git -recurse-submodule update submodule\u306e\u4e2d\u3067git pull\u3059\u308b\u3002 git submodule foreach 'git pull origin main'","title":"submodule"},{"location":"git/#others","text":"convertio.io wiki\u3092\u4f7f\u3046 octotree zenhub\u3092\u4f7f\u3046\uff1a\u30a2\u30b8\u30e3\u30a4\u30eb\u958b\u767a\u306e\u30ab\u30f3\u30d0\u30f3 git revert <commitID> git reset --hard git reset --sorf HEAD \u30d5\u30a1\u30a4\u30eb\u540d \u9593\u9055\u3063\u3066add \u3057\u305f\u3068\u304d git reset \u2013soft HEAD^ \u9593\u9055\u3063\u3066commit\u3057\u305f\u3068\u304d","title":"others"},{"location":"labrad/","text":"labrad_hdf5_dataloader \u00b6 labrad hdf5\u304b\u3089ndarray\u5f62\u5f0f\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_ndarray ( dir_path , file_num , file_name ): \"\"\"Load a hdf5 file and return the data (numpy.array) and columns of labels (list) Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '000## - measurement_name.hdf5' file_name : string Returns ------- data : ndarray variables : list list of parameters \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] raw_data = f . value attrs = f . attrs # Raw data to np.array data = np . array ([ list ( d ) for d in raw_data ]) # Get varialbles labels indep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Independent' ) and str ( x ) . endswith ( 'label' )]) dep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Dependent' ) and str ( x ) . endswith ( 'label' )]) indep_labels = [ attrs [ c ] for c in indep_keys ] dep_labels = [ attrs [ c ] for c in dep_keys ] variables = indep_labels + dep_labels return data , variables labrad_hdf5_get_parameters \u00b6 labrad hdf5\u304b\u3089DV.add_parameters()\u3067\u52a0\u3048\u305f\u6a5f\u5668\u306e\u8a2d\u5b9a\u306e\u60c5\u5831\u306a\u3069\u3092\u53d6\u5f97\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_get_parameters ( dir_path , file_num , file_name ): \"\"\"Get parameter settings (e.g., ferquency, time constant added by DV.add_parameters()) from a labrad hdf5 file Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '00033 - measurement_name.hdf5' file_name : string Returns ------- dictionary Pairs of paramter keys and values Notes ----- The default parameter values are encoded by labrad format. The prefix in endoded values is 'data:application/labrad;base64,' To decode these and get the raw value, we need to simply use DV.get_parameters() or change the backend script in datavault/backend.py This function works in the latter case. \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] attrs = f . attrs # Get parameters labels and values param_ukeys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Param' )]) param_keys = [ c [ 6 :] for c in param_ukeys ] param_values = [ attrs [ c ] for c in param_ukeys ] return { k : v for k , v in zip ( param_keys , param_values )} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_parameters_of_func ( offset = None ): \"\"\"Get a dictionary of paramteres of the function. Parameters ---------- offset : int default value is None Return ------ dictionary The dictionary includes pairs of paremeter's name and the corresponding values. References ---------- [1] https://tottoto.net/python3-get-args-of-current-function/ \"\"\" parent_frame = inspect . currentframe () . f_back info = inspect . getargvalues ( parent_frame ) return { key : info . locals [ key ] for key in info . args [ offset :]} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def create_labrad_hdf5file ( DV , file_path , scan_name , scan_var , meas_var ): \"\"\"Create a labrad hdf5 file from ndarray. Parameters ---------- DV : object file_path : string scan_name : string scan_var : list or tuple meas_var : list or tuple Returns ------- int The file number \"\"\" DV . cd ( '' ) try : DV . mkdir ( file_path ) DV . cd ( file_path ) except Exception : DV . cd ( file_path ) file_name = file_path + '_' + scan_name dv_file = DV . new ( file_name , scan_var , meas_var ) print ' \\r ' , \"new file created, file numer: \" , int ( dv_file [ 1 ][ 0 : 5 ]) return int ( dv_file [ 1 ][ 0 : 5 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def write_meas_parameters ( DV , file_path , file_number , date , scan_name , meas_parameters , amplitude , sensitivity ): \"\"\"Write measurement parameters to txt file and labrad hdf5 file. Parameters ---------- DV : object file_path : string file_number : int date : object scan_name : string meas_parameters : dict scan_var : list or tuple meas_var : list or tuple amplitude : float sensitivity : float Returns ------- None \"\"\" if not os . path . isfile ( meas_details_path + file_path + '.txt' ): with open ( meas_details_path + file_path + '.txt' , \"w+\" ) as f : pass with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"========\" + \" \\n \" ) f . write ( \"file_number: \" + str ( file_number ) + \" \\n \" + \"date: \" + str ( date ) + \" \\n \" + \"measurement:\" + str ( scan_name ) + \" \\n \" ) for k , v in sorted ( meas_parameters . items ()): print ( k , v ) f . write ( str ( k ) + \": \" + str ( v ) + \" \\n \" ) DV . add_parameter ( str ( k ), str ( v )) for i , LA in enumerate ( LAs ): tc = LA . time_constant () sens = LA . sensitivity () f . write ( \"time_constant_\" + str ( i ) + ' : ' + str ( tc ) + \" \\n \" ) f . write ( \"sensitivity_\" + str ( i ) + ' : ' + str ( sens ) + \" \\n \" ) DV . add_parameter ( \"time_constant_\" + str ( i ), tc ) DV . add_parameter ( \"sensitivity_\" + str ( i ), sens ) def write_meas_parameters_end ( date1 , date2 , file_path ): with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"end date: \" + str ( date2 ) + \" \\n \" + \"total time: \" + str ( date2 - date1 ) + \" \\n \" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def get_variables ( DV ): \"\"\"Get variables of a lablad hdf5 file Parameters ---------- DV : object (datavault) Return ------ list A variable of the a lablad hdf5 file \"\"\" variables = [ DV . variables ()[ 0 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 0 ]))] + [ DV . variables ()[ 1 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 1 ]))] return variables","title":"LabRAD"},{"location":"labrad/#labrad_hdf5_dataloader","text":"labrad hdf5\u304b\u3089ndarray\u5f62\u5f0f\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_ndarray ( dir_path , file_num , file_name ): \"\"\"Load a hdf5 file and return the data (numpy.array) and columns of labels (list) Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '000## - measurement_name.hdf5' file_name : string Returns ------- data : ndarray variables : list list of parameters \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] raw_data = f . value attrs = f . attrs # Raw data to np.array data = np . array ([ list ( d ) for d in raw_data ]) # Get varialbles labels indep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Independent' ) and str ( x ) . endswith ( 'label' )]) dep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Dependent' ) and str ( x ) . endswith ( 'label' )]) indep_labels = [ attrs [ c ] for c in indep_keys ] dep_labels = [ attrs [ c ] for c in dep_keys ] variables = indep_labels + dep_labels return data , variables","title":"labrad_hdf5_dataloader"},{"location":"labrad/#labrad_hdf5_get_parameters","text":"labrad hdf5\u304b\u3089DV.add_parameters()\u3067\u52a0\u3048\u305f\u6a5f\u5668\u306e\u8a2d\u5b9a\u306e\u60c5\u5831\u306a\u3069\u3092\u53d6\u5f97\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_get_parameters ( dir_path , file_num , file_name ): \"\"\"Get parameter settings (e.g., ferquency, time constant added by DV.add_parameters()) from a labrad hdf5 file Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '00033 - measurement_name.hdf5' file_name : string Returns ------- dictionary Pairs of paramter keys and values Notes ----- The default parameter values are encoded by labrad format. The prefix in endoded values is 'data:application/labrad;base64,' To decode these and get the raw value, we need to simply use DV.get_parameters() or change the backend script in datavault/backend.py This function works in the latter case. \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] attrs = f . attrs # Get parameters labels and values param_ukeys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Param' )]) param_keys = [ c [ 6 :] for c in param_ukeys ] param_values = [ attrs [ c ] for c in param_ukeys ] return { k : v for k , v in zip ( param_keys , param_values )} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_parameters_of_func ( offset = None ): \"\"\"Get a dictionary of paramteres of the function. Parameters ---------- offset : int default value is None Return ------ dictionary The dictionary includes pairs of paremeter's name and the corresponding values. References ---------- [1] https://tottoto.net/python3-get-args-of-current-function/ \"\"\" parent_frame = inspect . currentframe () . f_back info = inspect . getargvalues ( parent_frame ) return { key : info . locals [ key ] for key in info . args [ offset :]} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def create_labrad_hdf5file ( DV , file_path , scan_name , scan_var , meas_var ): \"\"\"Create a labrad hdf5 file from ndarray. Parameters ---------- DV : object file_path : string scan_name : string scan_var : list or tuple meas_var : list or tuple Returns ------- int The file number \"\"\" DV . cd ( '' ) try : DV . mkdir ( file_path ) DV . cd ( file_path ) except Exception : DV . cd ( file_path ) file_name = file_path + '_' + scan_name dv_file = DV . new ( file_name , scan_var , meas_var ) print ' \\r ' , \"new file created, file numer: \" , int ( dv_file [ 1 ][ 0 : 5 ]) return int ( dv_file [ 1 ][ 0 : 5 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def write_meas_parameters ( DV , file_path , file_number , date , scan_name , meas_parameters , amplitude , sensitivity ): \"\"\"Write measurement parameters to txt file and labrad hdf5 file. Parameters ---------- DV : object file_path : string file_number : int date : object scan_name : string meas_parameters : dict scan_var : list or tuple meas_var : list or tuple amplitude : float sensitivity : float Returns ------- None \"\"\" if not os . path . isfile ( meas_details_path + file_path + '.txt' ): with open ( meas_details_path + file_path + '.txt' , \"w+\" ) as f : pass with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"========\" + \" \\n \" ) f . write ( \"file_number: \" + str ( file_number ) + \" \\n \" + \"date: \" + str ( date ) + \" \\n \" + \"measurement:\" + str ( scan_name ) + \" \\n \" ) for k , v in sorted ( meas_parameters . items ()): print ( k , v ) f . write ( str ( k ) + \": \" + str ( v ) + \" \\n \" ) DV . add_parameter ( str ( k ), str ( v )) for i , LA in enumerate ( LAs ): tc = LA . time_constant () sens = LA . sensitivity () f . write ( \"time_constant_\" + str ( i ) + ' : ' + str ( tc ) + \" \\n \" ) f . write ( \"sensitivity_\" + str ( i ) + ' : ' + str ( sens ) + \" \\n \" ) DV . add_parameter ( \"time_constant_\" + str ( i ), tc ) DV . add_parameter ( \"sensitivity_\" + str ( i ), sens ) def write_meas_parameters_end ( date1 , date2 , file_path ): with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"end date: \" + str ( date2 ) + \" \\n \" + \"total time: \" + str ( date2 - date1 ) + \" \\n \" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def get_variables ( DV ): \"\"\"Get variables of a lablad hdf5 file Parameters ---------- DV : object (datavault) Return ------ list A variable of the a lablad hdf5 file \"\"\" variables = [ DV . variables ()[ 0 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 0 ]))] + [ DV . variables ()[ 1 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 1 ]))] return variables","title":"labrad_hdf5_get_parameters"},{"location":"linux_command/","text":"Linux \u30b3\u30de\u30f3\u30c9 \u00b6 \u30b7\u30a7\u30eb\u306f\u30ab\u30fc\u30cd\u30eb\u306b\u547d\u4ee4\u3092\u51fa\u3057\u3066\u30ab\u30fc\u30cd\u30eb\u304b\u3089\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u305f\u3081\u306e\u3082\u306e bash \u306f\u30b7\u30a7\u30eb\u306e\uff11\u3064\u3000echo $SHELL\u3067\u78ba\u8a8d sh\u306e1\u3064 \u30bf\u30fc\u30df\u30ca\u30eb\uff1a\u5165\u51fa\u529b\u3000\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\uff08Linux\u306e\u5834\u5408\u3067\u306f\u5165\u51fa\u529b\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u6307\u3059\u3002\u30b3\u30de\u30f3\u30c9\u3092\u3046\u3051\u3068\u3063\u305f\u308a\u51fa\u529b\uff09 \u30d7\u30ed\u30f3\u30d7\u30c8\uff1a\u30ab\u30fc\u30bd\u30eb\u306e\u5de6\u5074[ ^^^^@ ~]\u3000\u30b3\u30de\u30f3\u30c9\u5165\u529b\u3092\u4fc3\u3059 \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3: \u30d7\u30ed\u30f3\u30d7\u30c8\u306e\u53f3\u5074 \u57fa\u672c\u64cd\u4f5c ctl + a: atama\u306b\u79fb\u52d5 ctrl + e: end\u306b\u79fb\u52d5 ctrl + w:\u3000 word: \u5358\u8a9e\u5358\u4f4d\u3067\u524a\u9664 \u30ab\u30c3\u30c8\u3000\u30a2\u30f3\u30c9\u3000\u30e4\u30f3\u30af ctrl + u \u884c\u982d\u307e\u3067\u30ab\u30c3\u30c8 ctrl + k:\u884c\u672b\u307e\u3067\u30ab\u30c3\u30c8 ctrl + y (yank) \u30bf\u30d6\u3067\u30aa\u30fc\u30c8\u30b3\u30f3\u30d7\u30ea\u30fc\u30c8 ls ls -a echo \u30b3\u30de\u30f3\u30c9 cat/less space(\u4e00\u753b\u9762\u4e0b)\u3000b\uff08\u4e00\u753b\u9762\u4e0a\uff09 j\uff08\u4e00\u884c\u305a\u3064\u4e0b\uff09 k\uff08\u4e00\u884c\u305a\u3064\u4e0a\uff09 q \u306f\u3082\u3068\u306e\u753b\u9762\u306b\u623b\u308b\uff08quit\uff09 wget \u30b3\u30de\u30f3\u30c9 unzip touch \u30b3\u30de\u30f3\u30c9 rm \u3068rm -r echo \\(PATH export PATH = /path/to/something:\\) PATH\u3092\u8ffd\u52a0\u3059\u308b\u3002 cp /etc/crontab file2 cp file1 directory\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30b3\u30d4\u30fc\u53ef\u80fd \u6307\u5b9a\u3057\u305f\u30b3\u30d4\u30fc\u5148\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4e2d\u306b\u306a\u308b \u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u65b0\u3057\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308b\u3002 cp -r dir1 dir2\u3067\u518d\u5e30\u7684\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b3\u30d4\u30fc\u53ef\u80fd mv\u30b3\u30de\u30f3\u30c9\u3067\u540d\u524d\u3092\u5909\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3057\u3001\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5\u3067\u304d\u308b\u3002 sh -x \u3067\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u78ba\u8a8d docker \u30d5\u30a1\u30a4\u30eb\u306e\u3068\u304d\u306f-b -p\u3092\u3064\u3051\u308b\u3002 \u30cf\u30fc\u30c9\u30ea\u30f3\u30af \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af ln file1 file2 (file1\u306bfile2\u3068\u3044\u3046\u30cf\u30fc\u30c9\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b\u3002file1\u3092\u4f5c\u6210\u3057\u3066\u3082file2\u304c\u6b8b\u308b) ln -s file1 file2\u3067\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b mkdir -p dir1/dir2/dir3/target touch p dir1/dir2/dir3/target/file ln -s dir1/dir2/dir3/target/ target \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(/)\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(~)\u306e\u3061\u3083\u3093\u3068\u3057\u305f\u7406\u89e3 - Qiita \u3082\u306e\u3059\u3054\u3044\u7d30\u304b\u3044\u3053\u3068\u3060\u3051\u3069\u3001\u30d1\u30b9\u306e\u6307\u5b9a\u65b9\u6cd5\u3067\u306e ~ \uff08\u30c1\u30eb\u30c0\uff09\u3068 / \uff08\u30b9\u30e9\u30c3\u30b7\u30e5\uff09\u306e\u7406\u89e3\u304c\u66d6\u6627\u3067\u6c17\u6301\u3061\u60aa\u3044\u601d\u3044\u3092\u3057\u305f\u306e\u3067\u30e1\u30e2\u3002 / : \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~ \uff1a\u4eca\u306e\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~taro : taro\u3068\u3044\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u30b9\u30e9\u30c3\u30b7\u30e5\u306e\u610f\u5473\u5408\u3044 \u00b6 \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e / \u3068\u3001\u5404\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u524d\u306b\u3064\u304f / \u306f\u610f\u5473\u5408\u3044\u304c\u9055\u3063\u3066\u3044\u308b\u6a21\u69d8\u3002 \u524d\u8005\uff1a\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u305d\u306e\u3082\u306e \u5f8c\u8005\uff1a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u533a\u5207\u308b\u3082\u306e \u306a\u306e\u3067\u3001\u4e00\u898b\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u305b\u3044\u3067\u300c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u306f\u672b\u5c3e\u306b\u30b9\u30e9\u30c3\u30b7\u30e5\u304c\u4ed8\u3044\u3066\u3044\u308b\u3082\u306e\u300d\u3068\u3044\u3046\u52d8\u9055\u3044\u3092\uff08\u5c11\u306a\u304f\u3082\u7b46\u8005\u306f\uff09\u3057\u3061\u3083\u3046\u304c\u3001 hogehoge/ \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3067\u306f\u306a\u304f hogehoge \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3060\u3002\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ~/ \u3060\u3068\u601d\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u4eba\u306f\u591a\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f history !393\u3067\u4f7f\u3048\u308b \u30d4\u30ea\u30aa\u30c9\u3067\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea find . -name '*.txt' -print \u3053\u306e\u30a2\u30b9\u30bf\u30ea\u30b9\u30af\u306f\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3067\u30d1\u30b9\u540d\u5c55\u958b\u3068\u306f\u9055\u3046\u3002\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c6\u30b7\u30e7\u30f3\u304b\u3069\u3046\u304b find . -type d \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u691c\u7d22 find . -type d -a -name share locate \u30b3\u30de\u30f3\u30c9\u306ffind\u30b3\u30de\u30f3\u30c9\u3088\u308a\u3082\u9ad8\u901f\uff08\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u691c\u7d22\uff09 sudo updatedb\u3092\u3057\u3066\u304b\u3089 locate bash -A doc \u3000and \u691c\u7d22 locate bash doc grep bin /etc/crontab \u30d5\u30a3\u30eb\u30bf history | head wc:\u6587\u5b57\u6570\u3092\u6570\u3048\u308b wc -l ls / | wc -l \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u500b\u6570\u884c\u6570 \u30bd\u30fc\u30c8\u30b3\u30de\u30f3\u30c9 sort word.txt sort -r word.txt sort -n number.txt \u91cd\u8907\u3092\u53d6\u308a\u51fa\u3059 uniq number.txt sort -n number.txt | uniq sort -n number.txt | uniq -c | sort -nr | head -n 3 \u30d5\u30a1\u30a4\u30eb\u3092\u76e3\u8996\u3059\u308b tail -f log.txt \u30e1\u30e2\u30ea\u304b\u3089\u898b\u305f\u5b9f\u884c\u72b6\u614b\u306b\u3042\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30d7\u30ed\u30bb\u30b9\u3068\u3044\u3046 \u30b8\u30e7\u30d6\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306b\u5165\u529b\u3055\u308c\u305f\u884c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3068\u304d\u306f\u8907\u6570\u306b\u306a\u308b\u3002\uff09 ps\u30b3\u30de\u30f3\u30c9 ps -x ps -u sleep\u30b3\u30de\u30f3\u30c9 jobs\u30b3\u30de\u30f3\u30c9 fg\u30b3\u30de\u30f3\u30c9 bg\u30b3\u30de\u30f3\u30c9","title":"Linux"},{"location":"linux_command/#linux","text":"\u30b7\u30a7\u30eb\u306f\u30ab\u30fc\u30cd\u30eb\u306b\u547d\u4ee4\u3092\u51fa\u3057\u3066\u30ab\u30fc\u30cd\u30eb\u304b\u3089\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u305f\u3081\u306e\u3082\u306e bash \u306f\u30b7\u30a7\u30eb\u306e\uff11\u3064\u3000echo $SHELL\u3067\u78ba\u8a8d sh\u306e1\u3064 \u30bf\u30fc\u30df\u30ca\u30eb\uff1a\u5165\u51fa\u529b\u3000\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\uff08Linux\u306e\u5834\u5408\u3067\u306f\u5165\u51fa\u529b\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u6307\u3059\u3002\u30b3\u30de\u30f3\u30c9\u3092\u3046\u3051\u3068\u3063\u305f\u308a\u51fa\u529b\uff09 \u30d7\u30ed\u30f3\u30d7\u30c8\uff1a\u30ab\u30fc\u30bd\u30eb\u306e\u5de6\u5074[ ^^^^@ ~]\u3000\u30b3\u30de\u30f3\u30c9\u5165\u529b\u3092\u4fc3\u3059 \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3: \u30d7\u30ed\u30f3\u30d7\u30c8\u306e\u53f3\u5074 \u57fa\u672c\u64cd\u4f5c ctl + a: atama\u306b\u79fb\u52d5 ctrl + e: end\u306b\u79fb\u52d5 ctrl + w:\u3000 word: \u5358\u8a9e\u5358\u4f4d\u3067\u524a\u9664 \u30ab\u30c3\u30c8\u3000\u30a2\u30f3\u30c9\u3000\u30e4\u30f3\u30af ctrl + u \u884c\u982d\u307e\u3067\u30ab\u30c3\u30c8 ctrl + k:\u884c\u672b\u307e\u3067\u30ab\u30c3\u30c8 ctrl + y (yank) \u30bf\u30d6\u3067\u30aa\u30fc\u30c8\u30b3\u30f3\u30d7\u30ea\u30fc\u30c8 ls ls -a echo \u30b3\u30de\u30f3\u30c9 cat/less space(\u4e00\u753b\u9762\u4e0b)\u3000b\uff08\u4e00\u753b\u9762\u4e0a\uff09 j\uff08\u4e00\u884c\u305a\u3064\u4e0b\uff09 k\uff08\u4e00\u884c\u305a\u3064\u4e0a\uff09 q \u306f\u3082\u3068\u306e\u753b\u9762\u306b\u623b\u308b\uff08quit\uff09 wget \u30b3\u30de\u30f3\u30c9 unzip touch \u30b3\u30de\u30f3\u30c9 rm \u3068rm -r echo \\(PATH export PATH = /path/to/something:\\) PATH\u3092\u8ffd\u52a0\u3059\u308b\u3002 cp /etc/crontab file2 cp file1 directory\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30b3\u30d4\u30fc\u53ef\u80fd \u6307\u5b9a\u3057\u305f\u30b3\u30d4\u30fc\u5148\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4e2d\u306b\u306a\u308b \u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u65b0\u3057\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308b\u3002 cp -r dir1 dir2\u3067\u518d\u5e30\u7684\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b3\u30d4\u30fc\u53ef\u80fd mv\u30b3\u30de\u30f3\u30c9\u3067\u540d\u524d\u3092\u5909\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3057\u3001\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5\u3067\u304d\u308b\u3002 sh -x \u3067\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u78ba\u8a8d docker \u30d5\u30a1\u30a4\u30eb\u306e\u3068\u304d\u306f-b -p\u3092\u3064\u3051\u308b\u3002 \u30cf\u30fc\u30c9\u30ea\u30f3\u30af \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af ln file1 file2 (file1\u306bfile2\u3068\u3044\u3046\u30cf\u30fc\u30c9\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b\u3002file1\u3092\u4f5c\u6210\u3057\u3066\u3082file2\u304c\u6b8b\u308b) ln -s file1 file2\u3067\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b mkdir -p dir1/dir2/dir3/target touch p dir1/dir2/dir3/target/file ln -s dir1/dir2/dir3/target/ target \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(/)\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(~)\u306e\u3061\u3083\u3093\u3068\u3057\u305f\u7406\u89e3 - Qiita \u3082\u306e\u3059\u3054\u3044\u7d30\u304b\u3044\u3053\u3068\u3060\u3051\u3069\u3001\u30d1\u30b9\u306e\u6307\u5b9a\u65b9\u6cd5\u3067\u306e ~ \uff08\u30c1\u30eb\u30c0\uff09\u3068 / \uff08\u30b9\u30e9\u30c3\u30b7\u30e5\uff09\u306e\u7406\u89e3\u304c\u66d6\u6627\u3067\u6c17\u6301\u3061\u60aa\u3044\u601d\u3044\u3092\u3057\u305f\u306e\u3067\u30e1\u30e2\u3002 / : \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~ \uff1a\u4eca\u306e\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~taro : taro\u3068\u3044\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea","title":"Linux \u30b3\u30de\u30f3\u30c9"},{"location":"linux_command/#_1","text":"\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e / \u3068\u3001\u5404\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u524d\u306b\u3064\u304f / \u306f\u610f\u5473\u5408\u3044\u304c\u9055\u3063\u3066\u3044\u308b\u6a21\u69d8\u3002 \u524d\u8005\uff1a\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u305d\u306e\u3082\u306e \u5f8c\u8005\uff1a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u533a\u5207\u308b\u3082\u306e \u306a\u306e\u3067\u3001\u4e00\u898b\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u305b\u3044\u3067\u300c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u306f\u672b\u5c3e\u306b\u30b9\u30e9\u30c3\u30b7\u30e5\u304c\u4ed8\u3044\u3066\u3044\u308b\u3082\u306e\u300d\u3068\u3044\u3046\u52d8\u9055\u3044\u3092\uff08\u5c11\u306a\u304f\u3082\u7b46\u8005\u306f\uff09\u3057\u3061\u3083\u3046\u304c\u3001 hogehoge/ \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3067\u306f\u306a\u304f hogehoge \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3060\u3002\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ~/ \u3060\u3068\u601d\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u4eba\u306f\u591a\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f history !393\u3067\u4f7f\u3048\u308b \u30d4\u30ea\u30aa\u30c9\u3067\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea find . -name '*.txt' -print \u3053\u306e\u30a2\u30b9\u30bf\u30ea\u30b9\u30af\u306f\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3067\u30d1\u30b9\u540d\u5c55\u958b\u3068\u306f\u9055\u3046\u3002\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c6\u30b7\u30e7\u30f3\u304b\u3069\u3046\u304b find . -type d \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u691c\u7d22 find . -type d -a -name share locate \u30b3\u30de\u30f3\u30c9\u306ffind\u30b3\u30de\u30f3\u30c9\u3088\u308a\u3082\u9ad8\u901f\uff08\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u691c\u7d22\uff09 sudo updatedb\u3092\u3057\u3066\u304b\u3089 locate bash -A doc \u3000and \u691c\u7d22 locate bash doc grep bin /etc/crontab \u30d5\u30a3\u30eb\u30bf history | head wc:\u6587\u5b57\u6570\u3092\u6570\u3048\u308b wc -l ls / | wc -l \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u500b\u6570\u884c\u6570 \u30bd\u30fc\u30c8\u30b3\u30de\u30f3\u30c9 sort word.txt sort -r word.txt sort -n number.txt \u91cd\u8907\u3092\u53d6\u308a\u51fa\u3059 uniq number.txt sort -n number.txt | uniq sort -n number.txt | uniq -c | sort -nr | head -n 3 \u30d5\u30a1\u30a4\u30eb\u3092\u76e3\u8996\u3059\u308b tail -f log.txt \u30e1\u30e2\u30ea\u304b\u3089\u898b\u305f\u5b9f\u884c\u72b6\u614b\u306b\u3042\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30d7\u30ed\u30bb\u30b9\u3068\u3044\u3046 \u30b8\u30e7\u30d6\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306b\u5165\u529b\u3055\u308c\u305f\u884c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3068\u304d\u306f\u8907\u6570\u306b\u306a\u308b\u3002\uff09 ps\u30b3\u30de\u30f3\u30c9 ps -x ps -u sleep\u30b3\u30de\u30f3\u30c9 jobs\u30b3\u30de\u30f3\u30c9 fg\u30b3\u30de\u30f3\u30c9 bg\u30b3\u30de\u30f3\u30c9","title":"\u30b9\u30e9\u30c3\u30b7\u30e5\u306e\u610f\u5473\u5408\u3044"},{"location":"python_basis/","text":"\u57fa\u790e \u00b6 id():\u5909\u6570\u306e\u5834\u6240\u306eid\u3092\u8fd4\u3059 dir:attribute\u3092\u8fd4\u3059 is\u6f14\u7b97\u5b50\uff1a\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304b\u3069\u3046\u304b\u3092\u5224\u65ad\u3059\u308b \u6587\u5b57\u5217\u306b\u304a\u3051\u308bf\u69cb\u6587 input\u95a2\u6570 join, split dictionry \u306eget() \u578b\u5909\u63db\uff08casting\uff09 \u30a4\u30df\u30e5\u30fc\u30bf\u30d6\u30eb\u3067\u306f\u95a2\u6570\u306e\u4e2d\u3067\u65b0\u3057\u3044\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09ID\u304c\u4f5c\u3089\u308c\u308b\u3002 \uff08for\u6587\u3067\u306f\u8db3\u3057\u3066\u3044\u304f\u3068\u304d\u306f\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\u307b\u3046\u304c\u826f\u3044\u3002\uff09 \u30df\u30e5\u30fc\u30bf\u30d6\u30eb\u3067\u306f\u4e0a\u66f8\u304d\u3055\u308c\u308b Type annotation\uff1a\u306f\u3001\u3042\u307e\u308a\u3064\u3051\u306a\u3044(\u52d5\u7684\u578b\u4ed8\u3051\u306e\u601d\u60f3\u3068\u98df\u3044\u9055\u3063\u3066\u3044\u308b\u304b\u3089) add_nums(num1 : int, num2 : int) -> int: _\u306f\u76f4\u524d\u306e\u5b9f\u884c\u3057\u305f\u623b\u308a\u5024\u3092\u683c\u7d0d\u3059\u308b\u3002 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30bf\u30a4\u30d7\u3092isinstance\u3067\u78ba\u8a8d \u30d6\u30fc\u30ea\u30a2\u30f3\u306b\u6bd4\u8f03\u6f14\u7b97\u5b50\u3092\u4f7f\u308f\u306a\u3044\u3002 _\u306f\u76f4\u524d\u306e\u5b9f\u884c\u3057\u305f\u623b\u308a\u5024\u3092\u683c\u7d0d\u3059\u308b\u3002 return None\u3092\u66f8\u304f dir(object)\u3067attribute\u3092\u8868\u793a\u3057\u3066\u304f\u308c\u308b\u3002 dir()\u3060\u3068\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3082\u306e\u3092\u304c\u51fa\u308b\u3002 buidins__\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002 \u30a2\u30f3\u30c0\u30fc\u30b9\u30b3\u30a2\u3067\u56f2\u307e\u308c\u305f\u3082\u306e\u306f\u30de\u30b8\u30c3\u30af\u30e1\u30bd\u30c3\u30c9\u3002 __name__\u306f\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u3068\u304d\u306b\u30e2\u30b8\u30e5\u30fc\u30eb\u540d\u304c\u5165\u308b\u3002\u305d\u306e\u3082\u306e\u3092\u52d5\u304b\u3059\u3068main\u304c\u5165\u308b\u3002 __init .py\u3092\u4f5c\u308b .\u3068\u304b..\u3067\u76f8\u5bfe\u30a4\u30f3\u30dd\u30fc\u30c8\u3067\u304d\u308b\u3002 Python\u306f\u52d5\u7684\u578b\u4ed8\u3051\u8a00\u8a9e\u3001\u578b\u3088\u308a\u3082\u632f\u308b\u821e\u3044\u306b\u8208\u5473\u304c\u3042\u308b\u3002 \u30c0\u30c3\u30af\u30bf\u30a4\u30d4\u30f3\u30b0 py -m pip .py \u3068pip3 docstring \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30b9\u30bf\u30a4\u30eb \u00b6 pep8 \u00b6 =\u3068\u30aa\u30da\u30ec\u30fc\u30bf\u30fc\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9 \u95a2\u6570\u306e\u5f15\u6570\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9\u306f\u4e0d\u8981 \u30d7\u30e9\u30a4\u30aa\u30ea\u30c6\u30a3\u304c\u3042\u308b\u5834\u5408\u306f\u30b9\u30da\u30fc\u30b9\u3092\u7121\u304f\u3059 \u30ab\u30f3\u30de\u306e\u3042\u3068\u306b\u30b9\u30da\u30fc\u30b9\u3092\u5165\u308c\u308b\u3002 \u6700\u5f8c\u306e\u8981\u7d20\u306b\u30ab\u30f3\u30de\u3082\u3064\u3051\u308b\uff08\u62ec\u5f27\u9589\u3058\u3092\u6b21\u306e\u884c\u306b\u3059\u308b\u3002\uff09 \u95a2\u6570\u306e\u5f15\u6570\u306e\u982d\u3092\u63c3\u3048\u3066\u6539\u884c\u3059\u308b\u3002 \u95a2\u6570\u9593\u306f\u4e8c\u884c\u3042\u3051\u308b\u3002 \u30af\u30e9\u30b9\u306e\u30e1\u30bd\u30c3\u30c9\u9593\u306f1\u884c import \u306e\u9806\u756a standrd library third party our library local library Linter\u3068formatter \u00b6 linter: pycodestyle, pyflakes,flake8(\u4e21\u65b9), pylint(\u3088\u308a\u53b3\u3057\u3044) formetter, yapf, black \u2022 black: https://github.com/psf/black \u2022 flake8: https://github.com/PyCQA/flake8 \u2022 isort: https://github.com/PyCQA/isort \u95a2\u6570 \u00b6 Python\u3067\u306f \u5168\u3066\u53c2\u7167\u6e21\u3057 \u3002 constant variable \u5927\u6587\u5b57\u3067\u66f8\u304f \u30e2\u30b8\u30e5\u30fc\u30eb \uff08\u30d5\u30a1\u30a4\u30eb\u5358\u4f4d\u3067\u5206\u3051\u3066\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8f09\u3057\u305f\u3082\u306e\uff09\uff1c \u30d1\u30c3\u30b1\u30fc\u30b8 \uff08\u8907\u6570\u306e\u30d5\u30a1\u30a4\u30eb\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u304b\u3089\u306a\u308a\u3001\u30eb\u30fc\u30eb\u306b\u5f93\u3063\u3066\u305d\u308c\u3089\u3092\u3072\u3068\u56fa\u307e\u308a\u306b\u3057\u305f\u3082\u306e\uff09\uff1c \u30e9\u30a4\u30d6\u30e9\u30ea \u3002 \u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u968e\u5c64\u5316\u3055\u305b\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u8aad\u307f\u8fbc\u307e\u305b\u308b\u305f\u3081\u306b init .py \u304c\u5fc5\u8981\uff08\u3053\u306e\u3068\u304d\u76f8\u5bfeimport\u3082\u884c\u3046\uff09 \u5f15\u6570\uff08arguments\uff09 \uff1a\u5b9f\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024 \u30d1\u30e9\u30e1\u30fc\u30bf\uff08parameters\uff09 \uff1a\u4eee\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024\u306e\u30d7\u30ec\u30fc\u30b9\u30db\u30eb\u30c0\u3002 https://qiita.com/raviqqe/items/ee2bcb6bef86502f8cc6#%E5%BC%95%E6%95%B0%E3%81%AF-2-x-2--4-%E7%A8%AE%E9%A1%9E positional paremeters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\uff08arguments\uff09\u306a\u3057\u3002 keyword parameters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\u3042\u308a\u3002 \u53ef\u5909\u9577\u5f15\u6570 : args, *kwargs, \u69d8\u3005\u306a\u9577\u3055\u306e\u5f15\u6570\u3092\u53d7\u3051\u53d6\u308c\u308b\u3002 global \u3068 nonlocal \uff08nested\u95a2\u6570\u306e\u3068\u304d\u306b\u5b9a\u7fa9\uff09 \u30e9\u30e0\u30c0\u95a2\u6570 \uff08\u95a2\u6570\u540d\u304c\u7121\u3044\u95a2\u6570\uff09:\u95a2\u6570\u540d\u3068\"return\"\u3092\u7121\u304f\u3059\u3002filter \u95a2\u6570\u306e\u969b\u306b\u4f7f\u3046\u3002 \u95a2\u6570\u3082\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3001\u95a2\u6570\u3092\u5f15\u6570\u3067\u3068\u308c\u308b\u3001\u95a2\u6570\u3082return\u3067\u304d\u308b\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u3066\u8fd4\u3059\u3002\uff09Closure:\u72b6\u614b\u3092\u30ad\u30fc\u30d7\u3057\u305f\u95a2\u6570\u3002\uff08\u72b6\u614b\u3092\u52d5\u7684\u30fb\u9759\u7684\uff09 sys.path :\u306b\u5165\u308c\u308b\u3068\u30ab\u30b9\u30bf\u30e0\u3067\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3048\u308b\u3002pip\u3092\u4f7f\u3046\u3068site-packages\u306e\u4e2d\u3067\u7ba1\u7406\u3055\u308c\u308b\u3002 \u6b63\u898f\u8868\u73fe \u00b6 re.search('[0-9]', string) re.search('^[0-9]', string):\u6700\u521d\u306e\u6587\u5b57 re.search('^[0-9]{4}', string):\u6700\u521d\u306e\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}', string):\u6700\u521d\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}$', string):\u6700\u5f8c\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('a*b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('a+b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30921\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('ab?c', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u304b1\u56de\u7e70\u308a\u8fd4\u3059 abc|012 or te(s|x)t \u30b0\u30eb\u30fc\u30d7 'h.t'\u4efb\u610f\u306e\u4e00\u6587\u5b57 \u30a8\u30b9\u30b1\u30fc\u30d7'h.t' \\w [a-zA-Z0-9_]\u306b\u30de\u30c3\u30c1 \u30af\u30e9\u30b9 \u00b6 \u5c5e\u6027\u3068\u30e1\u30bd\u30c3\u30c9\uff1a \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3068\u30af\u30e9\u30b9\u5909\u6570 Python\u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea\uff1a https://docs.python.org/ja/3/library/functions.html https://qiita.com/ichi_taro3/items/cd71a8e43040abb446a1 * dir() * id() * \u6163\u7fd2\u7684\u306a\u547d\u540d\u898f\u5247\u3068\u3057\u3066\u306e\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8(non public)\u5316\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc\uff09 * \u30cd\u30fc\u30e0\u30de\u30f3\u30b0\u30ea\u30f3\u30b0\uff08\u96e3\u53f7\u5316\uff09\u8981\u7d20\u540d\u306e\u524d\u306b\"__\"\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc2\u3064\uff09\u3092\u3064\u3051\u307e\u3059\u3002 \u30c7\u30b3\u30ec\u30fc\u30bf \u00b6 https://qiita.com/koshigoe/items/848ddc0272b3cee92134 * staticmethod * classmrthod * property * @staticmethod\uff1a\u307b\u3068\u3093\u3069\u30af\u30e9\u30b9\u5916\u306e\u95a2\u6570\u3068\u3057\u3066\u6271\u3046\u3002\uff08self\u306f\u3044\u3089\u306a\u3044\uff09 @classmethod, cls\u306b\u5f15\u6570\u3092\u3068\u3063\u3066\u3001class\u306e\u60c5\u5831\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002 \u540d\u524d\u4fee\u98fe\uff1a__nameto\u3059\u308b\u3068\u3001_class__name getter\u3068setter\u3092\u3064\u3051\u308b\u3068\u304d\u306f\u305d\u306e\u5909\u6570\u3092non public\u306b\u3059\u308b\uff1d\uff1e\u30d7\u30ed\u30d1\u30c6\u30a3\u3092\u4f7f\u3046\u3002@property (\u5fc5\u305a_\u3092\u3064\u3051\u308b\uff09 \u30de\u30b8\u30c3\u30af\u30e1\u30bd\u30c3\u30c9\uff08\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9\uff09 \u00b6 http://ichitcltk.hustle.ne.jp/gudon2/index.php?pageType=file&id=python_class_special_attribute.md http://g6no3.wp.xdomain.jp/2019/10/03/python%E3%82%AF%E3%83%A9%E3%82%B9%E3%81%AE%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/ https://python.civic-apps.com/__xxx__/ https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee http://diveintopython3-ja.rdy.jp/special-method-names.html init call len getitem next iter str repr name :\u30e2\u30b8\u30e5\u30fc\u30eb\u5185\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u304a\u3044\u3066\u306f\u30e2\u30b8\u30e5\u30fc\u30eb\u540d\u3002\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u304a\u3044\u3066\u306f\u95a2\u6570\u540d\u306b\u306a\u308b\u3002 doc \u5c5e\u6027\u306e\u8ffd\u52a0\u3092\u7981\u6b62\u3059\u308b - slots__\u5c5e\u6027 \u30c7\u30b9\u30c8\u30e9\u30af\u30bf__del__\u30e1\u30bd\u30c3\u30c9 \u3068 with \u6587 def __contains (self,x) \u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u30002 in Odd()\u306e\u3088\u3046\u306bin\u30e1\u30bd\u30c3\u30c9\u306b\u5bfe\u3059\u308b\u771f\u507d\u5024\u3092\u8fd4\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 - \u5c5e\u6027\u3068\u30e1\u30bd\u30c3\u30c9 __init__\u306f\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3068\u547c\u3076 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3068\u30af\u30e9\u30b9\u5909\u6570 \u5916\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3057\u306a\u3044\u95a2\u6570\u306b\u306f_\u3092\u5148\u982d\u306b\u3064\u3051\u308b\u3002 \u7d99\u627f\u6642\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30c8\u3092super(). init ()\u3092\u3064\u3051\u308b\u3002 \u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\uff1a\u30b5\u30d6\u30af\u30e9\u30b9\u306e\u3067\u540c\u3058\u540d\u524d\u306e\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3002 super.func()\u3067\u89aa\u95a2\u6570\u3092\u547c\u3079\u308b\u3002 \u7d99\u627f\u3059\u308b\u3068\u304d\u306fstaticmethod\u3067\u306f\u547c\u3076\u3068\u304d\u306b\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3002classmethod\u3092\u4f7f\u3046\u3002 \u7d99\u627f\u6642\u306e\u540d\u524d\u4fee\u98fe\u306f__\u3092\u4f7f\u3044\u3053\u306a\u3059\u3002 \u30dd\u30ea\u30e2\u30fc\u30d5\u30a3\u30ba\u30e0\u306f\u7d99\u627f\u3092\u3057\u3066\u3044\u308bint\u3082str\u3082print\u3092\u3059\u308b\u3068\u540c\u3058\u3088\u3046\u306b\u632f\u308b\u821e\u3046 generetor \u5f0f \u00b6 https://www.atmarkit.co.jp/ait/articles/1908/20/news024.html https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee generator \u95a2\u6570\u3067\u5b9a\u7fa9\u3059\u308b range(10)\uff08iterator\u3067\u3082\u3042\u308b\u3002\uff09 iteration\u6642\u306b\u8a08\u7b97\u3057\u3066\u51fa\u529b\u3057\u3066\u3044\u308b\u3002 yield\u304cgenerator\u306b\u5fc5\u8981\u3002 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u3063\u3066yield\u3092\u8fd4\u3059\u3002 next\u3067\u5024\u3092\u53d6\u5f97\u3067\u304d\u308b\u3002 yield\u3092\u4f7f\u308f\u306a\u3044\u3067\u5185\u5305\u8868\u8a18\u3067generator expression\u3092\u304b\u3051\u308b\u3002 \u30e1\u30e2\u30ea\u306e\u7bc0\u7d04\u306b\u306a\u308b\u3002 iteraor\u3068\u306fnext()\u3067\u56de\u3059\u3053\u3068\u304c\u3067\u304d\u308b iter()\u95a2\u6570\u3067iterator\u3092\u8fd4\u3059\u3082\u306e\u3092iterable\u3068\u3044\u3046\u3002 \u30ea\u30b9\u30c8\u306fiterator\u3067\u306f\u306a\u3044\u304c,ieteratable. iterable iteretaro\u306f\u3000\u30af\u30e9\u30b9\u3067\u5b9a\u7fa9\u3059\u308b\u3002 next__\u3068__iter \u30ab\u30c3\u30b3\u3067\u304f\u304f\u3063\u305f\u3082\u306e (len(x) for x in something) \u7a7a\u5024\u304b\u3069\u3046\u304b\u306f if somelist\u3067\u8a55\u4fa1 range\u3088\u308a\u306fenumerate try/except/else/finally None\u3092\u8fd4\u3059\u306e\u3067\u306f\u306a\u304f\u3001\u4f8b\u5916\u51e6\u7406\u3092\u3059\u308b\u3002 \u30a8\u30e9\u30fc \u00b6 Syntax Error Exception try: \u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 except ZeroDivisionError(\u7279\u5b9a\u306e\u30a8\u30e9\u30fc\u306e\u7a2e\u985e) as e: except ValueError(\u7279\u5b9a\u306e\u30a8\u30e9\u30fc\u306e\u7a2e\u985e) as e: else: \u4f8b\u5916\u304c\u8d77\u304d\u305f\u3068\u304d\u306f\u5b9f\u884c\u3057\u306a\u3044\u30b3\u30fc\u30c9 finally: \u5e38\u306b\u5b9f\u884c\u3059\u308b\u30b3\u30fc\u30c9 finally \u306f\u30ad\u30e3\u30c3\u30c1\u3055\u308c\u306a\u304f\u3066\u3082\u30a8\u30e9\u30fc\u306e\u524d\u306b\u5b9f\u884c\u3055\u308c\u308b raise \u306f\u30a8\u30e9\u30fc\u3092\u767a\u751f\u3055\u305b\u308b\u3002 \u4f8b\u5916\u306e\u81ea\u4f5c Exception \u30af\u30e9\u30b9\u3092\u7d99\u627f\u3059\u308b \u578b\u306e\u30c1\u30a7\u30c3\u30af isinstance\u3067\u30c1\u30a7\u30c3\u30af method = getattr(animal, 'walk', None) if callable(method) traceback.print_exc() tracebackmodudle \u30b3\u30e1\u30f3\u30c8\u3067# TODO \u30a4\u30c6\u30ec\u30fc\u30bf\u306f1\u3064\u305a\u3064\u53d6\u3063\u3066\u3053\u308c\u308b\u3002 \u6587\u7ae0\u3092\u66f8\u304f\u3068\u304d\u306fwith open\u3092\u4f7f\u3046\u3002 read\u3068readline, readlines\u3092\u4f7f\u3044\u5206\u3051\u308b\u3002 w, \u66f8\u304d\u8fbc\u307f\uff46.write mode = 'a', 'r+', w+, a+ f.seek(0) csv\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046\u3002 json\u30e2\u30b8\u30e5\u30fc\u30eb \u30c6\u30b9\u30c8 \u00b6 assert\u30b3\u30fc\u30c9\u3067\u30c6\u30b9\u30c8\u3057\u3066\u3044\u304f\u3002 \u30c6\u30b9\u30c8\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u304f\u3002 Test runnner: unittest, self.assertEqual(power(base, exp), 8) python -m unittest test.py\u3092\u4f7f\u3046 \u4f8b\u5916\u30b1\u30fc\u30b9\u306fwith \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u3063\u3066\u66f8\u304f\u3002 with self.assertRaise(Typeerror) pytest assert\u3067\u7c21\u6f54\u306b\u66f8\u3051\u308b\u3002 \u30c6\u30b9\u30c8\u30ab\u30d0\u30ec\u30c3\u30b8 pytest-cov:\u30ab\u30d0\u30fc\u7387\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002 html\u3084xml\u3067\u51fa\u529b\u3067\u304d\u308b\u3002--cov-append","title":"Basis"},{"location":"python_basis/#_1","text":"id():\u5909\u6570\u306e\u5834\u6240\u306eid\u3092\u8fd4\u3059 dir:attribute\u3092\u8fd4\u3059 is\u6f14\u7b97\u5b50\uff1a\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304b\u3069\u3046\u304b\u3092\u5224\u65ad\u3059\u308b \u6587\u5b57\u5217\u306b\u304a\u3051\u308bf\u69cb\u6587 input\u95a2\u6570 join, split dictionry \u306eget() \u578b\u5909\u63db\uff08casting\uff09 \u30a4\u30df\u30e5\u30fc\u30bf\u30d6\u30eb\u3067\u306f\u95a2\u6570\u306e\u4e2d\u3067\u65b0\u3057\u3044\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09ID\u304c\u4f5c\u3089\u308c\u308b\u3002 \uff08for\u6587\u3067\u306f\u8db3\u3057\u3066\u3044\u304f\u3068\u304d\u306f\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\u307b\u3046\u304c\u826f\u3044\u3002\uff09 \u30df\u30e5\u30fc\u30bf\u30d6\u30eb\u3067\u306f\u4e0a\u66f8\u304d\u3055\u308c\u308b Type annotation\uff1a\u306f\u3001\u3042\u307e\u308a\u3064\u3051\u306a\u3044(\u52d5\u7684\u578b\u4ed8\u3051\u306e\u601d\u60f3\u3068\u98df\u3044\u9055\u3063\u3066\u3044\u308b\u304b\u3089) add_nums(num1 : int, num2 : int) -> int: _\u306f\u76f4\u524d\u306e\u5b9f\u884c\u3057\u305f\u623b\u308a\u5024\u3092\u683c\u7d0d\u3059\u308b\u3002 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30bf\u30a4\u30d7\u3092isinstance\u3067\u78ba\u8a8d \u30d6\u30fc\u30ea\u30a2\u30f3\u306b\u6bd4\u8f03\u6f14\u7b97\u5b50\u3092\u4f7f\u308f\u306a\u3044\u3002 _\u306f\u76f4\u524d\u306e\u5b9f\u884c\u3057\u305f\u623b\u308a\u5024\u3092\u683c\u7d0d\u3059\u308b\u3002 return None\u3092\u66f8\u304f dir(object)\u3067attribute\u3092\u8868\u793a\u3057\u3066\u304f\u308c\u308b\u3002 dir()\u3060\u3068\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3082\u306e\u3092\u304c\u51fa\u308b\u3002 buidins__\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002 \u30a2\u30f3\u30c0\u30fc\u30b9\u30b3\u30a2\u3067\u56f2\u307e\u308c\u305f\u3082\u306e\u306f\u30de\u30b8\u30c3\u30af\u30e1\u30bd\u30c3\u30c9\u3002 __name__\u306f\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u3068\u304d\u306b\u30e2\u30b8\u30e5\u30fc\u30eb\u540d\u304c\u5165\u308b\u3002\u305d\u306e\u3082\u306e\u3092\u52d5\u304b\u3059\u3068main\u304c\u5165\u308b\u3002 __init .py\u3092\u4f5c\u308b .\u3068\u304b..\u3067\u76f8\u5bfe\u30a4\u30f3\u30dd\u30fc\u30c8\u3067\u304d\u308b\u3002 Python\u306f\u52d5\u7684\u578b\u4ed8\u3051\u8a00\u8a9e\u3001\u578b\u3088\u308a\u3082\u632f\u308b\u821e\u3044\u306b\u8208\u5473\u304c\u3042\u308b\u3002 \u30c0\u30c3\u30af\u30bf\u30a4\u30d4\u30f3\u30b0 py -m pip .py \u3068pip3 docstring","title":"\u57fa\u790e"},{"location":"python_basis/#_2","text":"","title":"\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30b9\u30bf\u30a4\u30eb"},{"location":"python_basis/#pep8","text":"=\u3068\u30aa\u30da\u30ec\u30fc\u30bf\u30fc\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9 \u95a2\u6570\u306e\u5f15\u6570\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9\u306f\u4e0d\u8981 \u30d7\u30e9\u30a4\u30aa\u30ea\u30c6\u30a3\u304c\u3042\u308b\u5834\u5408\u306f\u30b9\u30da\u30fc\u30b9\u3092\u7121\u304f\u3059 \u30ab\u30f3\u30de\u306e\u3042\u3068\u306b\u30b9\u30da\u30fc\u30b9\u3092\u5165\u308c\u308b\u3002 \u6700\u5f8c\u306e\u8981\u7d20\u306b\u30ab\u30f3\u30de\u3082\u3064\u3051\u308b\uff08\u62ec\u5f27\u9589\u3058\u3092\u6b21\u306e\u884c\u306b\u3059\u308b\u3002\uff09 \u95a2\u6570\u306e\u5f15\u6570\u306e\u982d\u3092\u63c3\u3048\u3066\u6539\u884c\u3059\u308b\u3002 \u95a2\u6570\u9593\u306f\u4e8c\u884c\u3042\u3051\u308b\u3002 \u30af\u30e9\u30b9\u306e\u30e1\u30bd\u30c3\u30c9\u9593\u306f1\u884c import \u306e\u9806\u756a standrd library third party our library local library","title":"pep8"},{"location":"python_basis/#linterformatter","text":"linter: pycodestyle, pyflakes,flake8(\u4e21\u65b9), pylint(\u3088\u308a\u53b3\u3057\u3044) formetter, yapf, black \u2022 black: https://github.com/psf/black \u2022 flake8: https://github.com/PyCQA/flake8 \u2022 isort: https://github.com/PyCQA/isort","title":"Linter\u3068formatter"},{"location":"python_basis/#_3","text":"Python\u3067\u306f \u5168\u3066\u53c2\u7167\u6e21\u3057 \u3002 constant variable \u5927\u6587\u5b57\u3067\u66f8\u304f \u30e2\u30b8\u30e5\u30fc\u30eb \uff08\u30d5\u30a1\u30a4\u30eb\u5358\u4f4d\u3067\u5206\u3051\u3066\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8f09\u3057\u305f\u3082\u306e\uff09\uff1c \u30d1\u30c3\u30b1\u30fc\u30b8 \uff08\u8907\u6570\u306e\u30d5\u30a1\u30a4\u30eb\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u304b\u3089\u306a\u308a\u3001\u30eb\u30fc\u30eb\u306b\u5f93\u3063\u3066\u305d\u308c\u3089\u3092\u3072\u3068\u56fa\u307e\u308a\u306b\u3057\u305f\u3082\u306e\uff09\uff1c \u30e9\u30a4\u30d6\u30e9\u30ea \u3002 \u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u968e\u5c64\u5316\u3055\u305b\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u8aad\u307f\u8fbc\u307e\u305b\u308b\u305f\u3081\u306b init .py \u304c\u5fc5\u8981\uff08\u3053\u306e\u3068\u304d\u76f8\u5bfeimport\u3082\u884c\u3046\uff09 \u5f15\u6570\uff08arguments\uff09 \uff1a\u5b9f\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024 \u30d1\u30e9\u30e1\u30fc\u30bf\uff08parameters\uff09 \uff1a\u4eee\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024\u306e\u30d7\u30ec\u30fc\u30b9\u30db\u30eb\u30c0\u3002 https://qiita.com/raviqqe/items/ee2bcb6bef86502f8cc6#%E5%BC%95%E6%95%B0%E3%81%AF-2-x-2--4-%E7%A8%AE%E9%A1%9E positional paremeters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\uff08arguments\uff09\u306a\u3057\u3002 keyword parameters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\u3042\u308a\u3002 \u53ef\u5909\u9577\u5f15\u6570 : args, *kwargs, \u69d8\u3005\u306a\u9577\u3055\u306e\u5f15\u6570\u3092\u53d7\u3051\u53d6\u308c\u308b\u3002 global \u3068 nonlocal \uff08nested\u95a2\u6570\u306e\u3068\u304d\u306b\u5b9a\u7fa9\uff09 \u30e9\u30e0\u30c0\u95a2\u6570 \uff08\u95a2\u6570\u540d\u304c\u7121\u3044\u95a2\u6570\uff09:\u95a2\u6570\u540d\u3068\"return\"\u3092\u7121\u304f\u3059\u3002filter \u95a2\u6570\u306e\u969b\u306b\u4f7f\u3046\u3002 \u95a2\u6570\u3082\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3001\u95a2\u6570\u3092\u5f15\u6570\u3067\u3068\u308c\u308b\u3001\u95a2\u6570\u3082return\u3067\u304d\u308b\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u3066\u8fd4\u3059\u3002\uff09Closure:\u72b6\u614b\u3092\u30ad\u30fc\u30d7\u3057\u305f\u95a2\u6570\u3002\uff08\u72b6\u614b\u3092\u52d5\u7684\u30fb\u9759\u7684\uff09 sys.path :\u306b\u5165\u308c\u308b\u3068\u30ab\u30b9\u30bf\u30e0\u3067\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3048\u308b\u3002pip\u3092\u4f7f\u3046\u3068site-packages\u306e\u4e2d\u3067\u7ba1\u7406\u3055\u308c\u308b\u3002","title":"\u95a2\u6570"},{"location":"python_basis/#_4","text":"re.search('[0-9]', string) re.search('^[0-9]', string):\u6700\u521d\u306e\u6587\u5b57 re.search('^[0-9]{4}', string):\u6700\u521d\u306e\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}', string):\u6700\u521d\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}$', string):\u6700\u5f8c\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('a*b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('a+b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30921\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('ab?c', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u304b1\u56de\u7e70\u308a\u8fd4\u3059 abc|012 or te(s|x)t \u30b0\u30eb\u30fc\u30d7 'h.t'\u4efb\u610f\u306e\u4e00\u6587\u5b57 \u30a8\u30b9\u30b1\u30fc\u30d7'h.t' \\w [a-zA-Z0-9_]\u306b\u30de\u30c3\u30c1","title":"\u6b63\u898f\u8868\u73fe"},{"location":"python_basis/#_5","text":"\u5c5e\u6027\u3068\u30e1\u30bd\u30c3\u30c9\uff1a \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3068\u30af\u30e9\u30b9\u5909\u6570 Python\u6a19\u6e96\u30e9\u30a4\u30d6\u30e9\u30ea\uff1a https://docs.python.org/ja/3/library/functions.html https://qiita.com/ichi_taro3/items/cd71a8e43040abb446a1 * dir() * id() * \u6163\u7fd2\u7684\u306a\u547d\u540d\u898f\u5247\u3068\u3057\u3066\u306e\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8(non public)\u5316\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc\uff09 * \u30cd\u30fc\u30e0\u30de\u30f3\u30b0\u30ea\u30f3\u30b0\uff08\u96e3\u53f7\u5316\uff09\u8981\u7d20\u540d\u306e\u524d\u306b\"__\"\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc2\u3064\uff09\u3092\u3064\u3051\u307e\u3059\u3002","title":"\u30af\u30e9\u30b9"},{"location":"python_basis/#_6","text":"https://qiita.com/koshigoe/items/848ddc0272b3cee92134 * staticmethod * classmrthod * property * @staticmethod\uff1a\u307b\u3068\u3093\u3069\u30af\u30e9\u30b9\u5916\u306e\u95a2\u6570\u3068\u3057\u3066\u6271\u3046\u3002\uff08self\u306f\u3044\u3089\u306a\u3044\uff09 @classmethod, cls\u306b\u5f15\u6570\u3092\u3068\u3063\u3066\u3001class\u306e\u60c5\u5831\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002 \u540d\u524d\u4fee\u98fe\uff1a__nameto\u3059\u308b\u3068\u3001_class__name getter\u3068setter\u3092\u3064\u3051\u308b\u3068\u304d\u306f\u305d\u306e\u5909\u6570\u3092non public\u306b\u3059\u308b\uff1d\uff1e\u30d7\u30ed\u30d1\u30c6\u30a3\u3092\u4f7f\u3046\u3002@property (\u5fc5\u305a_\u3092\u3064\u3051\u308b\uff09","title":"\u30c7\u30b3\u30ec\u30fc\u30bf"},{"location":"python_basis/#_7","text":"http://ichitcltk.hustle.ne.jp/gudon2/index.php?pageType=file&id=python_class_special_attribute.md http://g6no3.wp.xdomain.jp/2019/10/03/python%E3%82%AF%E3%83%A9%E3%82%B9%E3%81%AE%E7%89%B9%E6%AE%8A%E3%83%A1%E3%82%BD%E3%83%83%E3%83%89/ https://python.civic-apps.com/__xxx__/ https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee http://diveintopython3-ja.rdy.jp/special-method-names.html init call len getitem next iter str repr name :\u30e2\u30b8\u30e5\u30fc\u30eb\u5185\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u304a\u3044\u3066\u306f\u30e2\u30b8\u30e5\u30fc\u30eb\u540d\u3002\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u304a\u3044\u3066\u306f\u95a2\u6570\u540d\u306b\u306a\u308b\u3002 doc \u5c5e\u6027\u306e\u8ffd\u52a0\u3092\u7981\u6b62\u3059\u308b - slots__\u5c5e\u6027 \u30c7\u30b9\u30c8\u30e9\u30af\u30bf__del__\u30e1\u30bd\u30c3\u30c9 \u3068 with \u6587 def __contains (self,x) \u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067\u30002 in Odd()\u306e\u3088\u3046\u306bin\u30e1\u30bd\u30c3\u30c9\u306b\u5bfe\u3059\u308b\u771f\u507d\u5024\u3092\u8fd4\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 - \u5c5e\u6027\u3068\u30e1\u30bd\u30c3\u30c9 __init__\u306f\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3068\u547c\u3076 \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3068\u30af\u30e9\u30b9\u5909\u6570 \u5916\u304b\u3089\u30a2\u30af\u30bb\u30b9\u3057\u306a\u3044\u95a2\u6570\u306b\u306f_\u3092\u5148\u982d\u306b\u3064\u3051\u308b\u3002 \u7d99\u627f\u6642\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30c8\u3092super(). init ()\u3092\u3064\u3051\u308b\u3002 \u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\uff1a\u30b5\u30d6\u30af\u30e9\u30b9\u306e\u3067\u540c\u3058\u540d\u524d\u306e\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3002 super.func()\u3067\u89aa\u95a2\u6570\u3092\u547c\u3079\u308b\u3002 \u7d99\u627f\u3059\u308b\u3068\u304d\u306fstaticmethod\u3067\u306f\u547c\u3076\u3068\u304d\u306b\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3002classmethod\u3092\u4f7f\u3046\u3002 \u7d99\u627f\u6642\u306e\u540d\u524d\u4fee\u98fe\u306f__\u3092\u4f7f\u3044\u3053\u306a\u3059\u3002 \u30dd\u30ea\u30e2\u30fc\u30d5\u30a3\u30ba\u30e0\u306f\u7d99\u627f\u3092\u3057\u3066\u3044\u308bint\u3082str\u3082print\u3092\u3059\u308b\u3068\u540c\u3058\u3088\u3046\u306b\u632f\u308b\u821e\u3046","title":"\u30de\u30b8\u30c3\u30af\u30e1\u30bd\u30c3\u30c9\uff08\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9\uff09"},{"location":"python_basis/#generetor","text":"https://www.atmarkit.co.jp/ait/articles/1908/20/news024.html https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee generator \u95a2\u6570\u3067\u5b9a\u7fa9\u3059\u308b range(10)\uff08iterator\u3067\u3082\u3042\u308b\u3002\uff09 iteration\u6642\u306b\u8a08\u7b97\u3057\u3066\u51fa\u529b\u3057\u3066\u3044\u308b\u3002 yield\u304cgenerator\u306b\u5fc5\u8981\u3002 \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u3063\u3066yield\u3092\u8fd4\u3059\u3002 next\u3067\u5024\u3092\u53d6\u5f97\u3067\u304d\u308b\u3002 yield\u3092\u4f7f\u308f\u306a\u3044\u3067\u5185\u5305\u8868\u8a18\u3067generator expression\u3092\u304b\u3051\u308b\u3002 \u30e1\u30e2\u30ea\u306e\u7bc0\u7d04\u306b\u306a\u308b\u3002 iteraor\u3068\u306fnext()\u3067\u56de\u3059\u3053\u3068\u304c\u3067\u304d\u308b iter()\u95a2\u6570\u3067iterator\u3092\u8fd4\u3059\u3082\u306e\u3092iterable\u3068\u3044\u3046\u3002 \u30ea\u30b9\u30c8\u306fiterator\u3067\u306f\u306a\u3044\u304c,ieteratable. iterable iteretaro\u306f\u3000\u30af\u30e9\u30b9\u3067\u5b9a\u7fa9\u3059\u308b\u3002 next__\u3068__iter \u30ab\u30c3\u30b3\u3067\u304f\u304f\u3063\u305f\u3082\u306e (len(x) for x in something) \u7a7a\u5024\u304b\u3069\u3046\u304b\u306f if somelist\u3067\u8a55\u4fa1 range\u3088\u308a\u306fenumerate try/except/else/finally None\u3092\u8fd4\u3059\u306e\u3067\u306f\u306a\u304f\u3001\u4f8b\u5916\u51e6\u7406\u3092\u3059\u308b\u3002","title":"generetor \u5f0f"},{"location":"python_basis/#_8","text":"Syntax Error Exception try: \u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 except ZeroDivisionError(\u7279\u5b9a\u306e\u30a8\u30e9\u30fc\u306e\u7a2e\u985e) as e: except ValueError(\u7279\u5b9a\u306e\u30a8\u30e9\u30fc\u306e\u7a2e\u985e) as e: else: \u4f8b\u5916\u304c\u8d77\u304d\u305f\u3068\u304d\u306f\u5b9f\u884c\u3057\u306a\u3044\u30b3\u30fc\u30c9 finally: \u5e38\u306b\u5b9f\u884c\u3059\u308b\u30b3\u30fc\u30c9 finally \u306f\u30ad\u30e3\u30c3\u30c1\u3055\u308c\u306a\u304f\u3066\u3082\u30a8\u30e9\u30fc\u306e\u524d\u306b\u5b9f\u884c\u3055\u308c\u308b raise \u306f\u30a8\u30e9\u30fc\u3092\u767a\u751f\u3055\u305b\u308b\u3002 \u4f8b\u5916\u306e\u81ea\u4f5c Exception \u30af\u30e9\u30b9\u3092\u7d99\u627f\u3059\u308b \u578b\u306e\u30c1\u30a7\u30c3\u30af isinstance\u3067\u30c1\u30a7\u30c3\u30af method = getattr(animal, 'walk', None) if callable(method) traceback.print_exc() tracebackmodudle \u30b3\u30e1\u30f3\u30c8\u3067# TODO \u30a4\u30c6\u30ec\u30fc\u30bf\u306f1\u3064\u305a\u3064\u53d6\u3063\u3066\u3053\u308c\u308b\u3002 \u6587\u7ae0\u3092\u66f8\u304f\u3068\u304d\u306fwith open\u3092\u4f7f\u3046\u3002 read\u3068readline, readlines\u3092\u4f7f\u3044\u5206\u3051\u308b\u3002 w, \u66f8\u304d\u8fbc\u307f\uff46.write mode = 'a', 'r+', w+, a+ f.seek(0) csv\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046\u3002 json\u30e2\u30b8\u30e5\u30fc\u30eb","title":"\u30a8\u30e9\u30fc"},{"location":"python_basis/#_9","text":"assert\u30b3\u30fc\u30c9\u3067\u30c6\u30b9\u30c8\u3057\u3066\u3044\u304f\u3002 \u30c6\u30b9\u30c8\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u304f\u3002 Test runnner: unittest, self.assertEqual(power(base, exp), 8) python -m unittest test.py\u3092\u4f7f\u3046 \u4f8b\u5916\u30b1\u30fc\u30b9\u306fwith \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u3063\u3066\u66f8\u304f\u3002 with self.assertRaise(Typeerror) pytest assert\u3067\u7c21\u6f54\u306b\u66f8\u3051\u308b\u3002 \u30c6\u30b9\u30c8\u30ab\u30d0\u30ec\u30c3\u30b8 pytest-cov:\u30ab\u30d0\u30fc\u7387\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002 html\u3084xml\u3067\u51fa\u529b\u3067\u304d\u308b\u3002--cov-append","title":"\u30c6\u30b9\u30c8"},{"location":"pytorch_basis/","text":"def func_kwargs(**kwargs): print('kwargs: ', kwargs) print('type: ', type(kwargs)) func_kwargs(key1=1, key2=2, key3=3) kwargs: {'key1': 1, 'key2': 2, 'key3': 3} \u00b6 type: \u00b6 def func_kwargs_positional(arg1, arg2, **kwargs): print('arg1: ', arg1) print('arg2: ', arg2) print('kwargs: ', kwargs) func_kwargs_positional(0, 1, key1=1) arg1: 0 \u00b6 arg2: 1 \u00b6 kwargs: {'key1': 1} \u00b6 Summary\u306e\u51fa\u3057\u65b9 \u00b6 1 2 3 4 5 from torchvision import models from torchsummary import summary vgg = models . vgg16 () summary ( vgg , ( 3 , 224 , 224 )) \u3053\u308c\u306f\u5b9f\u306f\uff0cCrossEntropyLoss\u306fcall\u3067forward\u3092\u547c\u3076\u3088\u3046\u306b\u306a\u3063\u3066\u304a\u308a\uff0c\u3064\u307e\u308a\uff0c loss = criterion(outputs, labels) loss = criterion.forward(outputs, labels) \u3053\u306e\u4e8c\u3064\u306f\u540c\u3058\u3053\u3068\u3092\u3057\u3066\u3044\u307e\u3059\uff0e \u306a\u306e\u3067loss = criterion(outputs, labels)\u304cforward\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0e x = torch.autograd.Variable(torch.Tensor([3,4]), requires_grad=True) requires_grad=True\u3067\uff0c\u3053\u306eVariable\u306f\u5fae\u5206\u3059\u308b\u305e\u3068\u4f1d\u3048\u308b \u00b6 print(\"x.grad : \", x.grad) None \u00b6 \u3053\u306e\u6642\u70b9\u3067\u306f\u307e\u3060\u4f55\u3082\u5165\u3063\u3066\u3044\u306a\u3044\uff0e \u00b6 \u9069\u5f53\u306b\u76ee\u7684\u95a2\u6570\u3092\u4f5c\u308b\uff0e \u00b6 y = x[0] 2 + 5 x[1] + x[0] x[1] x[0]\u306e\u5c0e\u95a2\u6570 : 2*x[0] + x[1] \u00b6 x[0]\u306e\u5fae\u5206\u4fc2\u6570 : 2*3 + 4 = 10 \u00b6 x[1]\u306e\u5c0e\u95a2\u6570 : 5 + x[0] \u00b6 x[1]\u306e\u5fae\u5206\u4fc2\u6570 : 5 + 3 = 8 \u00b6 y.backward() torch.autograd.backward(y)\u3000\u3067\u3082\u826f\u3044\uff0e \u00b6 print(\"x.grad : \", x.grad) 10 \u00b6 8 \u00b6 .zero_grad()\u306e\u4ee3\u308f\u308a \u00b6 x.grad = None for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # zero the parameter gradients optimizer.zero_grad() #\u52fe\u914d\u306e\u521d\u671f\u5316 # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) #\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u5c64\u306e\uff08\u30d0\u30c3\u30c1\u6570, \u6b21\u5143(ex \u30af\u30e9\u30b9\u6570)\uff09\u304c\u51fa\u529b\u3055\u308c\u308b print(f'outputs: {outputs}') _, preds = torch.max(outputs, 1) #\u6700\u5927\u3068\u306a\u308bindex\u3092\u8fd4\u3059 print(f'preds: {preds}, labels: {labels}') loss = criterion(outputs, labels) #\u640d\u5931\u5024\u3092\u51fa\u3059 print(f'loss: {loss}') # backward + optimize only if in training phase if phase == 'train': loss.backward()\u3000#\u5c0e\u95a2\u6570\u306e\u7d50\u679c\u304c\u7d2f\u7a4d optimizer.step()\u3000#parameter\u306e\u66f4\u65b0 date_info = {'year': \"2020\", 'month': \"01\", 'day': \"01\"} filename = \"{year}-{month}-{day}.txt\".format(**date_info) filename '2020-01-01.txt' scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch) for epoch in range(0, 100): #\u3053\u3053\u306f\u4ee5\u4e0b\u7701\u7565 scheduler.step() os.cpu_count() psutill.cpu_count AMP https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587 **data\u306f\u8f9e\u66f8\u3092\u53d7\u3051\u53d6\u308b self\u3067\u81ea\u5206\u81ea\u8eab\u3064\u307e\u308aforward\u304c\u547c\u3070\u308c\u308b callback\u95a2\u6570\u306f\u975e\u540c\u671f\u3063\u307d\u3044\u3082\u306e \u640d\u5931\u95a2\u6570 https://yoshinashigoto-blog.herokuapp.com/detail/27/ from torchvision import models model = models.mnasnet0_5() torch.save(model.to('cpu').state_dict(), 'model.pth') from torchvision import models model = models.mnasnet0_5() model.load_state_dict(torch.load('model.pth')) \u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b \u00b6 epoch = 10 \u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u4fdd\u5b58\u3059\u308b\u3002 \u00b6 torch.save( { \"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), }, \"model.tar\", ) \u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u8aad\u307f\u8fbc\u3080\u3002 \u00b6 checkpoint = torch.load(\"model.tar\") model.load_state_dict(checkpoint[\"model_state_dict\"]) optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) epoch = checkpoint[\"epoch\"] 01.\u3000\u640d\u5931\u95a2\u6570\u3068\u306f \u307e\u305a\u640d\u5931\u95a2\u6570\u3068\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\u304c\u3046\u307e\u304f\u884c\u3063\u305f\u306e\u304b\u3069\u3046\u304b\u5224\u65ad\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3059\u308b\u95a2\u6570\u3067\u3059\u3002 \u3053\u306e\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u4e88\u6e2c\u3068\u7b54\u3048\u306e\u8aa4\u5dee\u3092\u6c42\u3081\u307e\u3059\u3002 \u305d\u306e\u8aa4\u5dee\u304c\u6700\u5c0f\u306b\u306a\u308c\u3070\u4e88\u6e2c\u306f\u3088\u308a\u6b63\u78ba\u306a\u3082\u306e\u3060\u3063\u305f\u3068\u3044\u3046\u8a55\u4fa1\u304c\u306a\u3055\u308c\u307e\u3059\u3002 \u640d\u5931\u95a2\u6570\u306b\u306f\u4e0b\u3067\u89e6\u308c\u308b\u3060\u3051\u306e\u7a2e\u985e\u304c\u3042\u308a\u3001\u76ee\u7684\u306b\u3088\u3063\u3066\u4f7f\u3044\u5206\u3051\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u95a2\u6570\u3092\u7528\u3044\u3066\u6570\u5b66\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3059\u308b\u3053\u3068\u3067\u6a5f\u68b0\u5b66\u7fd2\u306e\u4e88\u6e2c\u306e\u6b63\u78ba\u6027\u3092\u9ad8\u3081\u3066\u3044\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u3067\u306f\u6570\u5b66\u7684\u306a\u8981\u7d20\u306b\u8e0f\u307f\u8fbc\u307f\u3059\u304e\u306a\u3044\u7a0b\u5ea6\u306b\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3067\u306e\u6d3b\u7528\u65b9\u6cd5\u3092\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u3057\u3066\u3044\u304d\u307e\u3059\u3002 \u3061\u306a\u307f\u306b\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3068\u306f\u4e0d\u898f\u5247\u6027\u306e\u7a0b\u5ea6\u3092\u8868\u3059\u91cf\u3092\u3044\u3044\u307e\u3059\u3002 \u305d\u306e\u901a\u308a\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u306d\u3002 _02.\u3000\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u304c2\u30af\u30e9\u30b9\u306e\u5834\u5408\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u3068\u3044\u3046\u306e\u306f\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u304c2\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u4e00\u7a2e\u306e\u8ddd\u96e2\u3092\u8868\u3059\u3088\u3046\u306a\u6307\u6a19\u3067\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u3068\u306e\u9593\u306b\u3069\u306e\u7a0b\u5ea6\u306e\u5dee\u304c\u3042\u308b\u306e\u304b\u3092\u793a\u3059\u5c3a\u5ea6\u3067\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931L(y,t)\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u78ba\u7387yi\u3068\u6b63\u89e3j\u30af\u30e9\u30b9ti\u3092\u8868\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u5024yi\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u5c64\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5024\u3092\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u5909\u63db\u3057\u305f\u78ba\u7387\u5024\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u51fa\u529b\u5c64\u304b\u3089\u306e\u51fa\u529b\u5024\u3092\u30ed\u30b8\u30c3\u30c8\u3068\u3044\u3044\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u3068\u306f\u3042\u308b\u3042\u308b\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3068\u305d\u3046\u3067\u306a\u3044\u78ba\u73871-pn\u306e\u6bd4\u306b\u5bfe\u6570\u3092\u3068\u3063\u305f\u5024\u3067\u3059\u3002 \u5148\u306b\u51fa\u3066\u304d\u305f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306f\u30ed\u30b8\u30c3\u30c8\u95a2\u6570\u306e\u9006\u95a2\u6570\u3067\u3059\u3002 \u305d\u306e\u305f\u3081\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u30ed\u30b8\u30c3\u30c8\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u3067\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u8981\u306f\u51fa\u529b\u5024\u30920\u304b\u30891\u306e\u7bc4\u56f2\u306b\u6291\u3048\u3064\u3064\u6271\u3044\u3084\u3059\u3044\u78ba\u7387\u306e\u5f62\u306b\u5909\u63db\u3067\u304d\u308b\u516c\u5f0f\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCELoss()\u3067\u3059\u3002 \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306fnn.Sigmoid()\u3067\u3059\u3002 \u306a\u304a\u3001nn.BCELoss\u306ftorch.float32\u578b\u3092\u30c7\u30fc\u30bf\u578b\u3068\u3057\u3066\u4f7f\u7528\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u305d\u306e\u305f\u3081\u6b63\u89e3\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u578b\u306f\u672c\u6765int\u3067\u3059\u304cfloat\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 import torch from torch import nn m = nn.Sigmoid() y = torch.rand(3) t = torch.empty(3, dtype=torch.float32).random_(2) criterion = nn.BCELoss() loss = criterion(m(y), t) print(\"y: {}\".format(y)) print(\"m(y): {}\".format(m(y))) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss)) \u5b9f\u884c\u7d50\u679c \u00b6 y: tensor([0.2744, 0.9147, 0.3309]) m(y): tensor([0.5682, 0.7140, 0.5820]) t: tensor([0., 1., 0.]) loss: 0.6830 loss\u304c\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 _03.\u3000\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306b\u6700\u521d\u304b\u3089\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u304c\u52a0\u3048\u3089\u308c\u305f\u3082\u306e\u3067\u3059\u3002 \u3059\u306a\u308f\u3061\u51fa\u529b\u5024\u3092\u305d\u306e\u307e\u307e\u4e0e\u3048\u308c\u3070\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u304c\u5f97\u3089\u308c\u307e\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u3001\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u306e\u30af\u30e9\u30b9ti\u3092L(y, t)\u3068\u3057\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCEWithLogitsLoss()\u3067\u3059\u3002 \u9577\u3044\u3067\u3059\u306d\u3002 import torch from torch import nn y = torch.rand(3) t = torch.empty(3, dype=torch.float32).random_(2) criterion = nn.BCEWithLogitsLoss() loss = criterion(y, t) print(\"y: {}\".format(y)) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss)) \u5b9f\u884c\u7d50\u679c \u00b6 y: tensor([0.9709, 0.8976, 0.3228]) t: tensor([0., 1., 0.]) loss: 0.8338 loss\u304c\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 .format()\u3067\u306f\u6307\u5b9a\u3057\u305f\u5909\u6570\u3092{}\u306e\u4e2d\u306b\u4ee3\u5165\u3057\u3066\u305d\u308c\u3092\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002 _04.\u3000\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3082\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u30af\u30e9\u30b9\u304c\u3069\u306e\u304f\u3089\u3044\u96e2\u308c\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u5c3a\u5ea6\u3067\u3059\u3002 \u7279\u306b2\u30af\u30e9\u30b9\u4ee5\u4e0a\u306e\u591a\u30af\u30e9\u30b9\u306b\u5206\u985e\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u306e\u5206\u985e\u3067\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u304c\u30012\u30af\u30e9\u30b9\u4ee5\u4e0a\u3067\u306f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fn\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b9k\u306e\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u30af\u30e9\u30b9ti\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066L(y, t)\u3067\u8868\u3059\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fnn.CrossEntropyLoss\u3067\u3059\u3002 import torch from torch import nn y = torch.rand(3, 5) t = torch.empty(3, dtype=torch.int64).random_(5) criterion = nn.CrossEntropyLoss() loss = criterion(y, t) print(\"y:{}\".format(y)) print(\"t:{}\".format(t)) print(\"loss: {:4f}\".format(loss)) \u5b9f\u884c\u7d50\u679c \u00b6 y: tensor([[0.7775, 0.7587, 0.9474, 0.5149, 0.7741], [0.5059, 0.4802, 0.9846, 0.6292, 0.0167], [0.4339, 0.6873, 0.4253, 0.7067, 0.5678]]) t: tensor([1, 4, 1]) loss: 1.757074 \u30c7\u30fc\u30bf\u6570\u306f3\u3064\u3067\u5404\u30af\u30e9\u30b9\u306b\u51fa\u529b\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u6570\u306f5\u3064\u3067\u3059\u3002 loss\u304c\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 torch.cuda.amp.autocast():\u306e\u6b63\u3057\u3044indent to('cpu').numpy() cpu().detach().numpy() \u9055\u3044 ver1 \u00b6 def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class CFG: project_name = 'sample2' model_name = 'resnet18' note = '2nd' batch_size= 4 n_fold= 4 num_workers =4 image_size =224 epochs = 25 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) from tqdm import tqdm class BasicNN(nn.Module): def init (self, cfg): super(). init () self.cfg = cfg self.model = models.resnet18(pretrained=True) self.model.fc = nn.Linear(self.model.fc.in_features, 2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 self.model = self.model.to('cuda') self.current_epoch = 0 self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler = None self.metrics = None self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, fp16, ): self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.loss() if not self.optimizer: self.optimizer = self.fetch_optimizer() if not self.scheduler: self.scheduler = self.fetch_scheduler() def _init_wandb(self): hyperparams = { 'model_name' : self.cfg.model_name, 'batch_size' : self.cfg.batch_size, 'n_fold' : self.cfg.n_fold, 'num_workers' : self.cfg.num_workers, 'image_size' : self.cfg.image_size, 'epochs' : self.cfg.epochs } wandb.init( config = hyperparams, project= self.cfg.project_name, name=self.cfg.exp_name, ) wandb.watch(self) def loss(self): loss = nn.CrossEntropyLoss() return loss def fetch_optimizer(self): #opt = torch.optim.Adam(self.parameters(), lr=5e-4) opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def fetch_scheduler(self): # sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( # self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 # ) sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def monitor_metrics(self, *args, **kwargs): self.metrics = None return def forward(self, x): return self.model(x) def model_fn(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') return self(inputs) def train_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) loss.backward() self.optimizer.step() return loss, preds, labels def train_one_epoch(self): self.train() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.train_loader: loss, preds, labels = self.train_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) self.scheduler.step() #\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u304bepoch\u5358\u4f4d\u304bbatch\u5358\u4f4d\u304b\u306b\u6ce8\u610f one_epoch_loss = running_loss / dataset_sizes['train'] one_epoch_acc = running_corrects.double() / dataset_sizes['train'] return one_epoch_loss, one_epoch_acc def validate_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) return loss, preds, labels def validate_one_epoch(self): self.eval() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.valid_loader: loss, preds, labels = self.validate_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) one_epoch_loss = running_loss / dataset_sizes['val'] one_epoch_acc = running_corrects.double()/ dataset_sizes['val'] return one_epoch_loss, one_epoch_acc def predict_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds_one_batch = torch.max(outputs, 1) return preds_one_batch def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle=True, num_workers= self.num_workers ) preds_list = [] for inputs, labels in self.test_loader: preds_one_batch = self.predict_one_batch(inputs, labels) preds_list.append(preds_one_batch.to('cpu').numpy()) preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler is not None: sch_state_dict = self.scheduler.state_dict() else: sch_state_dict = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler\"] = sch_state_dict model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 torch.save(model_dict, model_path) def load(self, model_path, device=\"cuda\"): self.device = device if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, train_dataset, valid_dataset= None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, fp16 = fp16 ) self._init_wandb() tk0 = tqdm(range(epochs), position = 0, leave = True) for epoch in enumerate(tk0, 1): train_loss, train_acc = self.train_one_epoch() if valid_dataset: valid_loss, valid_acc = self.validate_one_epoch() #writer.add_scalar(\"Loss/train\", 1.0, epoch) wandb.log({ 'epoch' : epoch, \"train_acc\" : train_acc, \"valid_acc\" : valid_acc, \"loss\": train_loss, }) tk0.set_postfix(train_acc = train_acc.item(), valid_acc = valid_acc.item()) tk0.close() wandb.finish() ver3 \u00b6 class CFG: project_name = 'SETI_test2' model_name = 'efficientnetv2_rw_s' note = '2nd' batch_size= 32 n_fold= 4 num_workers =4 image_size =224 epochs = 5 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count from tqdm import tqdm class BasicNN(nn.Module): def init (self, model_name, pretrained_path): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 self.model = timm.create_model(model_name, pretrained = False, in_chans=3) self.model.load_state_dict(torch.load(pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, 1) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, fp16, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if self.multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 self.train_metric_val = None self.valid_metric_val = None if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'model_name' : cfg.model_name, 'batch_size' : cfg.batch_size, 'n_fold' : cfg.n_fold, 'num_workers' : cfg.num_workers, 'image_size' : cfg.image_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, x, targets = None): x = self.conv1(x) outputs = self.model(x) if targets is not None: loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1)) return outputs, loss return outputs, None def train_one_batch(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): d1 = datetime.datetime.now() preds_one_batch, loss = self.train_one_batch(inputs, labels) running_loss.update(loss.item(), data_loader.batch_size) # wandb.log({ # \"train_loss\": running_loss.avg, # }) d2 = datetime.datetime.now() tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", one_step_time = d2-d1) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_step(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs, loss = self(inputs, labels) return outputs, loss def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, metrics = self.valid_metric_val, stage=\"validation\") wandb.log({ \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) self.valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return self.valid_metric_val, running_loss.avg def predict_one_step(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) with torch.no_grad(): outputs, _ = self(inputs, labels) return outputs def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= self.num_workers ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001 ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, fp16 = fp16 ) # self._init_wandb(cfg) if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) # wandb.log({ # \"epoch\" : epoch, # \"epch_train_loss\" : train_loss, # \"epoch_valid_loss\" : valid_loss, # \"epoch_valid_score\" : valid_score, # }) wandb.finish() ver4 \u00b6 !pip install wandb import os import sys import random from tqdm import tqdm import datetime import psutil import pandas as pd import numpy as np from sklearn import metrics from sklearn.model_selection import StratifiedKFold import torch import torch.nn as nn import torchvision import cv2 from PIL import Image import albumentations as A import wandb import warnings warnings.filterwarnings(\"ignore\") class ClassificationDataset(): def init (self, image_paths, targets, transform = None): self.image_paths = image_paths self.targets = targets self.transform = None 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __len__(self): return len(self.image_paths) def __getitem__(self, item): targets = self.targets[item] #image1 = np.load(self.image_paths[item]).astype(float) image1 = np.load(self.image_paths[item])[::2].astype(np.float32) image = np.vstack(image1).transpose((1, 0)) image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True)) image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True)) image = image.astype(np.float32)[np.newaxis, ] # image = np.load(self.image_paths[item]).astype(np.float32) # image = np.vstack(image).transpose((1, 0)) # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC) # image = image[np.newaxis, :, :] if self.transform: image = self.transform(image=image)[\"image\"] return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count class CFG: project_name = 'SETI_test2' pretrained_model_name = 'efficientnet_b0' pretrained = True prettained_path = '../input/timm_weight/efficientnet_b0_ra-3dd342df.pth' input_channels = 3 out_dim = 1 wandb_note = '' colab_or_kaggle = 'colab' wandb_exp_name = f'{pretrained_model_name} {colab_or_kaggle} {wandb_note}' batch_size= 32 epochs = 5 num_of_fold = 5 seed = 42 patience = 3 delta = 0.002 num_workers = 8 fp16 = True checkpoint_path = '' patience_mode = 'max' patience = 3 delta = 0.002 mixup_alpha = 1.0 train_aug = A.Compose( [ A.Resize(p = 1, height = 512, width = 512), #A.Transpose(p=0.5), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.ShiftScaleRotate(p=0.5, scale_limit=0.02, rotate_limit=10, border_mode = cv2.BORDER_REPLICATE), A.MotionBlur(p=0.5), # Horizontal, Verical, shiftscale rotate, one of (very small Blur, gaussian blur, median blur, motionblur), (\u5225\u67a0gassian noise\uff09, contrast, ] ) df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv') df['img_path'] = df['id'].apply( lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy' ) X = df.img_path.values Y = df.target.values skf = StratifiedKFold(n_splits = CFG.num_of_fold) class BasicNN(nn.Module): def init (self): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 self.model = timm.create_model(CFG.pretrained_model_name, pretrained = CFG.pretrained, in_chans = CFG.input_channels) if not CFG.pretrained: self.model.load_state_dict(torch.load(CFG.pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, CFG.out_dim) #self.fc = ppe.nn.LazyLinear(None, CFG.out_dim) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, num_workers, fp16, multiple_GPU, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if num_workers == -1: num_workers = psutil.cpu_count() self.multiple_GPU = multiple_GPU if multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= num_workers, drop_last = True, pin_memory = True ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = num_workers, drop_last = False, pin_memory = True ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'batch_size' : cfg.batch_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.wandb_exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def mixup_data(self, inputs, targets, alpha=1.0): if alpha > 0: lam = np.random.beta(alpha, alpha) else: lam = 1 batch_size = inputs.size()[0] index = torch.randperm(batch_size) mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :] targets_a, targets_b = targets, targets[index] return mixed_inputs, targets_a, targets_b, lam def mixup_criterion(self, criterion, outputs, targets_a, targets_b, lam): return lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b) def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, image, targets): image, targets_a, targets_b, lam = self.mixup_data(image, targets, alpha= CFG.mixup_alpha) image = self.conv1(image) outputs = self.model(image) if targets is not None: #loss = self.criterion(outputs, targets.view(-1, 1)) loss = self.mixup_criterion(self.criterion, outputs, targets_a.view(-1, 1), targets_b.view(-1, 1), lam) return outputs, loss return outputs, None def train_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, targets) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs, targets) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def validate_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) with torch.no_grad(): outputs, loss = self(inputs, targets) return outputs, loss def predict_one_step(self, inputs, targets): outputs, _ = validate_one_step(inputs, targets) return outputs def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.train_one_step(inputs, targets) running_loss.update(loss.item(), data_loader.batch_size) current_lr = self.optimizer.param_groups[0]['lr'] wandb.log({ \"train_step\" : batch_idx, \"train_loss\": running_loss.avg, \"lr\": current_lr }) tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", lr = current_lr) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, stage=\"validation\") wandb.log({ \"validate_step\" : batch_idx, \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return valid_metric_val, running_loss.avg def predict( self, dataset, batch_size = 16, num_workers = 8, ): self.eval() self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= num_workers, drop_last = False, pin_memory = True ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, num_workers = 8, fp16 = True, multiple_GPU = False, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001, ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, num_workers = num_workers, fp16 = fp16, multiple_GPU = multiple_GPU ) self._init_wandb(cfg) torch.backends.cudnn.benchmark = True if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping and save at the check points. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(CFG.checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) print(f'epoch: {epoch}, epoch_valid_score : {valid_score}') wandb.log({ \"epoch\" : epoch, \"epch_train_loss\" : train_loss, \"epoch_valid_loss\" : valid_loss, \"epoch_valid_score\" : valid_score, }) wandb.finish() for fold_cnt, (train_index, test_index) in enumerate(skf.split(X, Y), 1): train_images, valid_images = X[train_index], X[test_index] train_targets, valid_targets = Y[train_index], Y[test_index] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 train_dataset = ClassificationDataset( image_paths=train_images, targets=train_targets, transform = None ) valid_dataset = ClassificationDataset( image_paths=valid_images, targets=valid_targets, transform = None ) model = BasicNN() model.fit( cfg = CFG, train_dataset = train_dataset, valid_dataset = valid_dataset, valid_targets = valid_targets, epochs = CFG.epochs, train_batchsize = CFG.batch_size, valid_batchsize = CFG.batch_size, num_workers = CFG.num_workers, fp16 = CFG.fp16, checkpoint_save_path = CFG.checkpoint_path, mode = CFG.patience_mode, patience = CFG.patience, delta = CFG.delta )","title":"Pytorch basis"},{"location":"pytorch_basis/#kwargs-key1-1-key2-2-key3-3","text":"","title":"kwargs:  {'key1': 1, 'key2': 2, 'key3': 3}"},{"location":"pytorch_basis/#type","text":"def func_kwargs_positional(arg1, arg2, **kwargs): print('arg1: ', arg1) print('arg2: ', arg2) print('kwargs: ', kwargs) func_kwargs_positional(0, 1, key1=1)","title":"type:  "},{"location":"pytorch_basis/#arg1-0","text":"","title":"arg1:  0"},{"location":"pytorch_basis/#arg2-1","text":"","title":"arg2:  1"},{"location":"pytorch_basis/#kwargs-key1-1","text":"","title":"kwargs:  {'key1': 1}"},{"location":"pytorch_basis/#summary","text":"1 2 3 4 5 from torchvision import models from torchsummary import summary vgg = models . vgg16 () summary ( vgg , ( 3 , 224 , 224 )) \u3053\u308c\u306f\u5b9f\u306f\uff0cCrossEntropyLoss\u306fcall\u3067forward\u3092\u547c\u3076\u3088\u3046\u306b\u306a\u3063\u3066\u304a\u308a\uff0c\u3064\u307e\u308a\uff0c loss = criterion(outputs, labels) loss = criterion.forward(outputs, labels) \u3053\u306e\u4e8c\u3064\u306f\u540c\u3058\u3053\u3068\u3092\u3057\u3066\u3044\u307e\u3059\uff0e \u306a\u306e\u3067loss = criterion(outputs, labels)\u304cforward\u306b\u306a\u3063\u3066\u3044\u307e\u3059\uff0e x = torch.autograd.Variable(torch.Tensor([3,4]), requires_grad=True)","title":"Summary\u306e\u51fa\u3057\u65b9"},{"location":"pytorch_basis/#requires_gradtruevariable","text":"print(\"x.grad : \", x.grad)","title":"requires_grad=True\u3067\uff0c\u3053\u306eVariable\u306f\u5fae\u5206\u3059\u308b\u305e\u3068\u4f1d\u3048\u308b"},{"location":"pytorch_basis/#none","text":"","title":"None"},{"location":"pytorch_basis/#_1","text":"","title":"\u3053\u306e\u6642\u70b9\u3067\u306f\u307e\u3060\u4f55\u3082\u5165\u3063\u3066\u3044\u306a\u3044\uff0e"},{"location":"pytorch_basis/#_2","text":"y = x[0] 2 + 5 x[1] + x[0] x[1]","title":"\u9069\u5f53\u306b\u76ee\u7684\u95a2\u6570\u3092\u4f5c\u308b\uff0e"},{"location":"pytorch_basis/#x0-2x0-x1","text":"","title":"x[0]\u306e\u5c0e\u95a2\u6570 : 2*x[0] + x[1]"},{"location":"pytorch_basis/#x0-23-4-10","text":"","title":"x[0]\u306e\u5fae\u5206\u4fc2\u6570 : 2*3 + 4 = 10"},{"location":"pytorch_basis/#x1-5-x0","text":"","title":"x[1]\u306e\u5c0e\u95a2\u6570 : 5 + x[0]"},{"location":"pytorch_basis/#x1-5-3-8","text":"y.backward()","title":"x[1]\u306e\u5fae\u5206\u4fc2\u6570 : 5 + 3 = 8"},{"location":"pytorch_basis/#torchautogradbackwardy","text":"print(\"x.grad : \", x.grad)","title":"torch.autograd.backward(y)\u3000\u3067\u3082\u826f\u3044\uff0e"},{"location":"pytorch_basis/#10","text":"","title":"10"},{"location":"pytorch_basis/#8","text":"","title":"8"},{"location":"pytorch_basis/#zero_grad","text":"x.grad = None for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # zero the parameter gradients optimizer.zero_grad() #\u52fe\u914d\u306e\u521d\u671f\u5316 # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) #\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u51fa\u529b\u5c64\u306e\uff08\u30d0\u30c3\u30c1\u6570, \u6b21\u5143(ex \u30af\u30e9\u30b9\u6570)\uff09\u304c\u51fa\u529b\u3055\u308c\u308b print(f'outputs: {outputs}') _, preds = torch.max(outputs, 1) #\u6700\u5927\u3068\u306a\u308bindex\u3092\u8fd4\u3059 print(f'preds: {preds}, labels: {labels}') loss = criterion(outputs, labels) #\u640d\u5931\u5024\u3092\u51fa\u3059 print(f'loss: {loss}') # backward + optimize only if in training phase if phase == 'train': loss.backward()\u3000#\u5c0e\u95a2\u6570\u306e\u7d50\u679c\u304c\u7d2f\u7a4d optimizer.step()\u3000#parameter\u306e\u66f4\u65b0 date_info = {'year': \"2020\", 'month': \"01\", 'day': \"01\"} filename = \"{year}-{month}-{day}.txt\".format(**date_info) filename '2020-01-01.txt' scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch) for epoch in range(0, 100): #\u3053\u3053\u306f\u4ee5\u4e0b\u7701\u7565 scheduler.step() os.cpu_count() psutill.cpu_count AMP https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587 **data\u306f\u8f9e\u66f8\u3092\u53d7\u3051\u53d6\u308b self\u3067\u81ea\u5206\u81ea\u8eab\u3064\u307e\u308aforward\u304c\u547c\u3070\u308c\u308b callback\u95a2\u6570\u306f\u975e\u540c\u671f\u3063\u307d\u3044\u3082\u306e \u640d\u5931\u95a2\u6570 https://yoshinashigoto-blog.herokuapp.com/detail/27/ from torchvision import models model = models.mnasnet0_5() torch.save(model.to('cpu').state_dict(), 'model.pth') from torchvision import models model = models.mnasnet0_5() model.load_state_dict(torch.load('model.pth'))","title":".zero_grad()\u306e\u4ee3\u308f\u308a"},{"location":"pytorch_basis/#_3","text":"epoch = 10","title":"\u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b"},{"location":"pytorch_basis/#_4","text":"torch.save( { \"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), }, \"model.tar\", )","title":"\u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u4fdd\u5b58\u3059\u308b\u3002"},{"location":"pytorch_basis/#_5","text":"checkpoint = torch.load(\"model.tar\") model.load_state_dict(checkpoint[\"model_state_dict\"]) optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) epoch = checkpoint[\"epoch\"] 01.\u3000\u640d\u5931\u95a2\u6570\u3068\u306f \u307e\u305a\u640d\u5931\u95a2\u6570\u3068\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4e88\u6e2c\u304c\u3046\u307e\u304f\u884c\u3063\u305f\u306e\u304b\u3069\u3046\u304b\u5224\u65ad\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3059\u308b\u95a2\u6570\u3067\u3059\u3002 \u3053\u306e\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u3001\u4e88\u6e2c\u3068\u7b54\u3048\u306e\u8aa4\u5dee\u3092\u6c42\u3081\u307e\u3059\u3002 \u305d\u306e\u8aa4\u5dee\u304c\u6700\u5c0f\u306b\u306a\u308c\u3070\u4e88\u6e2c\u306f\u3088\u308a\u6b63\u78ba\u306a\u3082\u306e\u3060\u3063\u305f\u3068\u3044\u3046\u8a55\u4fa1\u304c\u306a\u3055\u308c\u307e\u3059\u3002 \u640d\u5931\u95a2\u6570\u306b\u306f\u4e0b\u3067\u89e6\u308c\u308b\u3060\u3051\u306e\u7a2e\u985e\u304c\u3042\u308a\u3001\u76ee\u7684\u306b\u3088\u3063\u3066\u4f7f\u3044\u5206\u3051\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u95a2\u6570\u3092\u7528\u3044\u3066\u6570\u5b66\u7684\u306a\u30a2\u30d7\u30ed\u30fc\u30c1\u3092\u3059\u308b\u3053\u3068\u3067\u6a5f\u68b0\u5b66\u7fd2\u306e\u4e88\u6e2c\u306e\u6b63\u78ba\u6027\u3092\u9ad8\u3081\u3066\u3044\u304d\u307e\u3059\u3002 \u4ee5\u4e0b\u3067\u306f\u6570\u5b66\u7684\u306a\u8981\u7d20\u306b\u8e0f\u307f\u8fbc\u307f\u3059\u304e\u306a\u3044\u7a0b\u5ea6\u306b\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3067\u306e\u6d3b\u7528\u65b9\u6cd5\u3092\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u3057\u3066\u3044\u304d\u307e\u3059\u3002 \u3061\u306a\u307f\u306b\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3068\u306f\u4e0d\u898f\u5247\u6027\u306e\u7a0b\u5ea6\u3092\u8868\u3059\u91cf\u3092\u3044\u3044\u307e\u3059\u3002 \u305d\u306e\u901a\u308a\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u306d\u3002 _02.\u3000\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bf\u306e\u30af\u30e9\u30b9\u304c2\u30af\u30e9\u30b9\u306e\u5834\u5408\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u3068\u3044\u3046\u306e\u306f\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u304c2\u3064\u3067\u3042\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u307e\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u4e00\u7a2e\u306e\u8ddd\u96e2\u3092\u8868\u3059\u3088\u3046\u306a\u6307\u6a19\u3067\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u3068\u306e\u9593\u306b\u3069\u306e\u7a0b\u5ea6\u306e\u5dee\u304c\u3042\u308b\u306e\u304b\u3092\u793a\u3059\u5c3a\u5ea6\u3067\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931L(y,t)\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u78ba\u7387yi\u3068\u6b63\u89e3j\u30af\u30e9\u30b9ti\u3092\u8868\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b91\u306e\u4e88\u6e2c\u5024yi\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u5c64\u304b\u3089\u51fa\u529b\u3055\u308c\u305f\u5024\u3092\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3067\u5909\u63db\u3057\u305f\u78ba\u7387\u5024\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u51fa\u529b\u5c64\u304b\u3089\u306e\u51fa\u529b\u5024\u3092\u30ed\u30b8\u30c3\u30c8\u3068\u3044\u3044\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u3068\u306f\u3042\u308b\u3042\u308b\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3068\u305d\u3046\u3067\u306a\u3044\u78ba\u73871-pn\u306e\u6bd4\u306b\u5bfe\u6570\u3092\u3068\u3063\u305f\u5024\u3067\u3059\u3002 \u5148\u306b\u51fa\u3066\u304d\u305f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306f\u30ed\u30b8\u30c3\u30c8\u95a2\u6570\u306e\u9006\u95a2\u6570\u3067\u3059\u3002 \u305d\u306e\u305f\u3081\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u30ed\u30b8\u30c3\u30c8\u3092\u5165\u529b\u3059\u308b\u3053\u3068\u3067\u30af\u30e9\u30b9\u306e\u78ba\u7387p\u3092\u6c42\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u8981\u306f\u51fa\u529b\u5024\u30920\u304b\u30891\u306e\u7bc4\u56f2\u306b\u6291\u3048\u3064\u3064\u6271\u3044\u3084\u3059\u3044\u78ba\u7387\u306e\u5f62\u306b\u5909\u63db\u3067\u304d\u308b\u516c\u5f0f\u3068\u3044\u3063\u305f\u611f\u3058\u3067\u3059\u3002 \u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCELoss()\u3067\u3059\u3002 \u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306fnn.Sigmoid()\u3067\u3059\u3002 \u306a\u304a\u3001nn.BCELoss\u306ftorch.float32\u578b\u3092\u30c7\u30fc\u30bf\u578b\u3068\u3057\u3066\u4f7f\u7528\u3057\u306a\u3051\u308c\u3070\u306a\u308a\u307e\u305b\u3093\u3002 \u305d\u306e\u305f\u3081\u6b63\u89e3\u30af\u30e9\u30b9\u306e\u30c7\u30fc\u30bf\u578b\u306f\u672c\u6765int\u3067\u3059\u304cfloat\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 import torch from torch import nn m = nn.Sigmoid() y = torch.rand(3) t = torch.empty(3, dtype=torch.float32).random_(2) criterion = nn.BCELoss() loss = criterion(m(y), t) print(\"y: {}\".format(y)) print(\"m(y): {}\".format(m(y))) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss))","title":"\u5b66\u7fd2\u9014\u4e2d\u306e\u72b6\u614b\u3092\u8aad\u307f\u8fbc\u3080\u3002"},{"location":"pytorch_basis/#_6","text":"y: tensor([0.2744, 0.9147, 0.3309]) m(y): tensor([0.5682, 0.7140, 0.5820]) t: tensor([0., 1., 0.]) loss: 0.6830 loss\u304c\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 _03.\u3000\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306b\u6700\u521d\u304b\u3089\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u304c\u52a0\u3048\u3089\u308c\u305f\u3082\u306e\u3067\u3059\u3002 \u3059\u306a\u308f\u3061\u51fa\u529b\u5024\u3092\u305d\u306e\u307e\u307e\u4e0e\u3048\u308c\u3070\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u304c\u5f97\u3089\u308c\u307e\u3059\u3002 n\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u3001\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306f\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u306e\u30af\u30e9\u30b9ti\u3092L(y, t)\u3068\u3057\u3066\u8868\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306e\u95a2\u6570\u306fnn.BCEWithLogitsLoss()\u3067\u3059\u3002 \u9577\u3044\u3067\u3059\u306d\u3002 import torch from torch import nn y = torch.rand(3) t = torch.empty(3, dype=torch.float32).random_(2) criterion = nn.BCEWithLogitsLoss() loss = criterion(y, t) print(\"y: {}\".format(y)) print(\"t: {}\".format(t)) print(\"loss: {:.4f}\".format(loss))","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"pytorch_basis/#_7","text":"y: tensor([0.9709, 0.8976, 0.3228]) t: tensor([0., 1., 0.]) loss: 0.8338 loss\u304c\u30ed\u30b8\u30c3\u30c8\u4ed8\u304d\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3067\u3059\u3002 .format()\u3067\u306f\u6307\u5b9a\u3057\u305f\u5909\u6570\u3092{}\u306e\u4e2d\u306b\u4ee3\u5165\u3057\u3066\u305d\u308c\u3092\u51fa\u529b\u3057\u3066\u3044\u307e\u3059\u3002 _04.\u3000\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3082\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u529b\u3068\u6b63\u89e3\u30af\u30e9\u30b9\u304c\u3069\u306e\u304f\u3089\u3044\u96e2\u308c\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3059\u308b\u5c3a\u5ea6\u3067\u3059\u3002 \u7279\u306b2\u30af\u30e9\u30b9\u4ee5\u4e0a\u306e\u591a\u30af\u30e9\u30b9\u306b\u5206\u985e\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306b\u7528\u3044\u3089\u308c\u307e\u3059\u3002 2\u30af\u30e9\u30b9\u306e\u5206\u985e\u3067\u306f\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u3092\u4f7f\u7528\u3057\u307e\u3057\u305f\u304c\u30012\u30af\u30e9\u30b9\u4ee5\u4e0a\u3067\u306f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fn\u500b\u306e\u30c7\u30fc\u30bf\u304c\u3042\u3063\u305f\u3068\u3057\u3066\u30c7\u30fc\u30bfi\u306b\u5bfe\u3059\u308b\u30af\u30e9\u30b9k\u306e\u30ed\u30b8\u30c3\u30c8yi\u3068\u6b63\u89e3\u30af\u30e9\u30b9ti\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066L(y, t)\u3067\u8868\u3059\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u306fnn.CrossEntropyLoss\u3067\u3059\u3002 import torch from torch import nn y = torch.rand(3, 5) t = torch.empty(3, dtype=torch.int64).random_(5) criterion = nn.CrossEntropyLoss() loss = criterion(y, t) print(\"y:{}\".format(y)) print(\"t:{}\".format(t)) print(\"loss: {:4f}\".format(loss))","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"pytorch_basis/#_8","text":"y: tensor([[0.7775, 0.7587, 0.9474, 0.5149, 0.7741], [0.5059, 0.4802, 0.9846, 0.6292, 0.0167], [0.4339, 0.6873, 0.4253, 0.7067, 0.5678]]) t: tensor([1, 4, 1]) loss: 1.757074 \u30c7\u30fc\u30bf\u6570\u306f3\u3064\u3067\u5404\u30af\u30e9\u30b9\u306b\u51fa\u529b\u3057\u307e\u3059\u3002 \u30af\u30e9\u30b9\u6570\u306f5\u3064\u3067\u3059\u3002 loss\u304c\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u640d\u5931\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 torch.cuda.amp.autocast():\u306e\u6b63\u3057\u3044indent to('cpu').numpy() cpu().detach().numpy() \u9055\u3044","title":"\u5b9f\u884c\u7d50\u679c"},{"location":"pytorch_basis/#ver1","text":"def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class CFG: project_name = 'sample2' model_name = 'resnet18' note = '2nd' batch_size= 4 n_fold= 4 num_workers =4 image_size =224 epochs = 25 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) from tqdm import tqdm class BasicNN(nn.Module): def init (self, cfg): super(). init () self.cfg = cfg self.model = models.resnet18(pretrained=True) self.model.fc = nn.Linear(self.model.fc.in_features, 2) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 self.model = self.model.to('cuda') self.current_epoch = 0 self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler = None self.metrics = None self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, fp16, ): self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.loss() if not self.optimizer: self.optimizer = self.fetch_optimizer() if not self.scheduler: self.scheduler = self.fetch_scheduler() def _init_wandb(self): hyperparams = { 'model_name' : self.cfg.model_name, 'batch_size' : self.cfg.batch_size, 'n_fold' : self.cfg.n_fold, 'num_workers' : self.cfg.num_workers, 'image_size' : self.cfg.image_size, 'epochs' : self.cfg.epochs } wandb.init( config = hyperparams, project= self.cfg.project_name, name=self.cfg.exp_name, ) wandb.watch(self) def loss(self): loss = nn.CrossEntropyLoss() return loss def fetch_optimizer(self): #opt = torch.optim.Adam(self.parameters(), lr=5e-4) opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def fetch_scheduler(self): # sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( # self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 # ) sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def monitor_metrics(self, *args, **kwargs): self.metrics = None return def forward(self, x): return self.model(x) def model_fn(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') return self(inputs) def train_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) loss.backward() self.optimizer.step() return loss, preds, labels def train_one_epoch(self): self.train() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.train_loader: loss, preds, labels = self.train_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) self.scheduler.step() #\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u304bepoch\u5358\u4f4d\u304bbatch\u5358\u4f4d\u304b\u306b\u6ce8\u610f one_epoch_loss = running_loss / dataset_sizes['train'] one_epoch_acc = running_corrects.double() / dataset_sizes['train'] return one_epoch_loss, one_epoch_acc def validate_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds = torch.max(outputs, 1) loss = self.criterion(outputs, labels) return loss, preds, labels def validate_one_epoch(self): self.eval() running_loss = 0.0 running_corrects = 0 for inputs, labels in self.valid_loader: loss, preds, labels = self.validate_one_batch(inputs, labels) running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) one_epoch_loss = running_loss / dataset_sizes['val'] one_epoch_acc = running_corrects.double()/ dataset_sizes['val'] return one_epoch_loss, one_epoch_acc def predict_one_batch(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs = self(inputs) _, preds_one_batch = torch.max(outputs, 1) return preds_one_batch def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle=True, num_workers= self.num_workers ) preds_list = [] for inputs, labels in self.test_loader: preds_one_batch = self.predict_one_batch(inputs, labels) preds_list.append(preds_one_batch.to('cpu').numpy()) preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler is not None: sch_state_dict = self.scheduler.state_dict() else: sch_state_dict = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler\"] = sch_state_dict model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 torch.save(model_dict, model_path) def load(self, model_path, device=\"cuda\"): self.device = device if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, train_dataset, valid_dataset= None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, fp16 = fp16 ) self._init_wandb() tk0 = tqdm(range(epochs), position = 0, leave = True) for epoch in enumerate(tk0, 1): train_loss, train_acc = self.train_one_epoch() if valid_dataset: valid_loss, valid_acc = self.validate_one_epoch() #writer.add_scalar(\"Loss/train\", 1.0, epoch) wandb.log({ 'epoch' : epoch, \"train_acc\" : train_acc, \"valid_acc\" : valid_acc, \"loss\": train_loss, }) tk0.set_postfix(train_acc = train_acc.item(), valid_acc = valid_acc.item()) tk0.close() wandb.finish()","title":"ver1"},{"location":"pytorch_basis/#ver3","text":"class CFG: project_name = 'SETI_test2' model_name = 'efficientnetv2_rw_s' note = '2nd' batch_size= 32 n_fold= 4 num_workers =4 image_size =224 epochs = 5 seed = 42 scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'] T_max = 6 # CosineAnnealingLR #T_0=6 # CosineAnnealingWarmRestarts lr=1e-4 min_lr=1e-6 exp_name = f'{model_name} {note} {batch_size}Batch' print(CFG.model_name) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count from tqdm import tqdm class BasicNN(nn.Module): def init (self, model_name, pretrained_path): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 self.model = timm.create_model(model_name, pretrained = False, in_chans=3) self.model.load_state_dict(torch.load(pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, 1) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False self.num_workers = 1 def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, fp16, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if self.multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) self.num_workers = min(4, psutil.cpu_count()) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= self.num_workers ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = self.num_workers ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 self.train_metric_val = None self.valid_metric_val = None if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'model_name' : cfg.model_name, 'batch_size' : cfg.batch_size, 'n_fold' : cfg.n_fold, 'num_workers' : cfg.num_workers, 'image_size' : cfg.image_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, x, targets = None): x = self.conv1(x) outputs = self.model(x) if targets is not None: loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1)) return outputs, loss return outputs, None def train_one_batch(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, labels) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): d1 = datetime.datetime.now() preds_one_batch, loss = self.train_one_batch(inputs, labels) running_loss.update(loss.item(), data_loader.batch_size) # wandb.log({ # \"train_loss\": running_loss.avg, # }) d2 = datetime.datetime.now() tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", one_step_time = d2-d1) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_step(self, inputs, labels): inputs = inputs.to('cuda') labels = labels.to('cuda') with torch.no_grad(): outputs, loss = self(inputs, labels) return outputs, loss def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, metrics = self.valid_metric_val, stage=\"validation\") wandb.log({ \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) self.valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return self.valid_metric_val, running_loss.avg def predict_one_step(self, inputs, labels): inputs = inputs.to(self.device) labels = labels.to(self.device) with torch.no_grad(): outputs, _ = self(inputs, labels) return outputs def predict( self, dataset, batch_size, ): self.eval() self.num_workers = min(4, psutil.cpu_count()) self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= self.num_workers ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, labels) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, labels) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, fp16 = True, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001 ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, fp16 = fp16 ) # self._init_wandb(cfg) if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.model_name}_epoch{epoch}.pth\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) # wandb.log({ # \"epoch\" : epoch, # \"epch_train_loss\" : train_loss, # \"epoch_valid_loss\" : valid_loss, # \"epoch_valid_score\" : valid_score, # }) wandb.finish()","title":"ver3"},{"location":"pytorch_basis/#ver4","text":"!pip install wandb import os import sys import random from tqdm import tqdm import datetime import psutil import pandas as pd import numpy as np from sklearn import metrics from sklearn.model_selection import StratifiedKFold import torch import torch.nn as nn import torchvision import cv2 from PIL import Image import albumentations as A import wandb import warnings warnings.filterwarnings(\"ignore\") class ClassificationDataset(): def init (self, image_paths, targets, transform = None): self.image_paths = image_paths self.targets = targets self.transform = None 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __len__(self): return len(self.image_paths) def __getitem__(self, item): targets = self.targets[item] #image1 = np.load(self.image_paths[item]).astype(float) image1 = np.load(self.image_paths[item])[::2].astype(np.float32) image = np.vstack(image1).transpose((1, 0)) image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True)) image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True)) image = image.astype(np.float32)[np.newaxis, ] # image = np.load(self.image_paths[item]).astype(np.float32) # image = np.vstack(image).transpose((1, 0)) # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC) # image = image[np.newaxis, :, :] if self.transform: image = self.transform(image=image)[\"image\"] return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float) def set_seed(seed = 0): np.random.seed(seed) random_state = np.random.RandomState(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False os.environ['PYTHONHASHSEED'] = str(seed) return random_state class AverageMeter: \"\"\" Computes and stores the average and current value \"\"\" def init (self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 1 2 3 4 5 6 7 8 9 10 11 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count class CFG: project_name = 'SETI_test2' pretrained_model_name = 'efficientnet_b0' pretrained = True prettained_path = '../input/timm_weight/efficientnet_b0_ra-3dd342df.pth' input_channels = 3 out_dim = 1 wandb_note = '' colab_or_kaggle = 'colab' wandb_exp_name = f'{pretrained_model_name} {colab_or_kaggle} {wandb_note}' batch_size= 32 epochs = 5 num_of_fold = 5 seed = 42 patience = 3 delta = 0.002 num_workers = 8 fp16 = True checkpoint_path = '' patience_mode = 'max' patience = 3 delta = 0.002 mixup_alpha = 1.0 train_aug = A.Compose( [ A.Resize(p = 1, height = 512, width = 512), #A.Transpose(p=0.5), A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.ShiftScaleRotate(p=0.5, scale_limit=0.02, rotate_limit=10, border_mode = cv2.BORDER_REPLICATE), A.MotionBlur(p=0.5), # Horizontal, Verical, shiftscale rotate, one of (very small Blur, gaussian blur, median blur, motionblur), (\u5225\u67a0gassian noise\uff09, contrast, ] ) df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv') df['img_path'] = df['id'].apply( lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy' ) X = df.img_path.values Y = df.target.values skf = StratifiedKFold(n_splits = CFG.num_of_fold) class BasicNN(nn.Module): def init (self): super(). init () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 self.model = timm.create_model(CFG.pretrained_model_name, pretrained = CFG.pretrained, in_chans = CFG.input_channels) if not CFG.pretrained: self.model.load_state_dict(torch.load(CFG.pretrained_path)) self.model.classifier = nn.Linear(self.model.classifier.in_features, CFG.out_dim) #self.fc = ppe.nn.LazyLinear(None, CFG.out_dim) self.conv1 = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=3, bias=False) self.valid_targets = None self.current_epoch = 0 self.device = None self.fp16 = True self.train_loader = None self.valid_loader = None self.scaler = True self.criterion = None self.optimizer = None self.scheduler_after_step = None self.scheduler_after_epoch = None self.metrics = None self.multiple_GPU = False def _init_model( self, train_dataset, valid_dataset, train_batchsize, valid_batchsize, valid_targets, num_workers, fp16, multiple_GPU, ): if self.device is None: self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if num_workers == -1: num_workers = psutil.cpu_count() self.multiple_GPU = multiple_GPU if multiple_GPU and torch.cuda.device_count() > 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") self = nn.DataParallel(self) self.to(self.device) if self.train_loader is None: self.train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = train_batchsize, shuffle=True, num_workers= num_workers, drop_last = True, pin_memory = True ) if self.valid_loader is None: self.valid_loader = torch.utils.data.DataLoader( dataset = valid_dataset, batch_size=valid_batchsize, shuffle=False, num_workers = num_workers, drop_last = False, pin_memory = True ) if self.valid_targets is None: self.valid_targets = valid_targets self.fp16 = fp16 if self.fp16: self.scaler = torch.cuda.amp.GradScaler() if not self.criterion: self.criterion = self.configure_criterion() if not self.optimizer: self.optimizer = self.configure_optimizer() if not self.scheduler_after_step: self.scheduler_after_step = self.configure_scheduler_after_step() if not self.scheduler_after_epoch: self.scheduler_after_epoch = self.configure_scheduler_after_epoch() def _init_wandb(self, cfg): hyperparams = { 'batch_size' : cfg.batch_size, 'epochs' : cfg.epochs } wandb.init( config = hyperparams, project= cfg.project_name, name=cfg.wandb_exp_name, ) wandb.watch(self) def configure_criterion(self): criterion = nn.BCEWithLogitsLoss() return criterion def mixup_data(self, inputs, targets, alpha=1.0): if alpha > 0: lam = np.random.beta(alpha, alpha) else: lam = 1 batch_size = inputs.size()[0] index = torch.randperm(batch_size) mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :] targets_a, targets_b = targets, targets[index] return mixed_inputs, targets_a, targets_b, lam def mixup_criterion(self, criterion, outputs, targets_a, targets_b, lam): return lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b) def configure_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=5e-4) #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9) return opt def configure_scheduler_after_step(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1) #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False, #step_size_up=1400,step_size_down=1400, mode=\"triangular2\") return sch def configure_scheduler_after_epoch(self): return None def epoch_metrics(self, outputs, targets): return metrics.roc_auc_score(targets, outputs) def forward(self, image, targets): image, targets_a, targets_b, lam = self.mixup_data(image, targets, alpha= CFG.mixup_alpha) image = self.conv1(image) outputs = self.model(image) if targets is not None: #loss = self.criterion(outputs, targets.view(-1, 1)) loss = self.mixup_criterion(self.criterion, outputs, targets_a.view(-1, 1), targets_b.view(-1, 1), lam) return outputs, loss return outputs, None def train_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) self.optimizer.zero_grad() with torch.set_grad_enabled(True): if self.fp16: with torch.cuda.amp.autocast(): outputs, loss = self(inputs, targets) self.scaler.scale(loss).backward() self.scaler.step(self.optimizer) self.scaler.update() else: outputs, loss = self(inputs, targets) loss.backward() self.optimizer.step() if self.scheduler_after_step: self.scheduler_after_step.step() return outputs, loss def validate_one_step(self, inputs, targets): inputs = inputs.to(self.device, non_blocking=True) targets = targets.to(self.device, non_blocking=True) with torch.no_grad(): outputs, loss = self(inputs, targets) return outputs, loss def predict_one_step(self, inputs, targets): outputs, _ = validate_one_step(inputs, targets) return outputs def train_one_epoch(self, data_loader): self.train() running_loss = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.train_one_step(inputs, targets) running_loss.update(loss.item(), data_loader.batch_size) current_lr = self.optimizer.param_groups[0]['lr'] wandb.log({ \"train_step\" : batch_idx, \"train_loss\": running_loss.avg, \"lr\": current_lr }) tk0.set_postfix(train_loss=running_loss.avg, stage=\"train\", lr = current_lr) if self.scheduler_after_epoch: self.scheduler_after_epoch.step() tk0.close() return running_loss.avg def validate_one_epoch(self, data_loader): self.eval() running_loss = AverageMeter() preds_list = [] tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch, loss = self.validate_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) running_loss.update(loss.item(), data_loader.batch_size) tk0.set_postfix(valid_loss = running_loss.avg, stage=\"validation\") wandb.log({ \"validate_step\" : batch_idx, \"validate_loss\": running_loss.avg, }) preds_arr = np.concatenate(preds_list) valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets) tk0.close() return valid_metric_val, running_loss.avg def predict( self, dataset, batch_size = 16, num_workers = 8, ): self.eval() self.test_loader = torch.utils.data.DataLoader( dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers= num_workers, drop_last = False, pin_memory = True ) preds_list = [] tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True) for batch_idx, (inputs, targets) in enumerate(tk0): preds_one_batch = self.predict_one_step(inputs, targets) preds_list.append(preds_one_batch.cpu().detach().numpy()) tk0.set_postfix(stage=\"inference\") tk0.close() preds_arr = np.concatenate(preds_list) return preds_arr def save(self, model_path): model_state_dict = self.state_dict() if self.optimizer is not None: opt_state_dict = self.optimizer.state_dict() else: opt_state_dict = None if self.scheduler_after_step is not None: sch_state_dict_after_step = self.scheduler_after_step.state_dict() else: sch_state_dict_after_step = None if self.scheduler_after_epoch is not None: sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict() else: sch_state_dict_after_epoch = None model_dict = {} model_dict[\"state_dict\"] = model_state_dict model_dict[\"optimizer\"] = opt_state_dict model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch model_dict[\"epoch\"] = self.current_epoch model_dict[\"fp16\"] = self.fp16 model_dict[\"multiple_GPU\"] = self.multiple_GPU torch.save(model_dict, model_path) def load(self, model_path): self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") if next(self.parameters()).device != self.device: self.to(self.device) model_dict = torch.load(model_path, map_location=torch.device(device)) self.load_state_dict(model_dict[\"state_dict\"]) def fit( self, cfg, train_dataset, valid_dataset= None, valid_targets = None, epochs = 10, train_batchsize = 16, valid_batchsize = 16, num_workers = 8, fp16 = True, multiple_GPU = False, checkpoint_save_path = '', mode = 'max', patience = 5, delta = 0.001, ): set_seed(CFG.seed) self._init_model( train_dataset = train_dataset, valid_dataset = valid_dataset, train_batchsize = train_batchsize, valid_batchsize = valid_batchsize, valid_targets = valid_targets, num_workers = num_workers, fp16 = fp16, multiple_GPU = multiple_GPU ) self._init_wandb(cfg) torch.backends.cudnn.benchmark = True if mode == 'max': current_best_valid_score = -float('inf') else: current_best_valid_score = float('inf') early_stopping_counter = 0 for epoch in range(epochs): train_loss = self.train_one_epoch(self.train_loader) if valid_dataset: valid_score, valid_loss = self.validate_one_epoch(self.valid_loader) # Early Stopping and save at the check points. if mode == 'max': if valid_score < current_best_valid_score + delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(CFG.checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) else: if valid_score > current_best_valid_score - delta: early_stopping_counter += 1 print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}') if early_stopping_counter >= patience: break else: print(f\"Validation score improved ({current_best_valid_score} --> {valid_score}). Saving the check point!\") current_best_valid_score = valid_score self.save(checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) print(f'epoch: {epoch}, epoch_valid_score : {valid_score}') wandb.log({ \"epoch\" : epoch, \"epch_train_loss\" : train_loss, \"epoch_valid_loss\" : valid_loss, \"epoch_valid_score\" : valid_score, }) wandb.finish() for fold_cnt, (train_index, test_index) in enumerate(skf.split(X, Y), 1): train_images, valid_images = X[train_index], X[test_index] train_targets, valid_targets = Y[train_index], Y[test_index] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 train_dataset = ClassificationDataset( image_paths=train_images, targets=train_targets, transform = None ) valid_dataset = ClassificationDataset( image_paths=valid_images, targets=valid_targets, transform = None ) model = BasicNN() model.fit( cfg = CFG, train_dataset = train_dataset, valid_dataset = valid_dataset, valid_targets = valid_targets, epochs = CFG.epochs, train_batchsize = CFG.batch_size, valid_batchsize = CFG.batch_size, num_workers = CFG.num_workers, fp16 = CFG.fp16, checkpoint_save_path = CFG.checkpoint_path, mode = CFG.patience_mode, patience = CFG.patience, delta = CFG.delta )","title":"ver4"},{"location":"templates/","text":"Note \u3053\u308c\u306f\u30ce\u30fc\u30c8\u3067\u3059\u3002 Tip \u30d2\u30f3\u30c8\u3067\u3059\u3002 Warning \u3053\u308c\u306f\u8b66\u544a\u3067\u3059\u3002 Danger \u3053\u308c\u306f\u5371\u967a\u3067\u3059\u3002 Success \u3053\u308c\u306f\u6210\u529f\u3067\u3059\u3002 Failure \u3053\u308c\u306f\u5931\u6557\u3067\u3059\u3002 Bug \u3053\u308c\u306f\u30d0\u30b0\u3067\u3059\u3002 Summary \u3053\u308c\u306f\u6982\u8981\u3067\u3059\u3002 Mkdocs \u3068\u306f\u9759\u7684\u30b5\u30a4\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3067\u3059\u3002 \u30b3\u30f3\u30c6\u30f3\u30c4\u306f\u57fa\u672c\u7684\u306b markdown 1 \u5f62\u5f0f\u3067\u8a18\u8ff0\u3057\u305f\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u306b\u306a\u308a\u307e\u3059\u3002 \u5b9a\u7fa9\u8a9e \u3053\u3053\u306b\u8aac\u660e\u3092\u66f8\u304d\u307e\u3059 \u6587\u66f8\u3092\u8a18\u8ff0\u3059\u308b\u305f\u3081\u306e\u8efd\u91cf\u30de\u30fc\u30af\u30a2\u30c3\u30d7\u8a00\u8a9e\u306e\u3072\u3068\u3064 \u21a9","title":"pytorch templates"}]}