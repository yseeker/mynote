{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Test \u00b6 test1 \u00b6 test2 \u00b6","title":"AL-DS"},{"location":"#test","text":"","title":"Test"},{"location":"#test1","text":"","title":"test1"},{"location":"#test2","text":"","title":"test2"},{"location":"albumentations/","text":"Image Augumentations \u00b6 \u30b5\u30f3\u30d7\u30eb\u5199\u771f\u306e\u8868\u793a \u00b6 \u30e9\u30a4\u30d6\u30e9\u30ea\u306eimport \u00b6 1 2 3 4 5 import matplotlib.pyplot as plt import albumentations as A import numpy as np import cv2 from PIL import Image Pillow \u3068OpenCV\u305d\u308c\u305e\u308c\u3067\u753b\u50cf\u3092\u8868\u793a \u00b6 \u753b\u50cf\u30c7\u30fc\u30bf\u306fKaggle\u306e Flowers Recognition \u304b\u3089\u53d6\u5f97\u3002Pillow\u3092\u4f7f\u3046\u5834\u5408\u306f\u3001\u8aad\u307f\u8fbc\u3093\u3060\u3068\u304d\u306bJpegImageFile\u306a\u306e\u3067openCV\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 image_path = '../input/flowers-recognition/flowers/daisy/10140303196_b88d3d6cec.jpg' Pillow\u306f\u5358\u306a\u308b\u753b\u50cf\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308a\u3001OpenCV\u306f\u300c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u300d\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u78ba\u304b\u306b\u6a5f\u80fd\u304c\u91cd\u8907\u3059\u308b\u90e8\u5206\u306f\u591a\u3044\uff08\u3064\u307e\u308aOpenCV\u306b\u306f\u304b\u306a\u308a\u306e\u753b\u50cf\u51e6\u7406\u6a5f\u80fd\u304c\u542b\u307e\u308c\u3066\u3044\u308b\uff09\u304c\u3001 \u305d\u306e\u6271\u3046\u5185\u5bb9\u306f\u5927\u304d\u304f\u7570\u306a\u308a\u307e\u3059\u3002\u6975\u7aef\u306a\u8a71\u3001\u753b\u50cf\u3092\u30ab\u30c3\u30c8\u3084\u30ea\u30b5\u30a4\u30ba\u3057\u305f\u3044\u6642\u3084\u3001\u5c11\u3057\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u3044\u5834\u5408\u306f Pillow \u3092\u4f7f\u3044\u3001\u7269\u4e8b\u3092\u300c\u898b\u3088\u3046\u300d\u3068\u601d\u3063\u3066\u3044\u308b\u30ed\u30dc\u30c3\u30c8\u3092\u7d44\u307f\u305f\u3044\u6642\u306b\u306f OpenCV \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u5f15\u7528\u5143\uff1a https://teratail.com/questions/71851 Pillow 1 2 3 4 5 6 img = Image . open ( image_path ) # img: JpegImageFile img = np . asarray ( img ) # \u3082\u3068\u306e\u753b\u50cf\u306b\u623b\u3059\u5834\u5408 # im = Image.fromarray(np.uint8(myarray*255)) plt . imshow ( img ) OpenCV 1 2 3 4 5 6 img = cv2 . imread ( image_path ) # img : ndarray (N-dimensional array, np.array\u306b\u3088\u3063\u3066\u751f\u6210) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # \u4e0b\u8a18\u3082RGB\u753b\u50cf\u2192BGR\u753b\u50cf\u3078\u306e\u5909\u63db #img = img[:,:,::-1] plt . imshow ( img ) \u53c2\u8003 - https://note.nkmk.me/python-image-processing-pillow-numpy-opencv/ - https://nixeneko.hatenablog.com/entry/2017/09/01/000000 - https://tomomai.com/python-opencv-pillow/ - https://www.codexa.net/opencv_python_introduction/ (open CV\u306b\u95a2\u3057\u3066) Note \u4e0b\u8a18\u306e\u753b\u50cf\u8868\u793a\u30b3\u30fc\u30c9\u306f\u3001 https://github.com/tkuri/albumentations_test/blob/master/albumentations_test.ipynb \u3000\u3092\u53c2\u8003\u306b\u3057\u305f\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 aug = [] n = 3 param1 = ( 1 , 20 ) param2 = ( 16 , 16 ) aug . append ( A . Compose ([ A . Blur ( p = 1 )])) aug . append ( A . Compose ([ A . MedianBlur ( p = 1 )]) aug . append ( A . Compose ([ A . GaussianBlur ( p = 1 )]) aug_img = [ aug [ i ]( image = img ) for i in range ( n )] fig , ax = plt . subplots ( 1 , 1 + n , figsize = ( 5 + 5 * n , 5 )) plt . subplots_adjust ( wspace = 0 ) plt . rcParams [ \"font.size\" ] = 18 [ ax [ i ] . tick_params ( bottom = False , left = False , right = False , top = False , labelbottom = False , labelleft = False , labelright = False , labeltop = False ) for i in range ( 1 + n )] ax [ 0 ] . set_xlabel ( \"Original\" ) ax [ 1 ] . set_xlabel ( \"Default Augmentation\" ) ax [ 2 ] . set_xlabel ( \"blur_limit= {} \" . format ( param1 )) ax [ 3 ] . set_xlabel ( \"blur_limit= {} \" . format ( param2 )) ax [ 0 ] . imshow ( img ) [ ax [ i + 1 ] . imshow ( aug_img [ i ][ 'image' ]) for i in range ( n )] Albumentations \u00b6 \u53c2\u8003\uff1a https://qiita.com/kurilab/items/b69e1be8d0224ae139ad Flip, Crop, Rotate etc.\uff08\u30d5\u30ea\u30c3\u30d7\u3001\u5207\u308a\u53d6\u308a\u3001\u56de\u8ee2\u306a\u3069\uff09 \u00b6 \u30d5\u30ea\u30c3\u30d7 \u00b6 \u5207\u308a\u53d6\u308a \u00b6 Blur, Noise\uff08\u307c\u304b\u3057\uff09 \u00b6 Blur \u00b6 \u9ad8\u5ea6\u5e7e\u4f55\u5909\u63db\u7cfb (Affine, Distortion) \u00b6","title":"albumentations"},{"location":"albumentations/#image-augumentations","text":"","title":"Image Augumentations"},{"location":"albumentations/#_1","text":"","title":"\u30b5\u30f3\u30d7\u30eb\u5199\u771f\u306e\u8868\u793a"},{"location":"albumentations/#import","text":"1 2 3 4 5 import matplotlib.pyplot as plt import albumentations as A import numpy as np import cv2 from PIL import Image","title":"\u30e9\u30a4\u30d6\u30e9\u30ea\u306eimport"},{"location":"albumentations/#pillow-opencv","text":"\u753b\u50cf\u30c7\u30fc\u30bf\u306fKaggle\u306e Flowers Recognition \u304b\u3089\u53d6\u5f97\u3002Pillow\u3092\u4f7f\u3046\u5834\u5408\u306f\u3001\u8aad\u307f\u8fbc\u3093\u3060\u3068\u304d\u306bJpegImageFile\u306a\u306e\u3067openCV\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 image_path = '../input/flowers-recognition/flowers/daisy/10140303196_b88d3d6cec.jpg' Pillow\u306f\u5358\u306a\u308b\u753b\u50cf\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3042\u308a\u3001OpenCV\u306f\u300c\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30d3\u30b8\u30e7\u30f3\u300d\u7528\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u78ba\u304b\u306b\u6a5f\u80fd\u304c\u91cd\u8907\u3059\u308b\u90e8\u5206\u306f\u591a\u3044\uff08\u3064\u307e\u308aOpenCV\u306b\u306f\u304b\u306a\u308a\u306e\u753b\u50cf\u51e6\u7406\u6a5f\u80fd\u304c\u542b\u307e\u308c\u3066\u3044\u308b\uff09\u304c\u3001 \u305d\u306e\u6271\u3046\u5185\u5bb9\u306f\u5927\u304d\u304f\u7570\u306a\u308a\u307e\u3059\u3002\u6975\u7aef\u306a\u8a71\u3001\u753b\u50cf\u3092\u30ab\u30c3\u30c8\u3084\u30ea\u30b5\u30a4\u30ba\u3057\u305f\u3044\u6642\u3084\u3001\u5c11\u3057\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3057\u305f\u3044\u5834\u5408\u306f Pillow \u3092\u4f7f\u3044\u3001\u7269\u4e8b\u3092\u300c\u898b\u3088\u3046\u300d\u3068\u601d\u3063\u3066\u3044\u308b\u30ed\u30dc\u30c3\u30c8\u3092\u7d44\u307f\u305f\u3044\u6642\u306b\u306f OpenCV \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u5f15\u7528\u5143\uff1a https://teratail.com/questions/71851 Pillow 1 2 3 4 5 6 img = Image . open ( image_path ) # img: JpegImageFile img = np . asarray ( img ) # \u3082\u3068\u306e\u753b\u50cf\u306b\u623b\u3059\u5834\u5408 # im = Image.fromarray(np.uint8(myarray*255)) plt . imshow ( img ) OpenCV 1 2 3 4 5 6 img = cv2 . imread ( image_path ) # img : ndarray (N-dimensional array, np.array\u306b\u3088\u3063\u3066\u751f\u6210) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) # \u4e0b\u8a18\u3082RGB\u753b\u50cf\u2192BGR\u753b\u50cf\u3078\u306e\u5909\u63db #img = img[:,:,::-1] plt . imshow ( img ) \u53c2\u8003 - https://note.nkmk.me/python-image-processing-pillow-numpy-opencv/ - https://nixeneko.hatenablog.com/entry/2017/09/01/000000 - https://tomomai.com/python-opencv-pillow/ - https://www.codexa.net/opencv_python_introduction/ (open CV\u306b\u95a2\u3057\u3066) Note \u4e0b\u8a18\u306e\u753b\u50cf\u8868\u793a\u30b3\u30fc\u30c9\u306f\u3001 https://github.com/tkuri/albumentations_test/blob/master/albumentations_test.ipynb \u3000\u3092\u53c2\u8003\u306b\u3057\u305f\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 aug = [] n = 3 param1 = ( 1 , 20 ) param2 = ( 16 , 16 ) aug . append ( A . Compose ([ A . Blur ( p = 1 )])) aug . append ( A . Compose ([ A . MedianBlur ( p = 1 )]) aug . append ( A . Compose ([ A . GaussianBlur ( p = 1 )]) aug_img = [ aug [ i ]( image = img ) for i in range ( n )] fig , ax = plt . subplots ( 1 , 1 + n , figsize = ( 5 + 5 * n , 5 )) plt . subplots_adjust ( wspace = 0 ) plt . rcParams [ \"font.size\" ] = 18 [ ax [ i ] . tick_params ( bottom = False , left = False , right = False , top = False , labelbottom = False , labelleft = False , labelright = False , labeltop = False ) for i in range ( 1 + n )] ax [ 0 ] . set_xlabel ( \"Original\" ) ax [ 1 ] . set_xlabel ( \"Default Augmentation\" ) ax [ 2 ] . set_xlabel ( \"blur_limit= {} \" . format ( param1 )) ax [ 3 ] . set_xlabel ( \"blur_limit= {} \" . format ( param2 )) ax [ 0 ] . imshow ( img ) [ ax [ i + 1 ] . imshow ( aug_img [ i ][ 'image' ]) for i in range ( n )]","title":"Pillow \u3068OpenCV\u305d\u308c\u305e\u308c\u3067\u753b\u50cf\u3092\u8868\u793a"},{"location":"albumentations/#albumentations","text":"\u53c2\u8003\uff1a https://qiita.com/kurilab/items/b69e1be8d0224ae139ad","title":"Albumentations"},{"location":"albumentations/#flip-crop-rotate-etc","text":"","title":"Flip, Crop, Rotate etc.\uff08\u30d5\u30ea\u30c3\u30d7\u3001\u5207\u308a\u53d6\u308a\u3001\u56de\u8ee2\u306a\u3069\uff09"},{"location":"albumentations/#_2","text":"","title":"\u30d5\u30ea\u30c3\u30d7"},{"location":"albumentations/#_3","text":"","title":"\u5207\u308a\u53d6\u308a"},{"location":"albumentations/#blur-noise","text":"","title":"Blur, Noise\uff08\u307c\u304b\u3057\uff09"},{"location":"albumentations/#blur","text":"","title":"Blur"},{"location":"albumentations/#affine-distortion","text":"","title":"\u9ad8\u5ea6\u5e7e\u4f55\u5909\u63db\u7cfb (Affine, Distortion)"},{"location":"colab/","text":"Google colab \u00b6 Timeout \u00b6 1 2 3 4 5 function KeepClicking (){ console . log ( \"Clicking\" ); document . querySelector ( \"colab-connect-button\" ). click () } setInterval ( KeepClicking , 60000 ) Data\u3092content\u76f4\u4e0b\u306b\u79fb\u52d5\u3057\u3066unzip \u00b6 1 2 3 %%capture !unzip \"/content/drive/MyDrive/kaggle/input/seti-breakthrough-listen/seti-train.zip\" -d \"/content\" print ( 'Downlaod done' ) Kaggle\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3046 \u00b6 1 2 3 4 5 6 7 8 import os import json f = open ( \"/content/drive/My Drive/Kaggle/kaggle.json\" , 'r' ) json_data = json . load ( f ) #JSON\u5f62\u5f0f\u3067\u8aad\u307f\u8fbc\u3080 os . environ [ 'KAGGLE_USERNAME' ] = json_data [ 'username' ] os . environ [ 'KAGGLE_KEY' ] = json_data [ 'key' ] ! kaggle competitions submit digit - recognizer - f my_submission . csv - m \"Yeah! I submit my file through the Google Colab!\" Github\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u00b6 1 !pip install git+https://github.com/yseeker/tez_custom Output\u30bb\u30eb\u3092\u975e\u8868\u793a \u00b6 1 %%capture","title":"Google Colab"},{"location":"colab/#google-colab","text":"","title":"Google colab"},{"location":"colab/#timeout","text":"1 2 3 4 5 function KeepClicking (){ console . log ( \"Clicking\" ); document . querySelector ( \"colab-connect-button\" ). click () } setInterval ( KeepClicking , 60000 )","title":"Timeout"},{"location":"colab/#datacontentunzip","text":"1 2 3 %%capture !unzip \"/content/drive/MyDrive/kaggle/input/seti-breakthrough-listen/seti-train.zip\" -d \"/content\" print ( 'Downlaod done' )","title":"Data\u3092content\u76f4\u4e0b\u306b\u79fb\u52d5\u3057\u3066unzip"},{"location":"colab/#kaggle","text":"1 2 3 4 5 6 7 8 import os import json f = open ( \"/content/drive/My Drive/Kaggle/kaggle.json\" , 'r' ) json_data = json . load ( f ) #JSON\u5f62\u5f0f\u3067\u8aad\u307f\u8fbc\u3080 os . environ [ 'KAGGLE_USERNAME' ] = json_data [ 'username' ] os . environ [ 'KAGGLE_KEY' ] = json_data [ 'key' ] ! kaggle competitions submit digit - recognizer - f my_submission . csv - m \"Yeah! I submit my file through the Google Colab!\"","title":"Kaggle\u30b3\u30de\u30f3\u30c9\u3092\u4f7f\u3046"},{"location":"colab/#github","text":"1 !pip install git+https://github.com/yseeker/tez_custom","title":"Github\u304b\u3089\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"colab/#output","text":"1 %%capture","title":"Output\u30bb\u30eb\u3092\u975e\u8868\u793a"},{"location":"dcgan/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 28 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval betwen image samples\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , z ): img = self . model ( z ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . model = nn . Sequential ( nn . Linear ( int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 256 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 256 , 1 ), nn . Sigmoid (), ) def forward ( self , img ): img_flat = img . view ( img . size ( 0 ), - 1 ) validity = self . model ( img_flat ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . size ( 0 ), 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . size ( 0 ), 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) DCGAN \u00b6 G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084\u8ee2\u79fb\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092tanh\u95a2\u6570\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092leaky relu\u306b\u4ee3\u7528 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) cuda = True if torch . cuda . is_available () else False def weights_init_normal ( m ): classname = m . __class__ . __name__ if classname . find ( \"Conv\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 0.0 , 0.02 ) elif classname . find ( \"BatchNorm2d\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 1.0 , 0.02 ) torch . nn . init . constant_ ( m . bias . data , 0.0 ) class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . init_size = opt . img_size // 4 self . l1 = nn . Sequential ( nn . Linear ( opt . latent_dim , 128 * self . init_size ** 2 )) self . conv_blocks = nn . Sequential ( nn . BatchNorm2d ( 128 ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 64 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 64 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Conv2d ( 64 , opt . channels , 3 , stride = 1 , padding = 1 ), nn . Tanh (), ) def forward ( self , z ): out = self . l1 ( z ) out = out . view ( out . shape [ 0 ], 128 , self . init_size , self . init_size ) img = self . conv_blocks ( out ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () def discriminator_block ( in_filters , out_filters , bn = True ): block = [ nn . Conv2d ( in_filters , out_filters , 3 , 2 , 1 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Dropout2d ( 0.25 )] if bn : block . append ( nn . BatchNorm2d ( out_filters , 0.8 )) return block self . model = nn . Sequential ( * discriminator_block ( opt . channels , 16 , bn = False ), * discriminator_block ( 16 , 32 ), * discriminator_block ( 32 , 64 ), * discriminator_block ( 64 , 128 ), ) # The height and width of downsampled image ds_size = opt . img_size // 2 ** 4 self . adv_layer = nn . Sequential ( nn . Linear ( 128 * ds_size ** 2 , 1 ), nn . Sigmoid ()) def forward ( self , img ): out = self . model ( img ) out = out . view ( out . shape [ 0 ], - 1 ) validity = self . adv_layer ( out ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Initialize weights generator . apply ( weights_init_normal ) discriminator . apply ( weights_init_normal ) # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) Conditional GAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) WGAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) Cycle GAN Star GAN Progressive GAN Pix2Pix","title":"other"},{"location":"dcgan/#dcgan","text":"G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084\u8ee2\u79fb\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092tanh\u95a2\u6570\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092leaky relu\u306b\u4ee3\u7528 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) cuda = True if torch . cuda . is_available () else False def weights_init_normal ( m ): classname = m . __class__ . __name__ if classname . find ( \"Conv\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 0.0 , 0.02 ) elif classname . find ( \"BatchNorm2d\" ) != - 1 : torch . nn . init . normal_ ( m . weight . data , 1.0 , 0.02 ) torch . nn . init . constant_ ( m . bias . data , 0.0 ) class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . init_size = opt . img_size // 4 self . l1 = nn . Sequential ( nn . Linear ( opt . latent_dim , 128 * self . init_size ** 2 )) self . conv_blocks = nn . Sequential ( nn . BatchNorm2d ( 128 ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 128 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Upsample ( scale_factor = 2 ), nn . Conv2d ( 128 , 64 , 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 64 , 0.8 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Conv2d ( 64 , opt . channels , 3 , stride = 1 , padding = 1 ), nn . Tanh (), ) def forward ( self , z ): out = self . l1 ( z ) out = out . view ( out . shape [ 0 ], 128 , self . init_size , self . init_size ) img = self . conv_blocks ( out ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () def discriminator_block ( in_filters , out_filters , bn = True ): block = [ nn . Conv2d ( in_filters , out_filters , 3 , 2 , 1 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Dropout2d ( 0.25 )] if bn : block . append ( nn . BatchNorm2d ( out_filters , 0.8 )) return block self . model = nn . Sequential ( * discriminator_block ( opt . channels , 16 , bn = False ), * discriminator_block ( 16 , 32 ), * discriminator_block ( 32 , 64 ), * discriminator_block ( 64 , 128 ), ) # The height and width of downsampled image ds_size = opt . img_size // 2 ** 4 self . adv_layer = nn . Sequential ( nn . Linear ( 128 * ds_size ** 2 , 1 ), nn . Sigmoid ()) def forward ( self , img ): out = self . model ( img ) out = out . view ( out . shape [ 0 ], - 1 ) validity = self . adv_layer ( out ) return validity # Loss function adversarial_loss = torch . nn . BCELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Initialize weights generator . apply ( weights_init_normal ) discriminator . apply ( weights_init_normal ) # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) Tensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , _ ) in enumerate ( dataloader ): # Adversarial ground truths valid = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( Tensor ( imgs . shape [ 0 ], 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( Tensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise as generator input z = Variable ( Tensor ( np . random . normal ( 0 , 1 , ( imgs . shape [ 0 ], opt . latent_dim )))) # Generate a batch of images gen_imgs = generator ( z ) # Loss measures generator's ability to fool the discriminator g_loss = adversarial_loss ( discriminator ( gen_imgs ), valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Measure discriminator's ability to classify real from generated samples real_loss = adversarial_loss ( discriminator ( real_imgs ), valid ) fake_loss = adversarial_loss ( discriminator ( gen_imgs . detach ()), fake ) d_loss = ( real_loss + fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : save_image ( gen_imgs . data [: 25 ], \"images/ %d .png\" % batches_done , nrow = 5 , normalize = True ) Conditional GAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) WGAN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 import argparse import os import numpy as np import math import torchvision.transforms as transforms from torchvision.utils import save_image from torch.utils.data import DataLoader from torchvision import datasets from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F import torch os . makedirs ( \"images\" , exist_ok = True ) parser = argparse . ArgumentParser () parser . add_argument ( \"--n_epochs\" , type = int , default = 200 , help = \"number of epochs of training\" ) parser . add_argument ( \"--batch_size\" , type = int , default = 64 , help = \"size of the batches\" ) parser . add_argument ( \"--lr\" , type = float , default = 0.0002 , help = \"adam: learning rate\" ) parser . add_argument ( \"--b1\" , type = float , default = 0.5 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--b2\" , type = float , default = 0.999 , help = \"adam: decay of first order momentum of gradient\" ) parser . add_argument ( \"--n_cpu\" , type = int , default = 8 , help = \"number of cpu threads to use during batch generation\" ) parser . add_argument ( \"--latent_dim\" , type = int , default = 100 , help = \"dimensionality of the latent space\" ) parser . add_argument ( \"--n_classes\" , type = int , default = 10 , help = \"number of classes for dataset\" ) parser . add_argument ( \"--img_size\" , type = int , default = 32 , help = \"size of each image dimension\" ) parser . add_argument ( \"--channels\" , type = int , default = 1 , help = \"number of image channels\" ) parser . add_argument ( \"--sample_interval\" , type = int , default = 400 , help = \"interval between image sampling\" ) opt = parser . parse_args () print ( opt ) img_shape = ( opt . channels , opt . img_size , opt . img_size ) cuda = True if torch . cuda . is_available () else False class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () self . label_emb = nn . Embedding ( opt . n_classes , opt . n_classes ) def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim + opt . n_classes , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , noise , labels ): # Concatenate label embedding and image to produce input gen_input = torch . cat (( self . label_emb ( labels ), noise ), - 1 ) img = self . model ( gen_input ) img = img . view ( img . size ( 0 ), * img_shape ) return img class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . label_embedding = nn . Embedding ( opt . n_classes , opt . n_classes ) self . model = nn . Sequential ( nn . Linear ( opt . n_classes + int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 512 ), nn . Dropout ( 0.4 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 1 ), ) def forward ( self , img , labels ): # Concatenate label embedding and image to produce input d_in = torch . cat (( img . view ( img . size ( 0 ), - 1 ), self . label_embedding ( labels )), - 1 ) validity = self . model ( d_in ) return validity # Loss functions adversarial_loss = torch . nn . MSELoss () # Initialize generator and discriminator generator = Generator () discriminator = Discriminator () if cuda : generator . cuda () discriminator . cuda () adversarial_loss . cuda () # Configure data loader os . makedirs ( \"../../data/mnist\" , exist_ok = True ) dataloader = torch . utils . data . DataLoader ( datasets . MNIST ( \"../../data/mnist\" , train = True , download = True , transform = transforms . Compose ( [ transforms . Resize ( opt . img_size ), transforms . ToTensor (), transforms . Normalize ([ 0.5 ], [ 0.5 ])] ), ), batch_size = opt . batch_size , shuffle = True , ) # Optimizers optimizer_G = torch . optim . Adam ( generator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) optimizer_D = torch . optim . Adam ( discriminator . parameters (), lr = opt . lr , betas = ( opt . b1 , opt . b2 )) FloatTensor = torch . cuda . FloatTensor if cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if cuda else torch . LongTensor def sample_image ( n_row , batches_done ): \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\" # Sample noise z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( n_row ** 2 , opt . latent_dim )))) # Get labels ranging from 0 to n_classes for n rows labels = np . array ([ num for _ in range ( n_row ) for num in range ( n_row )]) labels = Variable ( LongTensor ( labels )) gen_imgs = generator ( z , labels ) save_image ( gen_imgs . data , \"images/ %d .png\" % batches_done , nrow = n_row , normalize = True ) # ---------- # Training # ---------- for epoch in range ( opt . n_epochs ): for i , ( imgs , labels ) in enumerate ( dataloader ): batch_size = imgs . shape [ 0 ] # Adversarial ground truths valid = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 1.0 ), requires_grad = False ) fake = Variable ( FloatTensor ( batch_size , 1 ) . fill_ ( 0.0 ), requires_grad = False ) # Configure input real_imgs = Variable ( imgs . type ( FloatTensor )) labels = Variable ( labels . type ( LongTensor )) # ----------------- # Train Generator # ----------------- optimizer_G . zero_grad () # Sample noise and labels as generator input z = Variable ( FloatTensor ( np . random . normal ( 0 , 1 , ( batch_size , opt . latent_dim )))) gen_labels = Variable ( LongTensor ( np . random . randint ( 0 , opt . n_classes , batch_size ))) # Generate a batch of images gen_imgs = generator ( z , gen_labels ) # Loss measures generator's ability to fool the discriminator validity = discriminator ( gen_imgs , gen_labels ) g_loss = adversarial_loss ( validity , valid ) g_loss . backward () optimizer_G . step () # --------------------- # Train Discriminator # --------------------- optimizer_D . zero_grad () # Loss for real images validity_real = discriminator ( real_imgs , labels ) d_real_loss = adversarial_loss ( validity_real , valid ) # Loss for fake images validity_fake = discriminator ( gen_imgs . detach (), gen_labels ) d_fake_loss = adversarial_loss ( validity_fake , fake ) # Total discriminator loss d_loss = ( d_real_loss + d_fake_loss ) / 2 d_loss . backward () optimizer_D . step () print ( \"[Epoch %d / %d ] [Batch %d / %d ] [D loss: %f ] [G loss: %f ]\" % ( epoch , opt . n_epochs , i , len ( dataloader ), d_loss . item (), g_loss . item ()) ) batches_done = epoch * len ( dataloader ) + i if batches_done % opt . sample_interval == 0 : sample_image ( n_row = 10 , batches_done = batches_done ) Cycle GAN Star GAN Progressive GAN Pix2Pix","title":"DCGAN"},{"location":"docker/","text":"Dockerhub\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb Dockerfile\u304b\u3089DockerImage, DockerImage\u304b\u3089\u30b3\u30f3\u30c6\u30ca https://scrapbox.io/llminatoll/docker_run%E3%81%AE%E3%82%AA%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D https://beyondjapan.com/blog/2016/08/docker-command-reverse-resolutions/ https://smot93516.hatenablog.jp/entry/2018/09/20/001052 https://morizyun.github.io/docker/about-docker-command.html docker login : dockerhub\u306b\u30ed\u30b0\u30a4\u30f3\u3059\u308b docker pull <image> dockerhub \u304b\u3089\u30a4\u30e1\u30fc\u30b8\u3092\u3068\u3063\u3066\u304f\u308b docker images dockerimage\u306e\u4e00\u89a7\u3092\u8868\u793a docker run <image> create + start, docker\u30a4\u30e1\u30fc\u30b8\u304b\u3089\u30b3\u30f3\u30c6\u30ca\u3092\u4f5c\u6210\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u304c\u5b9f\u884c\u3055\u308c\u308b\u3002 docker ps -a \u30b3\u30f3\u30c6\u30ca\u30a4\u30e1\u30fc\u30b8\u306e\u4e00\u89a7\u3092\u8868\u793a docker run -it ubuntu bash :docker \u3067ubuntu\u306ebash\u3092\u8d77\u52d5\uff08bash\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u30b3\u30de\u30f3\u30c9\u4e0a\u66f8\u304d\uff09, -it \u306fbash\u3092\u8d77\u52d5\u72b6\u614b\uff08up\uff09\u306b\u4fdd\u6301\u3059\u308b\u3002-it\u304c\u306a\u3044\u3068exit\u72b6\u614b\u306b\u5909\u308f\u308b\u3002-i:\u6a19\u6e96\u5165\u529b\u3092\u958b\u304f\u3001-t:\u51fa\u529b\u304c\u304d\u308c\u3044\u306b\u306a\u308b\u3002 ctri + p + q :detach \u7d42\u4e86 docker attach <container> attach\u3067up\u72b6\u614b\u306econtainer\u306b\u5165\u308b\u3002 exit :\u7d42\u4e86 docker restart <container> docker exec -it <container> <command> docker commit <container> <new image> \u30b3\u30f3\u30c6\u30ca\u304b\u3089new image\u3068\u3057\u3066\u4fdd\u5b58 docker commit <container> ubuntu:updated \u30bb\u30df\u30b3\u30ed\u30f3\u3067tag\u540d\u306b\u306a\u308b image\u540d\u306frepostitory\u540d\uff0btag\u540d docker tag <source> <target> docker tag ubuntu:updated <username>/my-repo :\u540d\u524d\u306e\u5909\u66f4 library/ubuntu\u306f, \u6b63\u5f0f\u306b\u306fregistry-1.docker.io/library/ubuntu:latest docker push <image> docker pull <image> docker rmi <image> docker image \u3092\u524a\u9664 docker rm <container> \u30b3\u30f3\u30c6\u30ca\u306e\u524a\u9664 docker stop <container> \u30b3\u30f3\u30c6\u30ca\u3092\u6b62\u3081\u308b docker system prune :\u30b3\u30f3\u30c6\u30ca\u5168\u524a\u9664 docker run --name <name> <image> :\u30b3\u30f3\u30c6\u30ca\u306e\u540d\u524d\u3092\u3064\u3051\u308b\u3002 docker run -d <image> :\u30b3\u30f3\u30c6\u30ca\u3092\u8d77\u52d5\u5f8c\u306bdetach\u3059\u308b\uff08host\u306b\u623b\u308b\uff09 docker run -rm <image> :\u30b3\u30f3\u30c6\u30ca\u3092exit\u5f8c\u306b\u524a\u9664\u3059\u308b\u3002 docker images -aq | xargs docker rmi docker ps -aq | xargs docker rm `` docker file\u306e\u4f5c\u6210 \u00b6 1 2 3 4 5 6 7 8 9 10 11 FROM ubuntu:latest ADD copressed.tar / COPY something /new_directory/ ENV key1 value RUN apt update && apt install -y \\ aaa \\ bbb \\ ccc WORKDIR /sample_folder RUN touch something CMD [ \"executable\" , \"param1\" , \"param2\" ] docker build <directory> docker build -t <name> <directory> docker build -f <dockerfilename> <build context> \u540d\u524d\u306f\u30c9\u30c3\u30c8\u3067\u3064\u306a\u304c\u308b\u3053\u3068\u304c\u591a\u3044\u3002Dockerfile\u3068\u3044\u3046\u540d\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u30d3\u30eb\u30c9\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u5165\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3002 FROM :\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30a4\u30e1\u30fc\u30b8\u3092\u6c7a\u5b9a RUN :Linux\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3002RUN\u6bce\u306bLayer\u304c\u4f5c\u3089\u308c\u308b\u3002Layer\u6570\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002&&\u3067\u3064\u306a\u3052\u308b\u3002(\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u3067)\\\u30d0\u30c3\u30af\u30b9\u30e9\u30c3\u30b7\u30e5\u3067\u6539\u884c\u3059\u308b\u3002 \u6700\u521d\u306fLayer\u3092\u7d30\u304b\u304f\u5206\u3051\u3066\u901a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\u6700\u5f8c\u306bLayer\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 CMD :\u30b3\u30f3\u30c6\u30ca\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u3092\u6307\u5b9a\u3002CMD [\"command\", \"param1\", \"paramn2\"] ex. CMD [/bin/bash], CMD\u306f\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u3089\u306a\u3044\u3002 Docker\u30b3\u30de\u30f3\u30c9\u3067Docker Daemon\u306b\u547d\u4ee4\u3092\u51fa\u3059 * COPY: \u5358\u7d14\u306b\u30d5\u30a1\u30a4\u30eb\u3084\u30d5\u30a9\u30eb\u30c0\u3092\u30b3\u30d4\u30fc\u3059\u308b\u5834\u5408 * ADD: tar\u306e\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u3092\u89e3\u7b54\u3059\u308b ENTRYPOINT \u306f\u4e0a\u66f8\u304d\u3067\u304d\u306a\u3044\uff08CMD\u306f\u4e0a\u66f8\u304d\u3067\u304d\u308b\uff09\u3002ENTRYPOINT\u304c\u3042\u308b\u3068\u304d\u306fCMD\u306fparams\u306e\u307f\u3092\u66f8\u304f\u3002 ENTRYPOINT\u306f\u30b3\u30f3\u30c6\u30ca\u3092\u30b3\u30de\u30f3\u30c9\u306e\u3088\u3046\u306b\u4f7f\u3044\u305f\u3044\u3068\u304d\u3002 ENV :\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u3002 WORKDIR \u5b9f\u884c\u74b0\u5883\u3092\u5909\u66f4\u3059\u308b\u3002 \u30db\u30b9\u30c8\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u3064\u306a\u3050 \u00b6 docker run -it -v <host>:<container> <image bash> docker run -it -u $(id -u):$(id -g) -v ~/mouted_folder:/new_dir <image> bash -u <uder id>:<group id>: \u30e6\u30fc\u30b6ID\u3068\u30b0\u30eb\u30fc\u30d7ID\u3092\u6307\u5b9a\u3059\u308b -p <host_port>:<container_port> docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash docker run -it --rm --cpus 4 --memory 2g ubuntu bash docker inspect <container> | grep -i cpu \u30ed\u30fc\u30ab\u30eb\u3067\u74b0\u5883\u69cb\u7bc9 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 FROM ubuntu:latest RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ] docker run -p 8888:8888 -v ~/Desktop/ds-pyhton:/work --name my-lab <container> GPU\u74b0\u5883\u4f8b \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip && pip install \\ keras == 2 .3 \\ scipy == 1 .4.1 \\ tensorflow-gpu == 2 .1 WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ]","title":"Docker"},{"location":"docker/#docker-file","text":"1 2 3 4 5 6 7 8 9 10 11 FROM ubuntu:latest ADD copressed.tar / COPY something /new_directory/ ENV key1 value RUN apt update && apt install -y \\ aaa \\ bbb \\ ccc WORKDIR /sample_folder RUN touch something CMD [ \"executable\" , \"param1\" , \"param2\" ] docker build <directory> docker build -t <name> <directory> docker build -f <dockerfilename> <build context> \u540d\u524d\u306f\u30c9\u30c3\u30c8\u3067\u3064\u306a\u304c\u308b\u3053\u3068\u304c\u591a\u3044\u3002Dockerfile\u3068\u3044\u3046\u540d\u306e\u30d5\u30a1\u30a4\u30eb\u304c\u30d3\u30eb\u30c9\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u5165\u3063\u3066\u3044\u306a\u3044\u5834\u5408\u3002 FROM :\u30d9\u30fc\u30b9\u3068\u306a\u308b\u30a4\u30e1\u30fc\u30b8\u3092\u6c7a\u5b9a RUN :Linux\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3002RUN\u6bce\u306bLayer\u304c\u4f5c\u3089\u308c\u308b\u3002Layer\u6570\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002&&\u3067\u3064\u306a\u3052\u308b\u3002(\u30d1\u30c3\u30b1\u30fc\u30b8\u540d\u3092\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u3067)\\\u30d0\u30c3\u30af\u30b9\u30e9\u30c3\u30b7\u30e5\u3067\u6539\u884c\u3059\u308b\u3002 \u6700\u521d\u306fLayer\u3092\u7d30\u304b\u304f\u5206\u3051\u3066\u901a\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\u3002\u6700\u5f8c\u306bLayer\u3092\u6700\u5c0f\u9650\u306b\u3059\u308b\u3002 CMD :\u30b3\u30f3\u30c6\u30ca\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u30b3\u30de\u30f3\u30c9\u3092\u6307\u5b9a\u3002CMD [\"command\", \"param1\", \"paramn2\"] ex. CMD [/bin/bash], CMD\u306f\u30ec\u30a4\u30e4\u30fc\u3092\u4f5c\u3089\u306a\u3044\u3002 Docker\u30b3\u30de\u30f3\u30c9\u3067Docker Daemon\u306b\u547d\u4ee4\u3092\u51fa\u3059 * COPY: \u5358\u7d14\u306b\u30d5\u30a1\u30a4\u30eb\u3084\u30d5\u30a9\u30eb\u30c0\u3092\u30b3\u30d4\u30fc\u3059\u308b\u5834\u5408 * ADD: tar\u306e\u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u3092\u89e3\u7b54\u3059\u308b ENTRYPOINT \u306f\u4e0a\u66f8\u304d\u3067\u304d\u306a\u3044\uff08CMD\u306f\u4e0a\u66f8\u304d\u3067\u304d\u308b\uff09\u3002ENTRYPOINT\u304c\u3042\u308b\u3068\u304d\u306fCMD\u306fparams\u306e\u307f\u3092\u66f8\u304f\u3002 ENTRYPOINT\u306f\u30b3\u30f3\u30c6\u30ca\u3092\u30b3\u30de\u30f3\u30c9\u306e\u3088\u3046\u306b\u4f7f\u3044\u305f\u3044\u3068\u304d\u3002 ENV :\u74b0\u5883\u5909\u6570\u3092\u8a2d\u5b9a\u3059\u308b\u3002 WORKDIR \u5b9f\u884c\u74b0\u5883\u3092\u5909\u66f4\u3059\u308b\u3002","title":"docker file\u306e\u4f5c\u6210"},{"location":"docker/#_1","text":"docker run -it -v <host>:<container> <image bash> docker run -it -u $(id -u):$(id -g) -v ~/mouted_folder:/new_dir <image> bash -u <uder id>:<group id>: \u30e6\u30fc\u30b6ID\u3068\u30b0\u30eb\u30fc\u30d7ID\u3092\u6307\u5b9a\u3059\u308b -p <host_port>:<container_port> docker run -it -p 8888:8888 --rm jupyter/datascience-notebook bash docker run -it --rm --cpus 4 --memory 2g ubuntu bash docker inspect <container> | grep -i cpu","title":"\u30db\u30b9\u30c8\u3068\u30b3\u30f3\u30c6\u30ca\u3092\u3064\u306a\u3050"},{"location":"docker/#_2","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 FROM ubuntu:latest RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ] docker run -p 8888:8888 -v ~/Desktop/ds-pyhton:/work --name my-lab <container>","title":"\u30ed\u30fc\u30ab\u30eb\u3067\u74b0\u5883\u69cb\u7bc9"},{"location":"docker/#gpu","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM nvidia/cuda:10.1-cudnn7-runtime-ubuntu18.04 RUN apt-get update && apt-get install -y \\ sudo \\ wget \\ vim WORKDIR /opt RUN wget https://repo.continuum.io/archive/Anaconda3-2019.10-Linux-x86_64.sh && \\ sh /opt/Anaconda3-2019.10-Linux-x86_64.sh -b -p /opt/anaconda3 \\ rm -f Anaconda3-2019.10-Linux-x86_64.sh ENV PATH /opt/anaconda3/bin: $PATH RUN pip install --upgrade pip && pip install \\ keras == 2 .3 \\ scipy == 1 .4.1 \\ tensorflow-gpu == 2 .1 WORKDIR / CMD [ \"jupyter\" , \"lab\" , \"--ip=0.0.0.0\" , \"--allow-root\" , \"--LabAPP.token=''\" ]","title":"GPU\u74b0\u5883\u4f8b"},{"location":"gan/","text":"\u751f\u6210\u30e2\u30c7\u30eb \u00b6 \u751f\u6210\u30e2\u30c7\u30eb\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3069\u306e\u3088\u3046\u306b\u751f\u6210\u3055\u308c\u308b\u304b\u78ba\u7387\u30e2\u30c7\u30eb\u306e\u89b3\u70b9\u304b\u3089\u8a18\u8ff0\u3059\u308b\u3002\u6f5c\u5728\u7a7a\u9593\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3002 Note \u8b58\u5225\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(y|\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u306e\u30e9\u30d9\u30eb \\(y\\) \u306e\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\uff09 \u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u89b3\u6e2c\u3055\u308c\u308b\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\uff09 \u6f5c\u5728\u5909\u6570\u3068\u306f\u751f\u6210\u753b\u50cf\u306e\u5143\u306b\u306a\u308b\u6b21\u5143\u524a\u6e1b\u3055\u308c\u305f\u7279\u5fb4\u91cf VAE\u306f\u6f5c\u5728\u5909\u6570\u3092\u6b63\u898f\u5206\u5e03\u3068\u4eee\u5b9a \u5909\u5206\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc \u00b6 \u6f5c\u5728\u7a7a\u9593\u304c\u6b63\u898f\u5206\u5e03 GAN \u00b6 \u8b58\u5225\u5668\u306e\u8a13\u7df4 \u00b6 \u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30ebx\u3092\u53d6\u308a\u51fa\u3059 \u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5927\u5316 \\(p_G(x_i|\\textbf{z})\\) \u3092 \\(p_{r}(x)\\) \u306b\u8fd1\u3065\u3051\u3066\u3044\u304f\u305f\u3081\u306e\u6307\u6a19\u3068\u3057\u3066Kullback\u2013Leibler divergence\uff08\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306e\u8ddd\u96e2\u306e\u5c3a\u5ea6\uff09\u3068Jensen\u2013Shannon (JS)divergence\u304c\u3042\u308b\u3002GAN\u306e\u640d\u5931\u95a2\u6570\u306f\u751f\u6210\u5668\u306eJS divergence\u306e\u6700\u5c0f\u5316\uff08\u8b58\u5225\u5668\u304b\u3089\u898b\u3066\u6700\u5927\u5316\uff09\u304b\u3089\u5c0e\u304b\u308c\u308b\u3002 GAN\u306e\u640d\u5931\u95a2\u6570 \u00b6 \\[\\min_{G}\\max_{D} E_{x\\sim p_r} [\\log D(z)] + E_{x\\sim p_z} [\\log (1-D(G(z)))]\\] \u751f\u6210\u5668\u306e\u8a13\u7df4 \u00b6 \u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\uff08- \u30c7\u30fc\u30bf\u304c\u5f93\u3046\u78ba\u7387\u5206\u5e03 \\(p_{r}(x)\\) \u305d\u306e\u3082\u306e\u306f\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u751f\u6210\u5668\u306e\u78ba\u7387 \\(p_G(x_i|\\textbf{z})\\) \u3067\u8fd1\u4f3c\u3059\u308b\u3002\uff09 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u3066x*\u304c\u672c\u7269\u304b\u63a8\u5b9a \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5c0f\u5316 https://www.iangoodfellow.com/slides/2019-05-07.pdf GAN\u306e\u53ce\u675f\u6761\u4ef6 \u00b6 \u30ca\u30c3\u30b7\u30e5\u5747\u8861 -\u3000\u751f\u6210\u5668\u304c\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4e2d\u306b\u3042\u308b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30eb\u3068\u898b\u5206\u3051\u304c\u3064\u304b\u306a\u3044\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\u3059\u308b \u8b58\u5225\u6a5f\u306e\u6b63\u7b54\u7387\u304c50%\uff08\u30e9\u30f3\u30c0\u30e0\u306b\u3057\u304b\u751f\u6210\u3067\u304d\u306a\u3044\uff09 GAN\u306e\u6b20\u70b9 \u00b6 \u5b66\u7fd2\u6642\u9593\u306e\u9577\u3055 \u30e2\u30fc\u30c9\u5d29\u58ca\uff1a\u3044\u304f\u3064\u304b\u306e\u30e2\u30fc\u30c9\u304c\u751f\u6210\u3055\u308c\u308b\u30b5\u30f3\u30d7\u30eb\u306b\u542b\u307e\u308c\u306a\u304f\u306a\u308b \u751f\u6210\u5668\u3068\u8b58\u5225\u5668\u306e\u30d0\u30e9\u30f3\u30b9\uff1a\u8b58\u5225\u5668\u304c\u5f37\u3059\u304e\u308b\uff1d\uff1e\u52fe\u914d\u6d88\u5931\u3001\u8b58\u5225\u5668\u304c\u5b66\u7fd2\u3057\u306a\u3044\uff1d\uff1e\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u4e0a\u304c\u3089\u306a\u3044 \u751f\u6210\u753b\u50cf\u306b\u7d30\u304b\u306a\u30ce\u30a4\u30ba\u304c\u5165\u308b \u6bd4\u8f03\u53ef\u80fd\u306a\u578b\u306e\u30c7\u30fc\u30bf\u3067\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044 \u640d\u5931\u95a2\u6570\u306e\u5024\u3068\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u5fc5\u305a\u3057\u3082\u76f8\u95a2\u3057\u306a\u3044\u3002 \u6539\u5584\u6cd5 \u00b6 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u6df1\u304f\u3059\u308b\u3002\uff08Progressive GAN\uff09 \u30b2\u30fc\u30e0\u306e\u8a2d\u5b9a\u3092\u5909\u3048\u308b\u3002 Min-Max\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 \u975e\u98fd\u548c\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 WassertsteinGAN \u30cf\u30c3\u30af \u00b6 \u5165\u529b\u306e\u6b63\u898f\u5316 \u52fe\u914d\u306e\u5236\u7d04 \u8b58\u5225\u5668\u3092\u3088\u308a\u591a\u304f\u8a13\u7df4\u3059\u308b \u758e\u306a\u52fe\u914d\u3092\u907f\u3051\u308b \u30bd\u30d5\u30c8\u306a\u3042\u308b\u3044\u306f\u30ce\u30a4\u30ba\u4ed8\u304d\u306e\u30e9\u30d9\u30eb\u306b\u5207\u308a\u66ff\u3048\u308b \u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , z ): img = self . model ( z ) img = img . view ( img . size ( 0 ), * img_shape ) return img \u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . model = nn . Sequential ( nn . Linear ( int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 256 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 256 , 1 ), nn . Sigmoid (), ) def forward ( self , img ): img_flat = img . view ( img . size ( 0 ), - 1 ) validity = self . model ( img_flat ) return validity GAN\u306e\u5b66\u7fd2\u306e1 step \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizer_G = optim . Adam ( netG . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) optimizer_D = optim . Adam ( netD . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) shape = ( batch_size , 1 , 1 , 1 ) labels_real = torch . ones ( shape ) . to ( device ) labels_fake = torch . zeros ( shape ) . to ( device ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss DCGAN (Deep Convolutional GAN) \u00b6 \u30ce\u30a4\u30ba\u30d9\u30af\u30c8\u30eb\u3092\u5165\u529b\u3057\u3066\u3001\u5e45\u3068\u9ad8\u3055\u3092\u62e1\u5927\u3057\u3064\u3064\u3001\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u6e1b\u3089\u3057\u3066\u3044\u304f\u3001\u6700\u7d42\u7684\u306b\uff08H x W x C\uff09\u3092\u51fa\u529b\u3002 G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084 \u8ee2\u7f6e\u7573\u307f\u8fbc\u307f \u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528 \u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Generator ( nn . Module ): def __init__ ( self , z_dim = 100 , ngf = 128 , nc = 1 ): super () . __init__ () self . convt1 = self . conv_trans_layers ( z_dim , 4 * ngf , 3 , 1 , 0 ) self . convt2 = self . conv_trans_layers ( 4 * ngf , 2 * ngf , 3 , 2 , 0 ) self . convt3 = self . conv_trans_layers ( 2 * ngf , ngf , 4 , 2 , 1 ) self . convt4 = nn . Sequential ( nn . ConvTranspose2d ( ngf , nc , 4 , 2 , 1 ), nn . Tanh () ) @staticmethod def conv_trans_layers ( in_channels , out_channels , kernel_size , stride , padding ): net = nn . Sequential ( nn . ConvTranspose2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ) ) return net def forward ( self , x ): out = self . convt1 ( x ) out = self . convt2 ( out ) out = self . convt3 ( out ) out = self . convt4 ( out ) return out \u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Discrimnator ( nn . Module ): def __init__ ( self , nc = 1 , ndf = 128 ): super () . __init__ () self . conv1 = self . conv_layers ( nc , ndf , has_batch_norm = False ) self . conv2 = self . conv_layers ( ndf , 2 * ndf ) self . conv3 = self . conv_layers ( 2 * ndf , 4 * ndf , 3 , 2 , 0 ) self . conv4 = nn . Sequential ( nn . Conv2d ( 4 * ndf , 1 , 3 , 1 , 0 ), nn . Sigmoid () ) @staticmethod def conv_layers ( in_channels , out_channels , kernel_size = 4 , stride = 2 , padding = 1 , has_batch_norm = True ): layers = [ nn . Conv2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ) ] if has_batch_norm : layers . append ( nn . BatchNorm2d ( out_channels )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) net = nn . Sequential ( * layers ) return net def forward ( self , x ): out = self . conv1 ( x ) out = self . conv2 ( out ) out = self . conv3 ( out ) out = self . conv4 ( out ) return out Conditional GAN \u00b6 \u30ce\u30a4\u30ba\u3084\u753b\u50cf\u306b\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\u3059\u308b\u3053\u3068\u3067\u7279\u5b9a\u306e\u753b\u50cf\u3092\u751f\u6210 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_noise_with_label ( noise , labels , device , n_class = 10 ): one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) . to ( device ) concat_noise = torch . cat (( noise , one_hot_vec ), dim = 1 ) return concat_noise def get_img_with_label ( imgs , labels , device , n_class = 10 ): B , _ , H , W = imgs . size () one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) one_hot_vec = one_hot_vec . expand ( B , n_class , H , W ) . to ( device ) concat_img = torch . cat (( imgs , one_hot_vec ), dim = 1 ) return concat_img def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # Get the noise with label noise_with_label = get_noise_with_label ( noise , labels , device ) # Get the real images with label real_imgs_with_label = get_img_with_label ( real_imgs , labels , device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss Wassersteing GAN \u00b6 \u8a13\u7df4\u306e\u5b89\u5b9a\u5316\u3068\u5224\u65ad\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u3001\u640d\u5931\u95a2\u6570\u306b Wasserstein\u640d\u5931 \u3092\u5c0e\u5165\u3002\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002 \u8b58\u5225\u5668\u306b 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3057\u305f\u3002 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3059\u305f\u3081\u306bWeight\u3092\u3042\u308b\u7bc4\u56f2\u3067\u30af\u30ea\u30c3\u30d7\u3057\u3001\u52fe\u914d\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u5247\u5316\u9805\u3092\u5897\u3084\u3059\u3002 \u8b58\u5225\u5668\u3092\u591a\u304f\u8a13\u7df4\u3059\u308b\u3002 optimizer\u306b RMSProp \u3092\u4f7f\u3046\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) for p in netD . parameters (): p . data . clamp_ ( opt . c_lower , opt . c_upper ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss Wassersteing GAN (Gradient penalty) \u00b6 -\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092False\u306b\u3059\u308b\u3002 -RMSprop\u306bweight_decay = 1e-4\u3092\u5165\u308c\u308b. 1 2 3 4 5 6 7 8 9 10 11 def gradient_penalty ( real_imgs , fake_img , gp_weight , netD , device ): batch_size = real_imgs . size ()[ 0 ] alpha = torch . randn ( batch_size , 1 , 1 , 1 ) alpha = alpha . expand_as ( real_imgs ) . to ( device ) interpolated_imgs = ( alpha * real_imgs . data + ( 1 - alpha ) * fake_img . data ) . requires_grad_ () grad_outputs = torch . autograd . grad ( inyerpolated_out , interpolated_imgs , grad_outputs = grad_outputs , retain_graph = True )[ 0 ] gradients = gradients . view ( batch_size , - 1 ) gradients_nrom = torch . sqrt ( torch . sum ( gradients ** 2 , dim = 1 ) + eps ) gp = gp_weight * (( gradients_norm - 1 ) ** 2 ) . mean () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr , weight_decay = 1e-4 ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr , weight_decay = 1e-4 ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # for p in netD.parameters(): # p.data.clamp_(opt.c_lower, opt.c_upper) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) gp_loss = gradient_penalty ( real_imgs , fake_imgs , opt . gp_weight , netD , device ) d_loss = real_loss + fake_loss + gp_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss Cycke GAN \u00b6 \u57fa\u672c\u7684\u306bEncoder-Decoder\u69cb\u9020 Instance Normalization : \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3067\u306f\u753b\u50cf\u5168\u4f53\u306e\u307f\u3067\u6b63\u898f\u5316\u3092\u884c\u3046\u3002\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba = 1\u306eBN\u3068\u540c\u3058 \u30ea\u30d5\u30ec\u30af\u30b7\u30e7\u30f3\u30d1\u30c3\u30c9\uff1a\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u3068\u306f\u7570\u306a\u308a\u30a8\u30c3\u30b8\u90e8\u5206\u3092\u7af6\u6cf3\u9762\u3068\u3057\u3066\u53cd\u5c04\u3055\u305b\u305f\u30d1\u30c7\u30a3\u30f3\u30b0\u65b9\u6cd5\u3002\u6298\u308a\u8fd4\u3057\u3066\u3064\u306a\u3052\u308b\u3053\u3068\u3067\u753b\u50cf\u306e\u4e2d\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u30a8\u30c3\u30b8\u5468\u8fba\u3067\u4fdd\u3064\u3002 \u30b5\u30a4\u30af\u30eb\u4e00\u8cab\u6027\u640d\u5931 \u540c\u4e00\u6027\u640d\u5931 Replay Buffer","title":"GAN basis"},{"location":"gan/#_1","text":"\u751f\u6210\u30e2\u30c7\u30eb\u3068\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3069\u306e\u3088\u3046\u306b\u751f\u6210\u3055\u308c\u308b\u304b\u78ba\u7387\u30e2\u30c7\u30eb\u306e\u89b3\u70b9\u304b\u3089\u8a18\u8ff0\u3059\u308b\u3002\u6f5c\u5728\u7a7a\u9593\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u65b0\u3057\u3044\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3002 Note \u8b58\u5225\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(y|\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u4e0e\u3048\u3089\u308c\u305f\u3068\u304d\u306e\u30e9\u30d9\u30eb \\(y\\) \u306e\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\uff09 \u751f\u6210\u30e2\u30c7\u30ea\u30f3\u30b0 \uff1a \\(p(\\textbf{x})\\) \uff08\u89b3\u6e2c \\(\\textbf{x}\\) \u304c\u89b3\u6e2c\u3055\u308c\u308b\u78ba\u7387\uff09\u3092\u63a8\u5b9a\u3059\u308b\u3002\uff08\u6559\u5e2b\u306a\u3057\u5b66\u7fd2\uff09 \u6f5c\u5728\u5909\u6570\u3068\u306f\u751f\u6210\u753b\u50cf\u306e\u5143\u306b\u306a\u308b\u6b21\u5143\u524a\u6e1b\u3055\u308c\u305f\u7279\u5fb4\u91cf VAE\u306f\u6f5c\u5728\u5909\u6570\u3092\u6b63\u898f\u5206\u5e03\u3068\u4eee\u5b9a","title":"\u751f\u6210\u30e2\u30c7\u30eb"},{"location":"gan/#_2","text":"\u6f5c\u5728\u7a7a\u9593\u304c\u6b63\u898f\u5206\u5e03","title":"\u5909\u5206\u30aa\u30fc\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc"},{"location":"gan/#gan","text":"","title":"GAN"},{"location":"gan/#_3","text":"\u8a13\u7df4\u30c7\u30fc\u30bf\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u306b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30ebx\u3092\u53d6\u308a\u51fa\u3059 \u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5927\u5316 \\(p_G(x_i|\\textbf{z})\\) \u3092 \\(p_{r}(x)\\) \u306b\u8fd1\u3065\u3051\u3066\u3044\u304f\u305f\u3081\u306e\u6307\u6a19\u3068\u3057\u3066Kullback\u2013Leibler divergence\uff08\u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306e\u8ddd\u96e2\u306e\u5c3a\u5ea6\uff09\u3068Jensen\u2013Shannon (JS)divergence\u304c\u3042\u308b\u3002GAN\u306e\u640d\u5931\u95a2\u6570\u306f\u751f\u6210\u5668\u306eJS divergence\u306e\u6700\u5c0f\u5316\uff08\u8b58\u5225\u5668\u304b\u3089\u898b\u3066\u6700\u5927\u5316\uff09\u304b\u3089\u5c0e\u304b\u308c\u308b\u3002","title":"\u8b58\u5225\u5668\u306e\u8a13\u7df4"},{"location":"gan/#gan_1","text":"\\[\\min_{G}\\max_{D} E_{x\\sim p_r} [\\log D(z)] + E_{x\\sim p_z} [\\log (1-D(G(z)))]\\]","title":"GAN\u306e\u640d\u5931\u95a2\u6570"},{"location":"gan/#_4","text":"\u65b0\u3057\u3044\u4e71\u6570\u304b\u3089\u751f\u6210\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\uff08- \u30c7\u30fc\u30bf\u304c\u5f93\u3046\u78ba\u7387\u5206\u5e03 \\(p_{r}(x)\\) \u305d\u306e\u3082\u306e\u306f\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u751f\u6210\u5668\u306e\u78ba\u7387 \\(p_G(x_i|\\textbf{z})\\) \u3067\u8fd1\u4f3c\u3059\u308b\u3002\uff09 \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u3066x*\u304c\u672c\u7269\u304b\u63a8\u5b9a \u8b58\u5225\u5668\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066x\u3068x*\u3092\u5206\u985e\u3057\u3001\u8aa4\u5dee\u9006\u4f1d\u642c\u3057\u3066\u5206\u985e\u8aa4\u5dee\u3092\u6700\u5c0f\u5316 https://www.iangoodfellow.com/slides/2019-05-07.pdf","title":"\u751f\u6210\u5668\u306e\u8a13\u7df4"},{"location":"gan/#gan_2","text":"\u30ca\u30c3\u30b7\u30e5\u5747\u8861 -\u3000\u751f\u6210\u5668\u304c\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4e2d\u306b\u3042\u308b\u672c\u7269\u306e\u30b5\u30f3\u30d7\u30eb\u3068\u898b\u5206\u3051\u304c\u3064\u304b\u306a\u3044\u507d\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u751f\u6210\u3059\u308b \u8b58\u5225\u6a5f\u306e\u6b63\u7b54\u7387\u304c50%\uff08\u30e9\u30f3\u30c0\u30e0\u306b\u3057\u304b\u751f\u6210\u3067\u304d\u306a\u3044\uff09","title":"GAN\u306e\u53ce\u675f\u6761\u4ef6"},{"location":"gan/#gan_3","text":"\u5b66\u7fd2\u6642\u9593\u306e\u9577\u3055 \u30e2\u30fc\u30c9\u5d29\u58ca\uff1a\u3044\u304f\u3064\u304b\u306e\u30e2\u30fc\u30c9\u304c\u751f\u6210\u3055\u308c\u308b\u30b5\u30f3\u30d7\u30eb\u306b\u542b\u307e\u308c\u306a\u304f\u306a\u308b \u751f\u6210\u5668\u3068\u8b58\u5225\u5668\u306e\u30d0\u30e9\u30f3\u30b9\uff1a\u8b58\u5225\u5668\u304c\u5f37\u3059\u304e\u308b\uff1d\uff1e\u52fe\u914d\u6d88\u5931\u3001\u8b58\u5225\u5668\u304c\u5b66\u7fd2\u3057\u306a\u3044\uff1d\uff1e\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u4e0a\u304c\u3089\u306a\u3044 \u751f\u6210\u753b\u50cf\u306b\u7d30\u304b\u306a\u30ce\u30a4\u30ba\u304c\u5165\u308b \u6bd4\u8f03\u53ef\u80fd\u306a\u578b\u306e\u30c7\u30fc\u30bf\u3067\u306a\u3044\u3068\u5b66\u7fd2\u3067\u304d\u306a\u3044 \u640d\u5931\u95a2\u6570\u306e\u5024\u3068\u753b\u50cf\u306e\u30af\u30aa\u30ea\u30c6\u30a3\u304c\u5fc5\u305a\u3057\u3082\u76f8\u95a2\u3057\u306a\u3044\u3002","title":"GAN\u306e\u6b20\u70b9"},{"location":"gan/#_5","text":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u6df1\u304f\u3059\u308b\u3002\uff08Progressive GAN\uff09 \u30b2\u30fc\u30e0\u306e\u8a2d\u5b9a\u3092\u5909\u3048\u308b\u3002 Min-Max\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 \u975e\u98fd\u548c\u65b9\u5f0f\u3068\u505c\u6b62\u57fa\u6e96 WassertsteinGAN","title":"\u6539\u5584\u6cd5"},{"location":"gan/#_6","text":"\u5165\u529b\u306e\u6b63\u898f\u5316 \u52fe\u914d\u306e\u5236\u7d04 \u8b58\u5225\u5668\u3092\u3088\u308a\u591a\u304f\u8a13\u7df4\u3059\u308b \u758e\u306a\u52fe\u914d\u3092\u907f\u3051\u308b \u30bd\u30d5\u30c8\u306a\u3042\u308b\u3044\u306f\u30ce\u30a4\u30ba\u4ed8\u304d\u306e\u30e9\u30d9\u30eb\u306b\u5207\u308a\u66ff\u3048\u308b","title":"\u30cf\u30c3\u30af"},{"location":"gan/#_7","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Generator ( nn . Module ): def __init__ ( self ): super ( Generator , self ) . __init__ () def block ( in_feat , out_feat , normalize = True ): layers = [ nn . Linear ( in_feat , out_feat )] if normalize : layers . append ( nn . BatchNorm1d ( out_feat , 0.8 )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) return layers self . model = nn . Sequential ( * block ( opt . latent_dim , 128 , normalize = False ), * block ( 128 , 256 ), * block ( 256 , 512 ), * block ( 512 , 1024 ), nn . Linear ( 1024 , int ( np . prod ( img_shape ))), nn . Tanh () ) def forward ( self , z ): img = self . model ( z ) img = img . view ( img . size ( 0 ), * img_shape ) return img","title":"\u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#_8","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Discriminator ( nn . Module ): def __init__ ( self ): super ( Discriminator , self ) . __init__ () self . model = nn . Sequential ( nn . Linear ( int ( np . prod ( img_shape )), 512 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 512 , 256 ), nn . LeakyReLU ( 0.2 , inplace = True ), nn . Linear ( 256 , 1 ), nn . Sigmoid (), ) def forward ( self , img ): img_flat = img . view ( img . size ( 0 ), - 1 ) validity = self . model ( img_flat ) return validity","title":"\u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#gan1-step","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizer_G = optim . Adam ( netG . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) optimizer_D = optim . Adam ( netD . parameters (), lr = opt . lr , betas = ( opt . beta1 , 0.999 ), weight_decay = 1e-5 ) shape = ( batch_size , 1 , 1 , 1 ) labels_real = torch . ones ( shape ) . to ( device ) labels_fake = torch . zeros ( shape ) . to ( device ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"GAN\u306e\u5b66\u7fd2\u306e1 step"},{"location":"gan/#dcgan-deep-convolutional-gan","text":"\u30ce\u30a4\u30ba\u30d9\u30af\u30c8\u30eb\u3092\u5165\u529b\u3057\u3066\u3001\u5e45\u3068\u9ad8\u3055\u3092\u62e1\u5927\u3057\u3064\u3064\u3001\u30c1\u30e3\u30cd\u30eb\u6570\u3092\u6e1b\u3089\u3057\u3066\u3044\u304f\u3001\u6700\u7d42\u7684\u306b\uff08H x W x C\uff09\u3092\u51fa\u529b\u3002 G\u30e2\u30c7\u30eb\u3068D\u30e2\u30c7\u30eb\u306e\u5185\u90e8\u306b\u30d7\u30fc\u30ea\u30f3\u30b0\u5c64\u3092\u4f7f\u308f\u306a\u3044\u7573\u307f\u8fbc\u307f\u3084 \u8ee2\u7f6e\u7573\u307f\u8fbc\u307f \u3092\u5229\u7528 \u5168\u7d50\u5408\u5c64\u306f\u5229\u7528\u3057\u306a\u3044\uff08\u30d7\u30fc\u30ea\u30f3\u30b0\u51e6\u7406\u306b\u3088\u308b\u7d30\u304b\u306a\u60c5\u5831\u304c\u6b20\u843d\u3059\u308b\u306e\u3092\u9632\u3050\u305f\u3081\u3002\uff09 \u30d0\u30c3\u30c1\u6b63\u898f\u5316 \u3092\u5229\u7528 G\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528 D\u30e2\u30c7\u30eb\u306e\u6d3b\u6027\u5316\u95a2\u6570\u3092\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002\u306b\u4ee3\u7528","title":"DCGAN (Deep Convolutional GAN)"},{"location":"gan/#_9","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Generator ( nn . Module ): def __init__ ( self , z_dim = 100 , ngf = 128 , nc = 1 ): super () . __init__ () self . convt1 = self . conv_trans_layers ( z_dim , 4 * ngf , 3 , 1 , 0 ) self . convt2 = self . conv_trans_layers ( 4 * ngf , 2 * ngf , 3 , 2 , 0 ) self . convt3 = self . conv_trans_layers ( 2 * ngf , ngf , 4 , 2 , 1 ) self . convt4 = nn . Sequential ( nn . ConvTranspose2d ( ngf , nc , 4 , 2 , 1 ), nn . Tanh () ) @staticmethod def conv_trans_layers ( in_channels , out_channels , kernel_size , stride , padding ): net = nn . Sequential ( nn . ConvTranspose2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ) ) return net def forward ( self , x ): out = self . convt1 ( x ) out = self . convt2 ( out ) out = self . convt3 ( out ) out = self . convt4 ( out ) return out","title":"\u8b58\u5225\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#_10","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Discrimnator ( nn . Module ): def __init__ ( self , nc = 1 , ndf = 128 ): super () . __init__ () self . conv1 = self . conv_layers ( nc , ndf , has_batch_norm = False ) self . conv2 = self . conv_layers ( ndf , 2 * ndf ) self . conv3 = self . conv_layers ( 2 * ndf , 4 * ndf , 3 , 2 , 0 ) self . conv4 = nn . Sequential ( nn . Conv2d ( 4 * ndf , 1 , 3 , 1 , 0 ), nn . Sigmoid () ) @staticmethod def conv_layers ( in_channels , out_channels , kernel_size = 4 , stride = 2 , padding = 1 , has_batch_norm = True ): layers = [ nn . Conv2d ( in_channels , out_channels , kernel_size , stride , padding , bias = False ) ] if has_batch_norm : layers . append ( nn . BatchNorm2d ( out_channels )) layers . append ( nn . LeakyReLU ( 0.2 , inplace = True )) net = nn . Sequential ( * layers ) return net def forward ( self , x ): out = self . conv1 ( x ) out = self . conv2 ( out ) out = self . conv3 ( out ) out = self . conv4 ( out ) return out","title":"\u751f\u6210\u5668\u30af\u30e9\u30b9\u306e\u5b9f\u88c5"},{"location":"gan/#conditional-gan","text":"\u30ce\u30a4\u30ba\u3084\u753b\u50cf\u306b\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e\u3059\u308b\u3053\u3068\u3067\u7279\u5b9a\u306e\u753b\u50cf\u3092\u751f\u6210 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_noise_with_label ( noise , labels , device , n_class = 10 ): one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) . to ( device ) concat_noise = torch . cat (( noise , one_hot_vec ), dim = 1 ) return concat_noise def get_img_with_label ( imgs , labels , device , n_class = 10 ): B , _ , H , W = imgs . size () one_hot_vec = torch . nn . functional . one_hot ( labels , num_classes = n_class ) . view ( - 1 , n_class , 1 , 1 ) one_hot_vec = one_hot_vec . expand ( B , n_class , H , W ) . to ( device ) concat_img = torch . cat (( imgs , one_hot_vec ), dim = 1 ) return concat_img def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # Get the noise with label noise_with_label = get_noise_with_label ( noise , labels , device ) # Get the real images with label real_imgs_with_label = get_img_with_label ( real_imgs , labels , device ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = BCELoss ()( out_real , labels_valid ) fake_loss = BCELoss ()( out_fake , labels_fake ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"Conditional GAN"},{"location":"gan/#wassersteing-gan","text":"\u8a13\u7df4\u306e\u5b89\u5b9a\u5316\u3068\u5224\u65ad\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b\u3001\u640d\u5931\u95a2\u6570\u306b Wasserstein\u640d\u5931 \u3092\u5c0e\u5165\u3002\u7279\u306b\u8b58\u5225\u5668\u306e\u640d\u5931\u95a2\u6570\u306f Earth Mover's distance \u3068\u547c\u3070\u308c\u308b\u3002 \u8b58\u5225\u5668\u306b 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3057\u305f\u3002 1-Lipschitz\u9023\u7d9a \u3092\u8ab2\u3059\u305f\u3081\u306bWeight\u3092\u3042\u308b\u7bc4\u56f2\u3067\u30af\u30ea\u30c3\u30d7\u3057\u3001\u52fe\u914d\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u6b63\u5247\u5316\u9805\u3092\u5897\u3084\u3059\u3002 \u8b58\u5225\u5668\u3092\u591a\u304f\u8a13\u7df4\u3059\u308b\u3002 optimizer\u306b RMSProp \u3092\u4f7f\u3046\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) for p in netD . parameters (): p . data . clamp_ ( opt . c_lower , opt . c_upper ) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) d_loss = real_loss + fake_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"Wassersteing GAN"},{"location":"gan/#wassersteing-gan-gradient-penalty","text":"-\u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3092False\u306b\u3059\u308b\u3002 -RMSprop\u306bweight_decay = 1e-4\u3092\u5165\u308c\u308b. 1 2 3 4 5 6 7 8 9 10 11 def gradient_penalty ( real_imgs , fake_img , gp_weight , netD , device ): batch_size = real_imgs . size ()[ 0 ] alpha = torch . randn ( batch_size , 1 , 1 , 1 ) alpha = alpha . expand_as ( real_imgs ) . to ( device ) interpolated_imgs = ( alpha * real_imgs . data + ( 1 - alpha ) * fake_img . data ) . requires_grad_ () grad_outputs = torch . autograd . grad ( inyerpolated_out , interpolated_imgs , grad_outputs = grad_outputs , retain_graph = True )[ 0 ] gradients = gradients . view ( batch_size , - 1 ) gradients_nrom = torch . sqrt ( torch . sum ( gradients ** 2 , dim = 1 ) + eps ) gp = gp_weight * (( gradients_norm - 1 ) ** 2 ) . mean () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 optimizerG = optim . RMSprop ( netG . parameters (), lr = opt . lr , weight_decay = 1e-4 ) optimizerD = optim . RMSprop ( netD . parameters (), lr = opt . lr , weight_decay = 1e-4 ) def train_one_step ( real_imgs , labels_valid , labels_fake ): # Sample noise as generator input noise = torch . randn ( batch_size , opt . z_dim , 1 , 1 ) . to ( device ) # for p in netD.parameters(): # p.data.clamp_(opt.c_lower, opt.c_upper) \"\"\"Train Discriminator\"\"\" optimizer_D . zero_grad () # Generate a batch of images gen_imgs = generator ( noise ) # Measure discriminator's ability to classify real from generated samples out_real = discriminator ( real_imgs ) out_fake = discriminator ( gen_imgs . detach ()) real_loss = - torch . mean ( output ) fake_loss = torch . mean ( output ) gp_loss = gradient_penalty ( real_imgs , fake_imgs , opt . gp_weight , netD , device ) d_loss = real_loss + fake_loss + gp_loss d_loss . backward () optimizer_D . step () \"\"\"Train Generator\"\"\" if i % opt . n_critic == 0 : optimizer_G . zero_grad () # Loss measures generator's ability to fool the discriminator g_loss = BCELoss ()( discriminator ( gen_imgs ), labels_valid ) g_loss . backward () optimizer_G . step () return g_loss , d_loss","title":"Wassersteing GAN (Gradient penalty)"},{"location":"gan/#cycke-gan","text":"\u57fa\u672c\u7684\u306bEncoder-Decoder\u69cb\u9020 Instance Normalization : \u30d0\u30c3\u30c1\u6b63\u898f\u5316\u3067\u306f\u753b\u50cf\u5168\u4f53\u306e\u307f\u3067\u6b63\u898f\u5316\u3092\u884c\u3046\u3002\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba = 1\u306eBN\u3068\u540c\u3058 \u30ea\u30d5\u30ec\u30af\u30b7\u30e7\u30f3\u30d1\u30c3\u30c9\uff1a\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u3068\u306f\u7570\u306a\u308a\u30a8\u30c3\u30b8\u90e8\u5206\u3092\u7af6\u6cf3\u9762\u3068\u3057\u3066\u53cd\u5c04\u3055\u305b\u305f\u30d1\u30c7\u30a3\u30f3\u30b0\u65b9\u6cd5\u3002\u6298\u308a\u8fd4\u3057\u3066\u3064\u306a\u3052\u308b\u3053\u3068\u3067\u753b\u50cf\u306e\u4e2d\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u30a8\u30c3\u30b8\u5468\u8fba\u3067\u4fdd\u3064\u3002 \u30b5\u30a4\u30af\u30eb\u4e00\u8cab\u6027\u640d\u5931 \u540c\u4e00\u6027\u640d\u5931 Replay Buffer","title":"Cycke GAN"},{"location":"gif/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \"\"\"Utility for creating a GIF. Creative Applications of Deep Learning w/ Tensorflow. Kadenze, Inc. Copyright Parag K. Mital, June 2016. \"\"\" import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation def build_gif ( imgs , interval = 0.1 , dpi = 72 , save_gif = True , saveto = 'animation.gif' , show_gif = False , cmap = None ): \"\"\"Take an array or list of images and create a GIF. Parameters ---------- imgs : np.ndarray or list List of images to create a GIF of interval : float, optional Spacing in seconds between successive images. dpi : int, optional Dots per inch. save_gif : bool, optional Whether or not to save the GIF. saveto : str, optional Filename of GIF to save. show_gif : bool, optional Whether or not to render the GIF using plt. cmap : None, optional Optional colormap to apply to the images. Returns ------- ani : matplotlib.animation.ArtistAnimation The artist animation from matplotlib. Likely not useful. \"\"\" imgs = np . asarray ( imgs ) h , w , * c = imgs [ 0 ] . shape fig , ax = plt . subplots ( figsize = ( np . round ( w / dpi ), np . round ( h / dpi ))) fig . subplots_adjust ( bottom = 0 ) fig . subplots_adjust ( top = 1 ) fig . subplots_adjust ( right = 1 ) fig . subplots_adjust ( left = 0 ) ax . set_axis_off () if cmap is not None : axs = list ( map ( lambda x : [ ax . imshow ( x , cmap = cmap )], imgs )) else : axs = list ( map ( lambda x : [ ax . imshow ( x )], imgs )) ani = animation . ArtistAnimation ( fig , axs , interval = interval * 1000 , repeat_delay = 0 , blit = True ) if save_gif : ani . save ( saveto , writer = 'imagemagick' , dpi = dpi ) if show_gif : plt . show () return ani","title":"Gif"},{"location":"git/","text":"Git \u00b6 Git\u3068\u306f \u00b6 Git\u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306e1\u3064\uff08\u5206\u6563\u7ba1\u7406\u65b9\u5f0f\uff09\u3002\u7279\u5b9a\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5dee\u5206\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3001\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3059\u308b\u3002 \u7528\u8a9e \u00b6 \u30ea\u30dd\u30b8\u30c8\u30ea\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5168\u3066\u306e\u30d5\u30a1\u30a4\u30eb\uff08\u5909\u66f4\u5c65\u6b74\u3084\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3082\u3059\u3079\u3066\uff09\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 \u30b3\u30df\u30c3\u30c8\uff1a\u89aa\u5b50\u95a2\u4fc2\u3092\u6301\u3064\u30b0\u30e9\u30d5\u3002\u30d5\u30a1\u30a4\u30eb\u306e\u72b6\u614b\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3053\u3068\u3002Working directory => staging area \uff08\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3082\u547c\u3070\u308c\u308b\uff09=> \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b3\u30df\u30c3\u30c8 \u30d6\u30e9\u30f3\u30c1\uff1a\u30b3\u30df\u30c3\u30c8\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002HEAD\u306f\u4eca\u81ea\u5206\u304c\u4f5c\u696d\u3057\u3066\u3044\u308b\u30d6\u30e9\u30f3\u30c1\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002\u30b3\u30df\u30c3\u30c8\u524d\u306b\u5206\u5c90\u3055\u305b\u308b\u3002\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u3057\u3066\u30de\u30fc\u30b8\u3055\u305b\u308b\u3002 Github \u00b6 \u30cf\u30a4\u30d5\u30f3\u3067\u540d\u524d\u3092\u533a\u5207\u308b\u306e\u304c\u4e00\u822c\u7684 ssh\u3067\u306e\u8a8d\u8a3c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 \u57fa\u672c\u7684\u306a\u6d41\u308c \u00b6 \u30ed\u30fc\u30ab\u30eb\u306b\u30e6\u30fc\u30b6\u60c5\u5831\u3092\u30bb\u30c3\u30c8\u78ba\u8a8d git config --global user.name \"<username, github\u306eusername>\" git config --global user.email \"<email>\" git config --global --list git config --global --replace-all core.pager \"less -F -X\" git config --global pull.rebase true \u30ea\u30dd\u30b8\u30c8\u30ea\u3092clone git clone <remote_repo_url> git remote -v : \u767b\u9332\u3057\u3066\u3042\u308b\u30ea\u30e2\u30fc\u30c8\u30ea\u30dc\u3092\u78ba\u8a8d git clone \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f origin \u304c\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b\u7d10\u4ed8\u3044\u3066\u3044\u308b \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\u6307\u5b9a\uff1a git clone https://github.com/user/{\u30ea\u30dd\u30b8\u30c8\u30ea\u540d}.git {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u30d6\u30e9\u30f3\u30c1\u3092\u6307\u5b9a\u3059\u308b\u5834\u5408 git clone -b [\u30d6\u30e9\u30f3\u30c1\u540d][\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30a2\u30c9\u30ec\u30b9] {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\uff08\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308b\uff09 git branch \u30d6\u30e9\u30f3\u30c1\u4e00\u89a7 git branch -a \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u542b\u3080\u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u306e\u4e00\u89a7 git branch <branch name> branch name\u3068\u3044\u3046branch\u3092\u4f5c\u6210, HEAD\u306e\u30dd\u30a4\u30f3\u30bf\u5148\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u3002 git branch -m <old name> <new name> git branch -d <branch-name> \u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664 git checkout <branch name> branch name\u306b\u79fb\u52d5\u3002HEAD\u30dd\u30a4\u30f3\u30bf\u306e\u5207\u308a\u66ff\u3048 git checkout -b <branch name> \uff08\u5b9f\u7528\u4e0a\uff09branch \u4f5c\u6210\u3057\u3066branch name\u306b\u79fb\u52d5 \u30d6\u30e9\u30f3\u30c1\u540d\u306f\u30cf\u30a4\u30d5\u30f3\u3067\u533a\u5207\u308b \u30ed\u30fc\u30ab\u30eb\u30d5\u30a1\u30a4\u30eb\uff08at working directory, working tree\uff09\u3092\u66f4\u65b0\u3057\u3066Staging\u30a8\u30ea\u30a2\u306b\u3042\u3052\u308b git diff --<filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD --<filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD --<filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ --<filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main --<filename> git add <file name> git add . git status \u72b6\u6cc1\u3092\u78ba\u8a8d \u30b3\u30df\u30c3\u30c8\u3059\u308b git commit -m \"commit message\" git tag <tagname> git tag --list git log --oneline --all --graph \u30b3\u30df\u30c3\u30c8\u3057\u305f\u5c65\u6b74\u3092\u78ba\u8a8d \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092pull\u3057\u3066\u304b\u3089\u3001\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306bpush\u3002pull\u306ffetch + merge git pull <remote ref> <branch name> git pull --rebase <remote ref> <branch name> : pull\u3059\u308b\u3068\u304d\u306brebase\u3059\u308b git pull origin main git push <remote ref> <branch name> git push origin new-branch git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b OSS\u306a\u3069\u306e\u5834\u5408\u3067\u306f\u3001\u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3059\u308b\u3002 git remote add upstream <repourl> git pull upstream main git push origin new-branch git\u7ba1\u7406\u5bfe\u8c61\u304b\u3089\u5916\u3059\u3002 git rm -r --cached push\u3057\u305f\u30d6\u30e9\u30f3\u30c1\u3092pull request\u3092\u4f5c\u3063\u3066\u30ea\u30e2\u30fc\u30c8\u306emain\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8 Github\u3067\u4f5c\u696d\u3002 pull request \u3092\u30af\u30ea\u30c3\u30af => base (main)\u3068 compare (new branch)\u3092\u6307\u5b9a,\u81ea\u5206\u306e\u30ea\u30dd\u304b\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u304b\u3092\u78ba\u8a8d => create pull request => Merge pull request \u3092\u62bc\u3059\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306emain\u306e\u53cd\u6620\u3092\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u306emain\u306b\u53cd\u6620\uff08pull\uff09 git checkout main git pull origin main \u4e0d\u8981\u306a\u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664\u3059\u308b\u3002 git branch -d <branch-name> Gighub\u3067 branches \u304b\u3089\u524a\u9664 \u57fa\u672c\u64cd\u4f5c \u00b6 \u30b9\u30af\u30e9\u30c3\u30c1\u304b\u3089\u4f5c\u6210\uff08.git\u306e\u4f5c\u6210\uff09 git init <project-name> .git \u306e\u524a\u9664 rm -rf .git \u65e2\u5b58\u306e\u30d5\u30a9\u30eb\u30c0\u3092git\u30ea\u30dd\u306b\u3059\u308b\u3002 git init \u3000\u305d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u304b\u3089.git\u306e\u4f5c\u6210 \u65e2\u5b58\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u81ea\u5206\u306e\u30ea\u30dd\u3092\u30d5\u30a9\u30fc\u30af\u3057\u3066clone git clone <httsps or ssh> track\u30d5\u30a1\u30a4\u30eb\u3068untrack\u30d5\u30a1\u30a4\u30eb git ls-files \u3067track\u3057\u3066\u3044\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u89a7\u3092\u78ba\u8a8d Staging area\u3078\u306eadd\u3092\u30ad\u30e3\u30f3\u30bb\u30eb(git\u306e\u5185\u90e8\u3067\u306f\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5185\u5bb9\u3092staging area\u306b\u4e0a\u66f8\u304d) git reset HEAD <filename> Working directory\u306e\u5185\u5bb9\u3092\u7121\u3057\u306b\u3059\u308b\u3002(git\u306e\u5185\u90e8\u3067\u306fworking directory\u306e\u5185\u5bb9\u3092staging area \u3067\u4e0a\u66f8\u304d\u3057\u3066\u3044\u308b\u3002) git checkout -- <file name> \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5909\u66f4\u3092git\u3067\u7ba1\u7406 git mv <filename1> <filename2> (\u30b7\u30a7\u30eb\u306emv\u3067\u5909\u66f4\u3057\u305f\u5834\u5408\u306f git add -A ) \u30d5\u30a1\u30a4\u30eb\u306e\u524a\u9664\u3092Git\u3067\u7ba1\u7406\u3059\u308b\u3002 git rm <filename> (\u30b3\u30df\u30c3\u30c8\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044) git commit -m \"deleted\" \u524a\u9664\u5185\u5bb9\u306e\u53d6\u308a\u6d88\u3057 git reset HEAD <filename> git checkout -- <file name> \u30b3\u30df\u30c3\u30c8\u306e\u5c65\u6b74\u3092\u78ba\u8a8d\u3059\u308b git log --oneline, --graph, --<filename>, --follow <filename> git show <commitID> Git\u306e\u7ba1\u7406\u304b\u3089\u5916\u3059\u3002 .gitignore \u30d5\u30a1\u30a4\u30eb \u30b5\u30a4\u30ba\u304c\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u3084\u30d0\u30a4\u30ca\u30ea\u30fc\u30d5\u30a1\u30a4\u30eb\u3001\u4e2d\u9593\u30d5\u30a1\u30a4\u30eb\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u542b\u3080\u30d5\u30a1\u30a4\u30eb\u3001\u304f\u30a2\u30c3\u30b7\u30e5\u30d5\u30a1\u30a4\u30eb\u306a\u3069 \u30d6\u30e9\u30f3\u30c1\u3068\u30ed\u30fc\u30ab\u30eb\u3067\u30de\u30fc\u30b8,\u30ed\u30fc\u30ab\u30eb\u3067\u306e\u307frebase\u3059\u308b \u00b6 git merge <branchname> \uff1abranchname\u3092\u4eca\u3044\u308b\u30d6\u30e9\u30f3\u30c1\uff08\u666e\u901a\u306fmain\u30d6\u30e9\u30f3\u30c1\uff09\u306b\u53cd\u6620\u3002 git diff <base> <compare> \uff1abase\uff08main\uff09\u3068compare\uff08\u30d6\u30e9\u30f3\u30c1\uff09\u3092\u4f5c\u6210 conflict\u304c\u8d77\u304d\u3066\u3044\u308b\u5834\u5408\u306f\u30a8\u30c7\u30a3\u30bf\u3067\u958b\u3044\u3066\u30a2\u30ce\u30fc\u30c6\u30b7\u30e7\u30f3\u7b87\u6240\u3092\u6d88\u3059\u3002 git rebase main : main \u30d6\u30e9\u30f3\u30c1\u3092rebase\u3059\u308b\u3002rebase\u306f\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea \u00b6 git fetch <remote_ref> :\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092\u3068\u3063\u3066\u304f\u308b\u3002 git pull <remote_ref> <branchname> : git pull \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u304c\u3042\u308b\u5834\u5408\u306f\u5bfe\u51e6\u3059\u308b\u3002 Github\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30d5\u30a9\u30fc\u30af\u5143\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3078\u306epull request\u3092\u51fa\u3059\u3002 git remote add upstream <repourl> : \u30ed\u30fc\u30ab\u30eb\u306b\u306forigin\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd \u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3057\u3066\u304b\u3089\u81ea\u5206\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3057\u3066pull request\u3092\u4f5c\u6210\u3002 \u5dee\u5206diff\u3092\u898b\u308b \u00b6 p4merge \u3092\u5c0e\u5165\u3059\u308b\u3002 git diff \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git HEAD \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -- <filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD -- <filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD -- <filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ -- <filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main -- <filename> Stash\u3092\u4f7f\u3046\u3002 \u00b6 \u4f5c\u696d\u5185\u5bb9\u306e\u4e00\u6642\u56de\u907f - git stash git stash -a git stash list git stash apply git stash drop git stash show stash @{<i>} conflict\u304c\u3042\u308b\u5834\u5408 git mergetool \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306b\u5bfe\u51e6\u3059\u308b\u3002 ## Commit\u306btag\u3092\u4f7f\u3046\u3002 - \u30de\u30a4\u30eb\u30b9\u30c8\u30fc\u30f3\u306btag\u3092\u4f7f\u3063\u3066version\u3092\u7ba1\u7406\u3059\u308b - git tag <tagname> - git tag --list - git tag --delete <tagname> - git tag -a <tagname> tag\u3092\u3064\u3051\u308b\u3002 - git diff <tagname1> <tagname2> - git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 - git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b - git push <remote_ref> :<tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8push\u304b\u3089\u524a\u9664 - git checkout tags/<tagname> - git fetch --tgas --all submodule \u00b6 git submodule add <submodule_url> git submodule update git -recurse-submodule update submodule\u306e\u4e2d\u3067git pull\u3059\u308b\u3002 git submodule foreach 'git pull origin main' others \u00b6 convertio.io wiki\u3092\u4f7f\u3046 octotree zenhub\u3092\u4f7f\u3046\uff1a\u30a2\u30b8\u30e3\u30a4\u30eb\u958b\u767a\u306e\u30ab\u30f3\u30d0\u30f3 git revert <commitID> git reset --hard git reset --sorf HEAD \u30d5\u30a1\u30a4\u30eb\u540d \u9593\u9055\u3063\u3066add \u3057\u305f\u3068\u304d git reset \u2013soft HEAD^ \u9593\u9055\u3063\u3066commit\u3057\u305f\u3068\u304d Github\u306bssh\u3067\u63a5\u7d9a \u00b6 \u9375\u306e\u4f5c\u6210 \u00b6 1 2 3 4 5 6 7 8 9 ssh-keygen -t rsa # ssh-keygen -t ed25519 Generating public/private rsa key pair. Enter file in which to save the key ( /Users/yu/.ssh/id_rsa ) : /Users/yu/.ssh/github_rsa Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: # .pub\u3092Github\u306b\u767b\u9332 .ssh/config\u3092\u5909\u66f4 \u00b6 1 2 3 4 Host github github.com HostName github.com IdentityFile ~/.ssh/github_rsa #\u3053\u3053\u306b\u81ea\u5206\u306e\u9375\u306e\u30d5\u30a1\u30a4\u30eb\u540d User git \u63a5\u7d9a\u78ba\u8a8d \u00b6 1 ssh -T git@github.com .git\u30d5\u30a1\u30a4\u30eb\u306e\u5909\u66f4\u307e\u305f\u306fssh\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30af\u30ed\u30fc\u30f3 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 [ core ] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true [ remote \"origin\" ] url = git@github.com:xxxxx/yyyyy.git ######\u3053\u3053\u3092\u5909\u66f4\u3059\u308b fetch = +refs/heads/*:refs/remotes/origin/* [ branch \"master\" ] remote = origin merge = refs/heads/master","title":"Git"},{"location":"git/#git","text":"","title":"Git"},{"location":"git/#git_1","text":"Git\u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u30b7\u30b9\u30c6\u30e0\u306e1\u3064\uff08\u5206\u6563\u7ba1\u7406\u65b9\u5f0f\uff09\u3002\u7279\u5b9a\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5dee\u5206\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3001\u524d\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3057\u305f\u308a\u3059\u308b\u3002","title":"Git\u3068\u306f"},{"location":"git/#_1","text":"\u30ea\u30dd\u30b8\u30c8\u30ea\uff1a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u5168\u3066\u306e\u30d5\u30a1\u30a4\u30eb\uff08\u5909\u66f4\u5c65\u6b74\u3084\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3082\u3059\u3079\u3066\uff09\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 \u30b3\u30df\u30c3\u30c8\uff1a\u89aa\u5b50\u95a2\u4fc2\u3092\u6301\u3064\u30b0\u30e9\u30d5\u3002\u30d5\u30a1\u30a4\u30eb\u306e\u72b6\u614b\u3092\u30bb\u30fc\u30d6\u3059\u308b\u3053\u3068\u3002Working directory => staging area \uff08\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3082\u547c\u3070\u308c\u308b\uff09=> \u30ea\u30dd\u30b8\u30c8\u30ea\u306b\u30b3\u30df\u30c3\u30c8 \u30d6\u30e9\u30f3\u30c1\uff1a\u30b3\u30df\u30c3\u30c8\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002HEAD\u306f\u4eca\u81ea\u5206\u304c\u4f5c\u696d\u3057\u3066\u3044\u308b\u30d6\u30e9\u30f3\u30c1\u3092\u6307\u3059\u30dd\u30a4\u30f3\u30bf\u3002\u30b3\u30df\u30c3\u30c8\u524d\u306b\u5206\u5c90\u3055\u305b\u308b\u3002\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u3057\u3066\u30de\u30fc\u30b8\u3055\u305b\u308b\u3002","title":"\u7528\u8a9e"},{"location":"git/#github","text":"\u30cf\u30a4\u30d5\u30f3\u3067\u540d\u524d\u3092\u533a\u5207\u308b\u306e\u304c\u4e00\u822c\u7684 ssh\u3067\u306e\u8a8d\u8a3c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002","title":"Github"},{"location":"git/#_2","text":"\u30ed\u30fc\u30ab\u30eb\u306b\u30e6\u30fc\u30b6\u60c5\u5831\u3092\u30bb\u30c3\u30c8\u78ba\u8a8d git config --global user.name \"<username, github\u306eusername>\" git config --global user.email \"<email>\" git config --global --list git config --global --replace-all core.pager \"less -F -X\" git config --global pull.rebase true \u30ea\u30dd\u30b8\u30c8\u30ea\u3092clone git clone <remote_repo_url> git remote -v : \u767b\u9332\u3057\u3066\u3042\u308b\u30ea\u30e2\u30fc\u30c8\u30ea\u30dc\u3092\u78ba\u8a8d git clone \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f origin \u304c\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306b\u7d10\u4ed8\u3044\u3066\u3044\u308b \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\u6307\u5b9a\uff1a git clone https://github.com/user/{\u30ea\u30dd\u30b8\u30c8\u30ea\u540d}.git {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u30d6\u30e9\u30f3\u30c1\u3092\u6307\u5b9a\u3059\u308b\u5834\u5408 git clone -b [\u30d6\u30e9\u30f3\u30c1\u540d][\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30a2\u30c9\u30ec\u30b9] {\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d} \u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210\uff08\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308b\uff09 git branch \u30d6\u30e9\u30f3\u30c1\u4e00\u89a7 git branch -a \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u542b\u3080\u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u306e\u4e00\u89a7 git branch <branch name> branch name\u3068\u3044\u3046branch\u3092\u4f5c\u6210, HEAD\u306e\u30dd\u30a4\u30f3\u30bf\u5148\u3092\u5207\u308a\u66ff\u3048\u3066\u3044\u308b\u3002 git branch -m <old name> <new name> git branch -d <branch-name> \u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664 git checkout <branch name> branch name\u306b\u79fb\u52d5\u3002HEAD\u30dd\u30a4\u30f3\u30bf\u306e\u5207\u308a\u66ff\u3048 git checkout -b <branch name> \uff08\u5b9f\u7528\u4e0a\uff09branch \u4f5c\u6210\u3057\u3066branch name\u306b\u79fb\u52d5 \u30d6\u30e9\u30f3\u30c1\u540d\u306f\u30cf\u30a4\u30d5\u30f3\u3067\u533a\u5207\u308b \u30ed\u30fc\u30ab\u30eb\u30d5\u30a1\u30a4\u30eb\uff08at working directory, working tree\uff09\u3092\u66f4\u65b0\u3057\u3066Staging\u30a8\u30ea\u30a2\u306b\u3042\u3052\u308b git diff --<filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD --<filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD --<filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ --<filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main --<filename> git add <file name> git add . git status \u72b6\u6cc1\u3092\u78ba\u8a8d \u30b3\u30df\u30c3\u30c8\u3059\u308b git commit -m \"commit message\" git tag <tagname> git tag --list git log --oneline --all --graph \u30b3\u30df\u30c3\u30c8\u3057\u305f\u5c65\u6b74\u3092\u78ba\u8a8d \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092pull\u3057\u3066\u304b\u3089\u3001\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea\u306bpush\u3002pull\u306ffetch + merge git pull <remote ref> <branch name> git pull --rebase <remote ref> <branch name> : pull\u3059\u308b\u3068\u304d\u306brebase\u3059\u308b git pull origin main git push <remote ref> <branch name> git push origin new-branch git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b OSS\u306a\u3069\u306e\u5834\u5408\u3067\u306f\u3001\u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3059\u308b\u3002 git remote add upstream <repourl> git pull upstream main git push origin new-branch git\u7ba1\u7406\u5bfe\u8c61\u304b\u3089\u5916\u3059\u3002 git rm -r --cached push\u3057\u305f\u30d6\u30e9\u30f3\u30c1\u3092pull request\u3092\u4f5c\u3063\u3066\u30ea\u30e2\u30fc\u30c8\u306emain\u30d6\u30e9\u30f3\u30c1\u306b\u30de\u30fc\u30b8 Github\u3067\u4f5c\u696d\u3002 pull request \u3092\u30af\u30ea\u30c3\u30af => base (main)\u3068 compare (new branch)\u3092\u6307\u5b9a,\u81ea\u5206\u306e\u30ea\u30dd\u304b\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u304b\u3092\u78ba\u8a8d => create pull request => Merge pull request \u3092\u62bc\u3059\u3002 \u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306emain\u306e\u53cd\u6620\u3092\u30ed\u30fc\u30ab\u30eb\u30ea\u30dd\u306emain\u306b\u53cd\u6620\uff08pull\uff09 git checkout main git pull origin main \u4e0d\u8981\u306a\u30d6\u30e9\u30f3\u30c1\u3092\u524a\u9664\u3059\u308b\u3002 git branch -d <branch-name> Gighub\u3067 branches \u304b\u3089\u524a\u9664","title":"\u57fa\u672c\u7684\u306a\u6d41\u308c"},{"location":"git/#_3","text":"\u30b9\u30af\u30e9\u30c3\u30c1\u304b\u3089\u4f5c\u6210\uff08.git\u306e\u4f5c\u6210\uff09 git init <project-name> .git \u306e\u524a\u9664 rm -rf .git \u65e2\u5b58\u306e\u30d5\u30a9\u30eb\u30c0\u3092git\u30ea\u30dd\u306b\u3059\u308b\u3002 git init \u3000\u305d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5\u3057\u3066\u304b\u3089.git\u306e\u4f5c\u6210 \u65e2\u5b58\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u3092\u81ea\u5206\u306e\u30ea\u30dd\u3092\u30d5\u30a9\u30fc\u30af\u3057\u3066clone git clone <httsps or ssh> track\u30d5\u30a1\u30a4\u30eb\u3068untrack\u30d5\u30a1\u30a4\u30eb git ls-files \u3067track\u3057\u3066\u3044\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4e00\u89a7\u3092\u78ba\u8a8d Staging area\u3078\u306eadd\u3092\u30ad\u30e3\u30f3\u30bb\u30eb(git\u306e\u5185\u90e8\u3067\u306f\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5185\u5bb9\u3092staging area\u306b\u4e0a\u66f8\u304d) git reset HEAD <filename> Working directory\u306e\u5185\u5bb9\u3092\u7121\u3057\u306b\u3059\u308b\u3002(git\u306e\u5185\u90e8\u3067\u306fworking directory\u306e\u5185\u5bb9\u3092staging area \u3067\u4e0a\u66f8\u304d\u3057\u3066\u3044\u308b\u3002) git checkout -- <file name> \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u5909\u66f4\u3092git\u3067\u7ba1\u7406 git mv <filename1> <filename2> (\u30b7\u30a7\u30eb\u306emv\u3067\u5909\u66f4\u3057\u305f\u5834\u5408\u306f git add -A ) \u30d5\u30a1\u30a4\u30eb\u306e\u524a\u9664\u3092Git\u3067\u7ba1\u7406\u3059\u308b\u3002 git rm <filename> (\u30b3\u30df\u30c3\u30c8\u3057\u3066\u304b\u3089\u3067\u306a\u3044\u3068\u4f7f\u3048\u306a\u3044) git commit -m \"deleted\" \u524a\u9664\u5185\u5bb9\u306e\u53d6\u308a\u6d88\u3057 git reset HEAD <filename> git checkout -- <file name> \u30b3\u30df\u30c3\u30c8\u306e\u5c65\u6b74\u3092\u78ba\u8a8d\u3059\u308b git log --oneline, --graph, --<filename>, --follow <filename> git show <commitID> Git\u306e\u7ba1\u7406\u304b\u3089\u5916\u3059\u3002 .gitignore \u30d5\u30a1\u30a4\u30eb \u30b5\u30a4\u30ba\u304c\u5927\u304d\u3044\u30d5\u30a1\u30a4\u30eb\u3084\u30d0\u30a4\u30ca\u30ea\u30fc\u30d5\u30a1\u30a4\u30eb\u3001\u4e2d\u9593\u30d5\u30a1\u30a4\u30eb\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u542b\u3080\u30d5\u30a1\u30a4\u30eb\u3001\u304f\u30a2\u30c3\u30b7\u30e5\u30d5\u30a1\u30a4\u30eb\u306a\u3069","title":"\u57fa\u672c\u64cd\u4f5c"},{"location":"git/#rebase","text":"git merge <branchname> \uff1abranchname\u3092\u4eca\u3044\u308b\u30d6\u30e9\u30f3\u30c1\uff08\u666e\u901a\u306fmain\u30d6\u30e9\u30f3\u30c1\uff09\u306b\u53cd\u6620\u3002 git diff <base> <compare> \uff1abase\uff08main\uff09\u3068compare\uff08\u30d6\u30e9\u30f3\u30c1\uff09\u3092\u4f5c\u6210 conflict\u304c\u8d77\u304d\u3066\u3044\u308b\u5834\u5408\u306f\u30a8\u30c7\u30a3\u30bf\u3067\u958b\u3044\u3066\u30a2\u30ce\u30fc\u30c6\u30b7\u30e7\u30f3\u7b87\u6240\u3092\u6d88\u3059\u3002 git rebase main : main \u30d6\u30e9\u30f3\u30c1\u3092rebase\u3059\u308b\u3002rebase\u306f\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u306a\u3044\u3002","title":"\u30d6\u30e9\u30f3\u30c1\u3068\u30ed\u30fc\u30ab\u30eb\u3067\u30de\u30fc\u30b8,\u30ed\u30fc\u30ab\u30eb\u3067\u306e\u307frebase\u3059\u308b"},{"location":"git/#_4","text":"git fetch <remote_ref> :\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306e\u60c5\u5831\u3092\u3068\u3063\u3066\u304f\u308b\u3002 git pull <remote_ref> <branchname> : git pull \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u304c\u3042\u308b\u5834\u5408\u306f\u5bfe\u51e6\u3059\u308b\u3002 Github\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30d5\u30a9\u30fc\u30af\u5143\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3078\u306epull request\u3092\u51fa\u3059\u3002 git remote add upstream <repourl> : \u30ed\u30fc\u30ab\u30eb\u306b\u306forigin\u3067\u30a2\u30af\u30bb\u30b9\u53ef\u80fd \u307e\u305a\u30d5\u30a9\u30fc\u30af\u3082\u3068\u306e\u30ea\u30dd\u3092pull\u3057\u3066\u304b\u3089\u81ea\u5206\u306e\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3057\u3066pull request\u3092\u4f5c\u6210\u3002","title":"\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u30b8\u30c8\u30ea"},{"location":"git/#diff","text":"p4merge \u3092\u5c0e\u5165\u3059\u308b\u3002 git diff \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git HEAD \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -- <filename> \u3067working directory\u3068staging area\u306ediss\u3092\u78ba\u8a8d git diff HEAD -- <filename> \u3067working directory\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff -staged HEAD -- <filename> \u3067staging area\u3068\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff HEAD HEAD^^ -- <filename> 2\u3064\u524d\u306e\u5dee\u5206\u3092\u78ba\u8a8d git diff origin/main main -- <filename>","title":"\u5dee\u5206diff\u3092\u898b\u308b"},{"location":"git/#stash","text":"\u4f5c\u696d\u5185\u5bb9\u306e\u4e00\u6642\u56de\u907f - git stash git stash -a git stash list git stash apply git stash drop git stash show stash @{<i>} conflict\u304c\u3042\u308b\u5834\u5408 git mergetool \u3067\u30b3\u30f3\u30d5\u30ea\u30af\u30c8\u306b\u5bfe\u51e6\u3059\u308b\u3002 ## Commit\u306btag\u3092\u4f7f\u3046\u3002 - \u30de\u30a4\u30eb\u30b9\u30c8\u30fc\u30f3\u306btag\u3092\u4f7f\u3063\u3066version\u3092\u7ba1\u7406\u3059\u308b - git tag <tagname> - git tag --list - git tag --delete <tagname> - git tag -a <tagname> tag\u3092\u3064\u3051\u308b\u3002 - git diff <tagname1> <tagname2> - git tag -a <tagname> <commitID> commit\u306btag\u3092\u3064\u3051\u308b\u3002 - git push <remote_ref> <tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8\u30ea\u30dd\u306bpush\u3059\u308b - git push <remote_ref> :<tagname> tag\u3092\u30ea\u30e2\u30fc\u30c8push\u304b\u3089\u524a\u9664 - git checkout tags/<tagname> - git fetch --tgas --all","title":"Stash\u3092\u4f7f\u3046\u3002"},{"location":"git/#submodule","text":"git submodule add <submodule_url> git submodule update git -recurse-submodule update submodule\u306e\u4e2d\u3067git pull\u3059\u308b\u3002 git submodule foreach 'git pull origin main'","title":"submodule"},{"location":"git/#others","text":"convertio.io wiki\u3092\u4f7f\u3046 octotree zenhub\u3092\u4f7f\u3046\uff1a\u30a2\u30b8\u30e3\u30a4\u30eb\u958b\u767a\u306e\u30ab\u30f3\u30d0\u30f3 git revert <commitID> git reset --hard git reset --sorf HEAD \u30d5\u30a1\u30a4\u30eb\u540d \u9593\u9055\u3063\u3066add \u3057\u305f\u3068\u304d git reset \u2013soft HEAD^ \u9593\u9055\u3063\u3066commit\u3057\u305f\u3068\u304d","title":"others"},{"location":"git/#githubssh","text":"","title":"Github\u306bssh\u3067\u63a5\u7d9a"},{"location":"git/#_5","text":"1 2 3 4 5 6 7 8 9 ssh-keygen -t rsa # ssh-keygen -t ed25519 Generating public/private rsa key pair. Enter file in which to save the key ( /Users/yu/.ssh/id_rsa ) : /Users/yu/.ssh/github_rsa Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: # .pub\u3092Github\u306b\u767b\u9332","title":"\u9375\u306e\u4f5c\u6210"},{"location":"git/#sshconfig","text":"1 2 3 4 Host github github.com HostName github.com IdentityFile ~/.ssh/github_rsa #\u3053\u3053\u306b\u81ea\u5206\u306e\u9375\u306e\u30d5\u30a1\u30a4\u30eb\u540d User git","title":".ssh/config\u3092\u5909\u66f4"},{"location":"git/#_6","text":"1 ssh -T git@github.com","title":"\u63a5\u7d9a\u78ba\u8a8d"},{"location":"git/#gitssh","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 [ core ] repositoryformatversion = 0 filemode = false bare = false logallrefupdates = true symlinks = false ignorecase = true [ remote \"origin\" ] url = git@github.com:xxxxx/yyyyy.git ######\u3053\u3053\u3092\u5909\u66f4\u3059\u308b fetch = +refs/heads/*:refs/remotes/origin/* [ branch \"master\" ] remote = origin merge = refs/heads/master","title":".git\u30d5\u30a1\u30a4\u30eb\u306e\u5909\u66f4\u307e\u305f\u306fssh\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30af\u30ed\u30fc\u30f3"},{"location":"image_process/","text":"png\u3092jpg\u306b\u5909\u63db \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import argparse import glob import os from PIL import Image from tqdm import tqdm def check_create_dir ( path ): if not os . path . isdir ( path ): os . makedirs ( path , exist_ok = True ) def get_file_list ( input_dir ): return [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] def jpg2png ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = Image . open ( file_path ) . resize (( 256 , 256 )) . convert ( \"RGBA\" ) img . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .png\" )) if __name__ == \"__main__\" : parser = argparse . ArgumentParser ( description = \"Image Annotation\" ) parser . add_argument ( \"--input_dir\" , type = str , default = \"./images\" , help = \"path for input data\" , ) parser . add_argument ( \"--output_dir\" , type = str , default = \"./converted_images\" , help = \"path for output csv file\" , ) args = parser . parse_args () check_create_dir ( args . output_dir ) file_list = get_file_list ( args . input_dir ) jpg2png ( file_list , args . output_dir ) \u753b\u50cf\u3092Grid\u4e0a\u306b\u8868\u793a \u00b6 torch \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torchvision import numpy as np import matplotlib.pyplot as plt import torchvision import torch from PIL import Image import glob import random def create_image_array ( image_folder_path , number_of_images = 100 , size = 256 ): file_list = glob . glob ( image_folder_path ) random_images = random . sample ( file_list , number_of_images ) image_array = np . zeros (( number_of_images , size , size , 3 ), np . uint8 ) for i , image_path in enumerate ( random_images ): im = Image . open ( image_path ) . resize (( size , size )) img = np . asarray ( im ) image_array [ i ] = img return image_array def torchvision_save ( image_array , save_path , nrows = 10 , padding = 2 ): # \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f images = image_array # save_image\u3067255\u639b\u3051\u308b\u305f\u3081[0,1]\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u3066\u304a\u304f images = ( images / 255.0 ) . astype ( np . float32 ) # 1\u679a\u306b\u7d50\u5408 images = np . transpose ( images , [ 0 , 3 , 1 , 2 ]) images_tensor = torch . as_tensor ( images ) torchvision . utils . save_image ( images_tensor , save_path , nrow = nrows , padding = padding ) image_array = create_image_array ( \"/directory/*.jpg\" ) torchvision_save ( image_array , \"joint_image.png\" ) without torch \u00b6 \u753b\u50cf\u3092\u4e26\u3079\u3066plot \u00b6 https://qiita.com/skotaro/items/08dc0b8c5704c94eafb9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import matplotlib.pyplot as plt import matplotlib.image as mpimg from PIL import Image IMAGE_DIR = \"/dataset/JAPANESE_FACES/personal_output/\" num_rows , num_cols = 6 , 6 f , axes = plt . subplots ( nrows = num_rows , ncols = num_cols , figsize = ( 24 , 24 )) for index , d in enumerate ( image_list ): plt . axis ( \"off\" ) plt . subplot ( 6 , 6 , index + 1 ) plt . imshow ( mpimg . imread ( IMAGE_DIR + d )) plt . figure ( figsize = ( 20 , 20 )) row , col = 5 , 5 for i in range ( row * col ): plt . subplot ( col , row , i + 1 ) image = cv2 . imread ( train . loc [ i , 'file_path' ]) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) target = train . loc [ i , 'Pawpularity' ] plt . imshow ( image ) plt . title ( f \"target: { target } \" ) plt . show ()","title":"Image Process"},{"location":"image_process/#pngjpg","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import argparse import glob import os from PIL import Image from tqdm import tqdm def check_create_dir ( path ): if not os . path . isdir ( path ): os . makedirs ( path , exist_ok = True ) def get_file_list ( input_dir ): return [ p for p in glob . glob ( os . path . join ( input_dir , \"**\" ), recursive = True ) if os . path . isfile ( p ) ] def jpg2png ( file_list , output_dir ): for file_path in tqdm ( file_list ): img = Image . open ( file_path ) . resize (( 256 , 256 )) . convert ( \"RGBA\" ) img . save ( os . path . join ( output_dir , f \" { os . path . basename ( file_path )[: - 4 ] } .png\" )) if __name__ == \"__main__\" : parser = argparse . ArgumentParser ( description = \"Image Annotation\" ) parser . add_argument ( \"--input_dir\" , type = str , default = \"./images\" , help = \"path for input data\" , ) parser . add_argument ( \"--output_dir\" , type = str , default = \"./converted_images\" , help = \"path for output csv file\" , ) args = parser . parse_args () check_create_dir ( args . output_dir ) file_list = get_file_list ( args . input_dir ) jpg2png ( file_list , args . output_dir )","title":"png\u3092jpg\u306b\u5909\u63db"},{"location":"image_process/#grid","text":"","title":"\u753b\u50cf\u3092Grid\u4e0a\u306b\u8868\u793a"},{"location":"image_process/#torch","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import torchvision import numpy as np import matplotlib.pyplot as plt import torchvision import torch from PIL import Image import glob import random def create_image_array ( image_folder_path , number_of_images = 100 , size = 256 ): file_list = glob . glob ( image_folder_path ) random_images = random . sample ( file_list , number_of_images ) image_array = np . zeros (( number_of_images , size , size , 3 ), np . uint8 ) for i , image_path in enumerate ( random_images ): im = Image . open ( image_path ) . resize (( size , size )) img = np . asarray ( im ) image_array [ i ] = img return image_array def torchvision_save ( image_array , save_path , nrows = 10 , padding = 2 ): # \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f images = image_array # save_image\u3067255\u639b\u3051\u308b\u305f\u3081[0,1]\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u3066\u304a\u304f images = ( images / 255.0 ) . astype ( np . float32 ) # 1\u679a\u306b\u7d50\u5408 images = np . transpose ( images , [ 0 , 3 , 1 , 2 ]) images_tensor = torch . as_tensor ( images ) torchvision . utils . save_image ( images_tensor , save_path , nrow = nrows , padding = padding ) image_array = create_image_array ( \"/directory/*.jpg\" ) torchvision_save ( image_array , \"joint_image.png\" )","title":"torch"},{"location":"image_process/#without-torch","text":"","title":"without torch"},{"location":"image_process/#plot","text":"https://qiita.com/skotaro/items/08dc0b8c5704c94eafb9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import matplotlib.pyplot as plt import matplotlib.image as mpimg from PIL import Image IMAGE_DIR = \"/dataset/JAPANESE_FACES/personal_output/\" num_rows , num_cols = 6 , 6 f , axes = plt . subplots ( nrows = num_rows , ncols = num_cols , figsize = ( 24 , 24 )) for index , d in enumerate ( image_list ): plt . axis ( \"off\" ) plt . subplot ( 6 , 6 , index + 1 ) plt . imshow ( mpimg . imread ( IMAGE_DIR + d )) plt . figure ( figsize = ( 20 , 20 )) row , col = 5 , 5 for i in range ( row * col ): plt . subplot ( col , row , i + 1 ) image = cv2 . imread ( train . loc [ i , 'file_path' ]) image = cv2 . cvtColor ( image , cv2 . COLOR_BGR2RGB ) target = train . loc [ i , 'Pawpularity' ] plt . imshow ( image ) plt . title ( f \"target: { target } \" ) plt . show ()","title":"\u753b\u50cf\u3092\u4e26\u3079\u3066plot"},{"location":"labrad/","text":"labrad_hdf5_dataloader \u00b6 labrad hdf5\u304b\u3089ndarray\u5f62\u5f0f\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_ndarray ( dir_path , file_num , file_name ): \"\"\"Load a hdf5 file and return the data (numpy.array) and columns of labels (list) Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '000## - measurement_name.hdf5' file_name : string Returns ------- data : ndarray variables : list list of parameters \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] raw_data = f . value attrs = f . attrs # Raw data to np.array data = np . array ([ list ( d ) for d in raw_data ]) # Get varialbles labels indep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Independent' ) and str ( x ) . endswith ( 'label' )]) dep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Dependent' ) and str ( x ) . endswith ( 'label' )]) indep_labels = [ attrs [ c ] for c in indep_keys ] dep_labels = [ attrs [ c ] for c in dep_keys ] variables = indep_labels + dep_labels return data , variables labrad_hdf5_get_parameters \u00b6 labrad hdf5\u304b\u3089DV.add_parameters()\u3067\u52a0\u3048\u305f\u6a5f\u5668\u306e\u8a2d\u5b9a\u306e\u60c5\u5831\u306a\u3069\u3092\u53d6\u5f97\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_get_parameters ( dir_path , file_num , file_name ): \"\"\"Get parameter settings (e.g., ferquency, time constant added by DV.add_parameters()) from a labrad hdf5 file Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '00033 - measurement_name.hdf5' file_name : string Returns ------- dictionary Pairs of paramter keys and values Notes ----- The default parameter values are encoded by labrad format. The prefix in endoded values is 'data:application/labrad;base64,' To decode these and get the raw value, we need to simply use DV.get_parameters() or change the backend script in datavault/backend.py This function works in the latter case. \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] attrs = f . attrs # Get parameters labels and values param_ukeys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Param' )]) param_keys = [ c [ 6 :] for c in param_ukeys ] param_values = [ attrs [ c ] for c in param_ukeys ] return { k : v for k , v in zip ( param_keys , param_values )} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_parameters_of_func ( offset = None ): \"\"\"Get a dictionary of paramteres of the function. Parameters ---------- offset : int default value is None Return ------ dictionary The dictionary includes pairs of paremeter's name and the corresponding values. References ---------- [1] https://tottoto.net/python3-get-args-of-current-function/ \"\"\" parent_frame = inspect . currentframe () . f_back info = inspect . getargvalues ( parent_frame ) return { key : info . locals [ key ] for key in info . args [ offset :]} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def create_labrad_hdf5file ( DV , file_path , scan_name , scan_var , meas_var ): \"\"\"Create a labrad hdf5 file from ndarray. Parameters ---------- DV : object file_path : string scan_name : string scan_var : list or tuple meas_var : list or tuple Returns ------- int The file number \"\"\" DV . cd ( '' ) try : DV . mkdir ( file_path ) DV . cd ( file_path ) except Exception : DV . cd ( file_path ) file_name = file_path + '_' + scan_name dv_file = DV . new ( file_name , scan_var , meas_var ) print ' \\r ' , \"new file created, file numer: \" , int ( dv_file [ 1 ][ 0 : 5 ]) return int ( dv_file [ 1 ][ 0 : 5 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def write_meas_parameters ( DV , file_path , file_number , date , scan_name , meas_parameters , amplitude , sensitivity ): \"\"\"Write measurement parameters to txt file and labrad hdf5 file. Parameters ---------- DV : object file_path : string file_number : int date : object scan_name : string meas_parameters : dict scan_var : list or tuple meas_var : list or tuple amplitude : float sensitivity : float Returns ------- None \"\"\" if not os . path . isfile ( meas_details_path + file_path + '.txt' ): with open ( meas_details_path + file_path + '.txt' , \"w+\" ) as f : pass with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"========\" + \" \\n \" ) f . write ( \"file_number: \" + str ( file_number ) + \" \\n \" + \"date: \" + str ( date ) + \" \\n \" + \"measurement:\" + str ( scan_name ) + \" \\n \" ) for k , v in sorted ( meas_parameters . items ()): print ( k , v ) f . write ( str ( k ) + \": \" + str ( v ) + \" \\n \" ) DV . add_parameter ( str ( k ), str ( v )) for i , LA in enumerate ( LAs ): tc = LA . time_constant () sens = LA . sensitivity () f . write ( \"time_constant_\" + str ( i ) + ' : ' + str ( tc ) + \" \\n \" ) f . write ( \"sensitivity_\" + str ( i ) + ' : ' + str ( sens ) + \" \\n \" ) DV . add_parameter ( \"time_constant_\" + str ( i ), tc ) DV . add_parameter ( \"sensitivity_\" + str ( i ), sens ) def write_meas_parameters_end ( date1 , date2 , file_path ): with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"end date: \" + str ( date2 ) + \" \\n \" + \"total time: \" + str ( date2 - date1 ) + \" \\n \" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def get_variables ( DV ): \"\"\"Get variables of a lablad hdf5 file Parameters ---------- DV : object (datavault) Return ------ list A variable of the a lablad hdf5 file \"\"\" variables = [ DV . variables ()[ 0 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 0 ]))] + [ DV . variables ()[ 1 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 1 ]))] return variables","title":"LabRAD"},{"location":"labrad/#labrad_hdf5_dataloader","text":"labrad hdf5\u304b\u3089ndarray\u5f62\u5f0f\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_ndarray ( dir_path , file_num , file_name ): \"\"\"Load a hdf5 file and return the data (numpy.array) and columns of labels (list) Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '000## - measurement_name.hdf5' file_name : string Returns ------- data : ndarray variables : list list of parameters \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] raw_data = f . value attrs = f . attrs # Raw data to np.array data = np . array ([ list ( d ) for d in raw_data ]) # Get varialbles labels indep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Independent' ) and str ( x ) . endswith ( 'label' )]) dep_keys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Dependent' ) and str ( x ) . endswith ( 'label' )]) indep_labels = [ attrs [ c ] for c in indep_keys ] dep_labels = [ attrs [ c ] for c in dep_keys ] variables = indep_labels + dep_labels return data , variables","title":"labrad_hdf5_dataloader"},{"location":"labrad/#labrad_hdf5_get_parameters","text":"labrad hdf5\u304b\u3089DV.add_parameters()\u3067\u52a0\u3048\u305f\u6a5f\u5668\u306e\u8a2d\u5b9a\u306e\u60c5\u5831\u306a\u3069\u3092\u53d6\u5f97\u3059\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def labrad_hdf5_get_parameters ( dir_path , file_num , file_name ): \"\"\"Get parameter settings (e.g., ferquency, time constant added by DV.add_parameters()) from a labrad hdf5 file Parameters ---------- dir_path : string Usually this is \"vault\" directory file_num : int hdf5 file number. ex. '00033 - measurement_name.hdf5' file_name : string Returns ------- dictionary Pairs of paramter keys and values Notes ----- The default parameter values are encoded by labrad format. The prefix in endoded values is 'data:application/labrad;base64,' To decode these and get the raw value, we need to simply use DV.get_parameters() or change the backend script in datavault/backend.py This function works in the latter case. \"\"\" # Load hdf5 file f_name = '0' * ( 5 - len ( str ( file_num ))) + str ( file_num ) + ' - ' + file_name + '.hdf5' f = h5py . File ( dir_path + f_name , 'r' )[ 'DataVault' ] attrs = f . attrs # Get parameters labels and values param_ukeys = sorted ([ str ( x ) for x in list ( attrs . keys ()) if str ( x ) . startswith ( 'Param' )]) param_keys = [ c [ 6 :] for c in param_ukeys ] param_values = [ attrs [ c ] for c in param_ukeys ] return { k : v for k , v in zip ( param_keys , param_values )} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_parameters_of_func ( offset = None ): \"\"\"Get a dictionary of paramteres of the function. Parameters ---------- offset : int default value is None Return ------ dictionary The dictionary includes pairs of paremeter's name and the corresponding values. References ---------- [1] https://tottoto.net/python3-get-args-of-current-function/ \"\"\" parent_frame = inspect . currentframe () . f_back info = inspect . getargvalues ( parent_frame ) return { key : info . locals [ key ] for key in info . args [ offset :]} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def create_labrad_hdf5file ( DV , file_path , scan_name , scan_var , meas_var ): \"\"\"Create a labrad hdf5 file from ndarray. Parameters ---------- DV : object file_path : string scan_name : string scan_var : list or tuple meas_var : list or tuple Returns ------- int The file number \"\"\" DV . cd ( '' ) try : DV . mkdir ( file_path ) DV . cd ( file_path ) except Exception : DV . cd ( file_path ) file_name = file_path + '_' + scan_name dv_file = DV . new ( file_name , scan_var , meas_var ) print ' \\r ' , \"new file created, file numer: \" , int ( dv_file [ 1 ][ 0 : 5 ]) return int ( dv_file [ 1 ][ 0 : 5 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def write_meas_parameters ( DV , file_path , file_number , date , scan_name , meas_parameters , amplitude , sensitivity ): \"\"\"Write measurement parameters to txt file and labrad hdf5 file. Parameters ---------- DV : object file_path : string file_number : int date : object scan_name : string meas_parameters : dict scan_var : list or tuple meas_var : list or tuple amplitude : float sensitivity : float Returns ------- None \"\"\" if not os . path . isfile ( meas_details_path + file_path + '.txt' ): with open ( meas_details_path + file_path + '.txt' , \"w+\" ) as f : pass with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"========\" + \" \\n \" ) f . write ( \"file_number: \" + str ( file_number ) + \" \\n \" + \"date: \" + str ( date ) + \" \\n \" + \"measurement:\" + str ( scan_name ) + \" \\n \" ) for k , v in sorted ( meas_parameters . items ()): print ( k , v ) f . write ( str ( k ) + \": \" + str ( v ) + \" \\n \" ) DV . add_parameter ( str ( k ), str ( v )) for i , LA in enumerate ( LAs ): tc = LA . time_constant () sens = LA . sensitivity () f . write ( \"time_constant_\" + str ( i ) + ' : ' + str ( tc ) + \" \\n \" ) f . write ( \"sensitivity_\" + str ( i ) + ' : ' + str ( sens ) + \" \\n \" ) DV . add_parameter ( \"time_constant_\" + str ( i ), tc ) DV . add_parameter ( \"sensitivity_\" + str ( i ), sens ) def write_meas_parameters_end ( date1 , date2 , file_path ): with open ( meas_details_path + file_path + '.txt' , \"a\" ) as f : f . write ( \"end date: \" + str ( date2 ) + \" \\n \" + \"total time: \" + str ( date2 - date1 ) + \" \\n \" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def get_variables ( DV ): \"\"\"Get variables of a lablad hdf5 file Parameters ---------- DV : object (datavault) Return ------ list A variable of the a lablad hdf5 file \"\"\" variables = [ DV . variables ()[ 0 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 0 ]))] + [ DV . variables ()[ 1 ][ i ][ 0 ] for i in range ( len ( DV . variables ()[ 1 ]))] return variables","title":"labrad_hdf5_get_parameters"},{"location":"linux_command/","text":"Linux \u00b6 Linux\u8a2d\u5b9a\u95a2\u9023 \u00b6 \u30b7\u30a7\u30eb\u306eAlias\u306e\u8a2d\u5b9a \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # \u3088\u304f\u5229\u7528\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u982d\u6587\u5b57\u306e\u9023\u7d50 alias abc = 'cd ~/aaa/bbb/ccc' alias g = 'git' alias ga = 'git add' alias gd = 'git diff' alias gs = 'git status' alias gp = 'git push' alias gb = 'git branch' alias gst = 'git status' alias gco = 'git checkout' alias gf = 'git fetch' alias gc = 'git commit' alias cp = 'cp -i' alias mv = 'mv -i' alias rm = 'rm -i' alias hg = 'history | grep' alias tma = 'tmux a -t' alias tmn = 'tmux new-session -s' # u \u304a\u3059\u3059\u3081.vimrc \u00b6 1 2 3 4 5 6 7 8 9 10 11 set number set expandtab set hlsearch set ignorecase set incsearch set smartcase set laststatus=2 set nocompatible set clipboard=unnamed,autoselect set clipboard& clipboard^=unnamedplus syntax on .netrc\u306e\u66f8\u304d\u65b9 \u00b6 1 2 3 4 5 6 7 machine api.wandb.ai login user password **** machine github.com login your_username password *** Linux\u30b3\u30de\u30f3\u30c9 \u00b6 \u53c2\u8003\u30ea\u30f3\u30af \u00b6 https://atmarkit.itmedia.co.jp/ait/articles/1906/05/news004.html https://qiita.com/nmrmsys/items/03f97f5eabec18a3a18b https://qiita.com/arene-calix/items/41d8d4ba572f1d652727 https://qiita.com/savaniased/items/d2c5c699188a0f1623ef https://tech-blog.rakus.co.jp/entry/20210604/linux https://pg-happy.jp/linux-command.html \u30d5\u30a1\u30a4\u30eb\u30fb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u95a2\u9023 \u00b6 pwd (print working directory) \u00b6 1 2 3 # pwd: \u4eca\u3044\u308b\u30d5\u30a9\u30eb\u30c0\u306e\u7d76\u5bfe\u30d1\u30b9\u3092\u8868\u793a yseeker@~/Desktop $ pwd /home/yseeker/Desktop ls (list) \u00b6 1 2 3 4 5 6 7 ls -alh ls -ltr # -a: \u96a0\u3057\u30d5\u30a1\u30a4\u30eb\u3082\u8868\u793a(\u7531\u6765: all) # -l: \u8a73\u7d30\u306a\u60c5\u5831\u3092\u8868\u793a # -h: M(\u30e1\u30ac)\u3001G(\u30ae\u30ac)\u306a\u3069\u3092\u4ed8\u3051\u3066\u30b5\u30a4\u30ba\u3092\u898b\u3084\u3059\u304f\u3059\u308b(\u7531\u6765:human readable) # -t: \u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u9806\u3067\u8868\u793a(\u7531\u6765: time) # -r: \u9006\u9806\u3067\u8868\u793a(\u7531\u6765: reverse) cd (change directory) \u00b6 1 cd - #\u76f4\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5 mkdir (make directory) \u00b6 1 mkdir -p aaa/bbb/ccc #\u6df1\u3044\u968e\u5c64\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4e00\u6c17\u306b\u4f5c\u6210 touch \u00b6 \u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\u3001\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u66f4\u65b0 mv \u00b6 \u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5\u3001\u540d\u524d\u5909\u66f4 cp \u00b6 1 2 3 4 # cp -r source/path destination/path: \u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30d4\u30fc # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u30b3\u30d4\u30fc(ecursive) # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(force) # * -p: \u30b3\u30d4\u30fc\u524d\u5f8c\u3067\u30d1\u30fc\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u4fdd\u6301(permission) rm \u00b6 1 2 3 4 5 6 # rm -rf directory_name: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664 # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(\u7531\u6765: force) # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u524a\u9664(\u7531\u6765: recursive) $ rm -f *.txt # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066txt\u30d5\u30a1\u30a4\u30eb\u3092\u5168\u524a\u9664 $ rm -rf dir* # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066\u4e00\u62ec\u524a\u9664 tar \u00b6 1 2 3 4 5 # tar -czvf xxx.tgz file1 file2 dir1 : \u5727\u7e2e(file1 file2 dir1\u3092\u30a2\u30fc\u30ab\u30a4\u30d6\u3057\u305f\u5727\u7e2e\u30d5\u30a1\u30a4\u30ebxxx.tgz\u3092\u4f5c\u6210) # tar -tzvf xxx.tgz: \u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306b\u542b\u307e\u308c\u308b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8868\u793a(=\u5c55\u958b\u306e\u30c6\u30b9\u30c8) # tar -xzvf xxx.tgz: \u5c55\u958b # * c(create), t(test), x(extract) + zvf\u3068\u899a\u3048\u308b tar czvf something.tgz dir* file* zip, unzip \u00b6 1 2 3 $ zip -r \u00abZIP\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb $ tar cvzf \u00abTARGZ\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb unzip -q foo.zip -d bar \u30c6\u30ad\u30b9\u30c8\u51e6\u7406 \u00b6 cat (concatenate) \u00b6 1 2 access.log error1.log error2.log $ cat error*.log # error1.log\u3068error2.log\u3092\u307e\u3068\u3081\u3066\u78ba\u8a8d tail \u00b6 1 2 3 4 5 # \u91cd\u305f\u3044\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u6700\u5f8c\u306e\u65b9\u3060\u3051\u898b\u308b tail -n 3 file1.txt 11 kkk KKK 12 lll LLL 13 mmm MMM less \u00b6 1 2 3 4 5 6 7 8 9 10 # less file1: file1\u3092\u898b\u308b(read only) # cat file1 | cmd1 | cmd2 | less: file1\u3092\u3044\u308d\u3044\u308d\u52a0\u5de5\u3057\u305f\u7d50\u679c\u3092\u898b\u308b # command | less - # grep 080 testData | less -N # less +F output # * \u30bf\u30fc\u30df\u30ca\u30eb\u306b\u51fa\u529b\u305b\u305a\u3001\u4f55\u304b\u3092\u898b\u305f\u3044\u3068\u304d\u306b\u3068\u308a\u3042\u3048\u305a\u4f7f\u3046\u30b3\u30de\u30f3\u30c9 # gg: \u5148\u982d\u884c\u3078\u79fb\u52d5 # G: \u6700\u7d42\u884c\u3078\u79fb\u52d5 # /pattern: pattern\u3067\u30d5\u30a1\u30a4\u30eb\u5185\u691c\u7d22 # q: \u9589\u3058\u308b wc (word count) \u00b6 1 2 3 4 5 6 7 $ wc -l error.log # \u884c\u6570\u30ab\u30a6\u30f3\u30c8(1) 7 error.log # \u30d5\u30a1\u30a4\u30eb\u6570\u30ab\u30a6\u30f3\u30c8 ls | wc -w find . -name \"*.jpg\" | wc -l ls -F | grep -v / | wc -l sort, uniq \u00b6 1 2 3 4 5 6 7 # sort file1: file1\u3092\u884c\u5358\u4f4d\u3067\u30bd\u30fc\u30c8 # uniq file1: file1\u306e\u91cd\u8907\u696d\u3092\u524a\u9664 # cat file1 | sort | uniq: file1\u3092\u30bd\u30fc\u30c8\u3057\u3066\u3001\u91cd\u8907\u696d\u3092\u6392\u9664 # * sort\u3068uniq\u306f\u30ef\u30f3\u30bb\u30c3\u30c8\u7684\u306a\u3068\u3053\u308d\u304c\u3042\u308b\u306e\u3067\u307e\u3068\u3081\u3066\u7d39\u4ecb # # * sort\u306f-r\u3067\u9006\u9806\u30bd\u30fc\u30c8\u3001-R\u3067\u30e9\u30f3\u30c0\u30e0\u30bd\u30fc\u30c8\u3001\u307f\u305f\u3044\u306b\u7d50\u69cb\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u591a\u5f69 # * ls -l\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u9806\u3067sort\u3059\u308b\u3001\u307f\u305f\u3044\u306b grep \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # grep ERROR *.log: \u62e1\u5f35\u5b50\u304clog\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u3001ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep -2 ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3068\u305d\u306e\u524d\u5f8c2\u884c\u3092\u51fa\u529b # cat error.log | grep -e ERROR -e WARN: error.log\u304b\u3089ERROR\u307e\u305f\u306fWARN\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa # cat error.log | grep ERROR | grep -v 400: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa\u3057\u3066\u3001400\u3092\u542b\u3080\u884c\u3092\u6392\u9664\u3057\u305f\u7d50\u679c\u3092\u8868\u793a # * -e: \u8907\u6570\u30ad\u30fc\u30ef\u30fc\u30c9\u3092AND\u6761\u4ef6\u3067\u6307\u5b9a(\u7531\u6765: ?? \u305f\u3076\u3093\u9055\u3046\u3051\u3069\u3001\u500b\u4eba\u7684\u306b\u306f\u30d5\u30e9\u30f3\u30b9\u8a9e\u306eet(=and)\u3060\u3068\u89e3\u91c8\u3057\u3066\u308b) # * -v: \u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u542b\u3080\u884c\u3092\u6392\u9664(\u7531\u6765: verbose??) # \u3069\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u5206\u304b\u3089\u306a\u3044\u3051\u3069\u3001Hoge\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u3067\u3001piyo\u3068\u3044\u3046\u6587\u5b57\u5217\u3092\u542b\u3093\u3067\u3044\u308b\u90e8\u5206\u3068\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u77e5\u308a\u305f\u3044 find ~/Hoge -name '*.txt' | xargs grep piyo $ pgrep -f vagrant # \u59cb\u672b $ pkill -f vagrant # \u30b7\u30b0\u30ca\u30eb\u3092\u6307\u5b9a $ pkill -SIGKILL -f vagrant https://qiita.com/uraura/items/12ff6112fd392f1be424 find \u00b6 1 2 3 4 5 6 7 8 9 10 # find dir1 -type f: dir1\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type f -name \"*.js\": dir1\u4ee5\u4e0b\u306ejs\u30d5\u30a1\u30a4\u30eb\u306e\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type d: dir1\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4e00\u89a7\u3092\u8868\u793a # * ls\u3068\u9055\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u304c\u51fa\u529b\u3055\u308c\u308b\u305f\u3081\u3001find xxx | xargs rm -rf \u307f\u305f\u3044\u306b\u4e00\u62ec\u64cd\u4f5c\u306b\u5411\u3044\u3066\u3044\u308b $ find src/ -type f find . -name '*.php' find . -name '???.txt' find . -name \"*.jpg\" | wc -l https://uguisu.skr.jp/Windows/find_xargs2.html sed \u00b6 1 2 3 4 5 6 7 # cat file1 | sed 's/BEFORE/AFTER/g': file1\u4e2d\u306eBEFORE\u3092AFTER\u306b\u4e00\u62ec\u7f6e\u63db # * s/BEFORE/AFTER/g: BEFORE\u3092AFTER\u306b\u7f6e\u63db(\u7531\u6765: substitute\u3068global?) #\u30b5\u30d6\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u542b\u3081\u305f\u6587\u5b57\u5217\u306e\u4e00\u62ec\u5909\u63db find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} \\; find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} + find ./ -type f | xargs sed -i \"s/hoge/fuga/g\" xargs \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # cmd1 | xargs cmd2: cmd1\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3068\u3057\u3066\u53d7\u3051\u53d6\u3063\u3066\u3001cmd2\u3092\u5b9f\u884c find . -name \"*.log\" | xargs rm -fv find TARGET -type d -empty | xargs rm -r find . -name \"*.log\" | xargs -i cp {} /tmp/. #\u6587\u5b57\u5217\u5909\u63db grep -rl 'hogehoge' ./* | xargs perl -i -pe \"s/hogehoge/fugafuga/g\" #100\u30d5\u30a1\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3093\u3067\u30b3\u30d4\u30fc\uff0e find /some/dir -type f -name \"*.jpg\" | shuf -n 100 | xargs cp -vt /target/dir/ #\u691c\u7d22\u3057\u3066\u898b\u3064\u304b\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5 find . -type f -print0 | xargs -0 mv -t /var/tmp/ https://uguisu.skr.jp/Windows/find_xargs2.html <, > , >> (\u30ea\u30c0\u30a4\u30ec\u30af\u30c8) \u00b6 1 2 3 4 5 6 7 8 9 10 11 $ echo \"4 ddd DDD\" >> file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u8ffd\u8a18) $ cat file1.txt $ echo \"4 ddd DDD\" > file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u4e0a\u66f8\u304d) #Python\u30b9\u30af\u30ea\u30d7\u30c8\u3078\u306e\u5165\u529b\u3092\u30d5\u30a1\u30a4\u30ebinput_data\u3078\u5909\u66f4\u3057\u3001 #\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30ebresult01\u306b\u51fa\u529b $ python3 sample02.py < input_data > result01 #sample02.py\u306e\u5b9f\u884c\u7d50\u679c\u3068\u30a8\u30e9\u30fc\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30ebresult02\u306b\u51fa\u529b $ python3 sample02.py > result02 2 > & 1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u00b6 echo \u00b6 1 2 3 # echo abc: \u6587\u5b57\u5217abc\u3092\u51fa\u529b # echo $PATH: \u74b0\u5883\u5909\u6570PATH\u3092\u51fa\u529b # print\u3068\u4e00\u7dd2 env \u00b6 1 2 # env | less: \u74b0\u5883\u5909\u6570\u3092\u78ba\u8a8d # * env\u3060\u3051\u3067\u3082\u898b\u308c\u308b\u304c\u3001\u74b0\u5883\u5909\u6570\u304c\u591a\u3044\u5834\u5408\u898b\u5207\u308c\u3066\u3057\u307e\u3046\u305f\u3081less\u3067\u78ba\u8a8d which \u00b6 1 # which cmd: cmd\u306e\u5b9f\u4f53\u304c\u7f6e\u304b\u308c\u3066\u3044\u308b\u5834\u6240\u3092\u8868\u793a source \u00b6 1 2 3 # source ~/.bashrc: .bashrc\u3092\u518d\u8aad\u307f\u8fbc\u307f # . ~/.bashrc: \u2191\u3068\u540c\u3058(.\u306fsource\u306e\u30a8\u30a4\u30ea\u30a2\u30b9) # * \u30b7\u30a7\u30eb\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u5909\u66f4\u3057\u305f\u5f8c\u306e\u518d\u8aad\u307f\u8fbc\u307f\u3067\u4f7f\u3046\u30b1\u30fc\u30b9\u304c100% chmod \u00b6 1 2 3 # chmod 755 *.sh: sh\u30d5\u30a1\u30a4\u30eb\u306b\u5b9f\u884c\u6a29\u9650\u3092\u4ed8\u4e0e # chmod 644 *.js: js\u30d5\u30a1\u30a4\u30eb\u3092\u666e\u901a\u306b\u8aad\u307f\u66f8\u304d\u3067\u304d\u308b\u8a2d\u5b9a\u306b\u3059\u308b # * \u8b0e\u306e\u6570\u5b57\u306b\u3082\u3061\u3083\u3093\u3068\u610f\u5473\u304c\u3042\u308b\u3093\u3060\u3051\u3069\u3001\u6b63\u76f4644\u3068755\u3057\u304b\u4f7f\u308f\u306a\u3044 OS\u95a2\u9023 \u00b6 df \u00b6 1 2 # df -h: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # df: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u8868 du \u00b6 1 2 3 4 5 # du -h: \u5404\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # \u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u8868\u793a\uff0e\u6df1\u30551 du -h -d1 . du -h -d 1 | sort -h free \u00b6 1 # free -h: \u30e1\u30e2\u30ea\u4f7f\u7528\u72b6\u6cc1\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) top, ps, kill \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # top: CPU\u3084\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u72b6\u6cc1\u3092\u78ba\u8a8d # * \u30c7\u30d5\u30a9\u30eb\u30c8\u3060\u3068CPU\u4f7f\u7528\u7387\u306e\u591a\u3044\u30d7\u30ed\u30bb\u30b9\u304c\u4e0a\u306b\u6765\u308b # * %CPU\u304cCPU\u4f7f\u7528\u7387\u3002\u3069\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u9ad8\u8ca0\u8377\u304b\u3092\u78ba\u8a8d\u3067\u304d\u308b\u3002 # ps -ef: \u5168\u3066\u306e\u30d7\u30ed\u30bb\u30b9\u306e\u8a73\u7d30\u306a\u60c5\u5831\u3092\u898b\u308b(\u7531\u6765: every, full) # * \u7528\u90141: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u304c\u751f\u304d\u3066\u308b\u304b\u3069\u3046\u304b\u30c1\u30a7\u30c3\u30af (web\u30b5\u30fc\u30d0\u8d77\u52d5\u3057\u3066\u308b?) # * \u7528\u90142: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u306ePID(\u30d7\u30ed\u30bb\u30b9ID)\u3092\u30c1\u30a7\u30c3\u30af -> kill ${PID} # kill 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u505c\u6b62\u3055\u305b\u308b(SIGTERM\u3092\u9001\u308b) # kill -9 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u554f\u7b54\u7121\u7528\u3067\u6bba\u3059(9\u306fSIGKILL\u306e\u30b7\u30b0\u30ca\u30eb\u756a\u53f7) # kill -KILL 123: -9\u3068\u540c\u3058 # pkill process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u7d42\u4e86\u3055\u305b\u308b # pkill -9 process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u554f\u7b54\u7121\u7528\u3067\u7d42\u4e86\u3055\u305b\u308b \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c \u00b6 1 2 3 4 # cmd1: cmd1\u3092\u30d5\u30a9\u30a2\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c # cmd1 &: cmd1\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c # * \u91cd\u305f\u3044\u30d0\u30c3\u30c1\u51e6\u7406\u3084\u3001\u4e00\u6642\u7684\u306bweb\u30b5\u30fc\u30d0\u3092\u52d5\u304b\u3057\u305f\u3044\u3068\u304d\u306f\u3001 # \u30b3\u30de\u30f3\u30c9\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c\u3059\u308b\u3068\u4fbf\u5229 &&, || \u00b6 1 2 3 4 # cmd1 && cmd2: cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # cmd1 || cmd2: cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # * \u7528\u90141: \u30ef\u30f3\u30e9\u30a4\u30ca\u30fc\u3067\u3061\u3087\u3063\u3068\u3057\u305f\u9010\u6b21\u51e6\u7406\u3092\u66f8\u304f # * \u7528\u90142: cmd1 || echo \"error message\" \u30ea\u30e2\u30fc\u30c8 \u00b6 ssh \u00b6 1 2 3 4 5 6 7 8 9 10 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -L: \u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0 ssh -L <host port>:localhost:<remote port> user@remote #https://qiita.com/wnoguchi/items/a72a042bb8159c35d056 # ECDSA521 bit ssh-keygen -t ecdsa -b 521 -C \"wnoguchi-mbp\" # \u5727\u5012\u7684\u306b\u610f\u8b58\u9ad8\u3044 Ed25519 ssh-keygen -t ed25519 -P \"\" -f serial-server.pem ssh-keygen -t ed25519 scp \u00b6 1 2 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -r : \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u79fb\u52d5 tmux \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # \u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux # \u540d\u524d\u3092\u3064\u3051\u3066\u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux new -s <\u30bb\u30c3\u30b7\u30e7\u30f3\u540d> # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4e00\u89a7\u8868\u793a tmux ls # \u63a5\u7d9a\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u4e00\u89a7\u8868\u793a tmux lsc # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u518d\u958b \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux a -t [ session-name ] # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u7d42\u4e86 \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux kill-session -t [ session-name ] # tmux\u5168\u4f53\u3092\u7d42\u4e86 tmux kill-server # \u305d\u306e\u4ed6\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c tmux [ command [ flags ]] ctrl-b c : \u65b0\u3057\u3044\u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u8ffd\u52a0 ctrl-b b : \u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u79fb\u52d5 ctrl-b & : \u30a2\u30af\u30c6\u30a3\u30d6\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u524a\u9664 ctrl-b % : \u5de6\u53f3\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 ctrl-b \" : \u4e0a\u4e0b\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 : ctrl-b o or \u30ab\u30fc\u30bd\u30eb\uff1a\u30da\u30a4\u30f3\u9593\u306e\u79fb\u52d5 ctrl-b x : \u30da\u30a4\u30f3\u5206\u5272\u306e\u89e3\u9664 ctrl-b d: detach","title":"Linux"},{"location":"linux_command/#linux","text":"","title":"Linux"},{"location":"linux_command/#linux_1","text":"","title":"Linux\u8a2d\u5b9a\u95a2\u9023"},{"location":"linux_command/#alias","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # \u3088\u304f\u5229\u7528\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u982d\u6587\u5b57\u306e\u9023\u7d50 alias abc = 'cd ~/aaa/bbb/ccc' alias g = 'git' alias ga = 'git add' alias gd = 'git diff' alias gs = 'git status' alias gp = 'git push' alias gb = 'git branch' alias gst = 'git status' alias gco = 'git checkout' alias gf = 'git fetch' alias gc = 'git commit' alias cp = 'cp -i' alias mv = 'mv -i' alias rm = 'rm -i' alias hg = 'history | grep' alias tma = 'tmux a -t' alias tmn = 'tmux new-session -s' # u","title":"\u30b7\u30a7\u30eb\u306eAlias\u306e\u8a2d\u5b9a"},{"location":"linux_command/#vimrc","text":"1 2 3 4 5 6 7 8 9 10 11 set number set expandtab set hlsearch set ignorecase set incsearch set smartcase set laststatus=2 set nocompatible set clipboard=unnamed,autoselect set clipboard& clipboard^=unnamedplus syntax on","title":"\u304a\u3059\u3059\u3081.vimrc"},{"location":"linux_command/#netrc","text":"1 2 3 4 5 6 7 machine api.wandb.ai login user password **** machine github.com login your_username password ***","title":".netrc\u306e\u66f8\u304d\u65b9"},{"location":"linux_command/#linux_2","text":"","title":"Linux\u30b3\u30de\u30f3\u30c9"},{"location":"linux_command/#_1","text":"https://atmarkit.itmedia.co.jp/ait/articles/1906/05/news004.html https://qiita.com/nmrmsys/items/03f97f5eabec18a3a18b https://qiita.com/arene-calix/items/41d8d4ba572f1d652727 https://qiita.com/savaniased/items/d2c5c699188a0f1623ef https://tech-blog.rakus.co.jp/entry/20210604/linux https://pg-happy.jp/linux-command.html","title":"\u53c2\u8003\u30ea\u30f3\u30af"},{"location":"linux_command/#_2","text":"","title":"\u30d5\u30a1\u30a4\u30eb\u30fb\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u95a2\u9023"},{"location":"linux_command/#pwd-print-working-directory","text":"1 2 3 # pwd: \u4eca\u3044\u308b\u30d5\u30a9\u30eb\u30c0\u306e\u7d76\u5bfe\u30d1\u30b9\u3092\u8868\u793a yseeker@~/Desktop $ pwd /home/yseeker/Desktop","title":"pwd (print working directory)"},{"location":"linux_command/#ls-list","text":"1 2 3 4 5 6 7 ls -alh ls -ltr # -a: \u96a0\u3057\u30d5\u30a1\u30a4\u30eb\u3082\u8868\u793a(\u7531\u6765: all) # -l: \u8a73\u7d30\u306a\u60c5\u5831\u3092\u8868\u793a # -h: M(\u30e1\u30ac)\u3001G(\u30ae\u30ac)\u306a\u3069\u3092\u4ed8\u3051\u3066\u30b5\u30a4\u30ba\u3092\u898b\u3084\u3059\u304f\u3059\u308b(\u7531\u6765:human readable) # -t: \u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u9806\u3067\u8868\u793a(\u7531\u6765: time) # -r: \u9006\u9806\u3067\u8868\u793a(\u7531\u6765: reverse)","title":"ls (list)"},{"location":"linux_command/#cd-change-directory","text":"1 cd - #\u76f4\u524d\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u79fb\u52d5","title":"cd (change directory)"},{"location":"linux_command/#mkdir-make-directory","text":"1 mkdir -p aaa/bbb/ccc #\u6df1\u3044\u968e\u5c64\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u4e00\u6c17\u306b\u4f5c\u6210","title":"mkdir (make directory)"},{"location":"linux_command/#touch","text":"\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\u3001\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\u66f4\u65b0","title":"touch"},{"location":"linux_command/#mv","text":"\u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5\u3001\u540d\u524d\u5909\u66f4","title":"mv"},{"location":"linux_command/#cp","text":"1 2 3 4 # cp -r source/path destination/path: \u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30b3\u30d4\u30fc # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u30b3\u30d4\u30fc(ecursive) # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(force) # * -p: \u30b3\u30d4\u30fc\u524d\u5f8c\u3067\u30d1\u30fc\u30df\u30c3\u30b7\u30e7\u30f3\u3092\u4fdd\u6301(permission)","title":"cp"},{"location":"linux_command/#rm","text":"1 2 3 4 5 6 # rm -rf directory_name: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u524a\u9664 # * -f: \u78ba\u8a8d\u7121\u3057\u3067\u5f37\u5236\u30b3\u30d4\u30fc(\u7531\u6765: force) # * -r: \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4ee5\u4e0b\u3092\u518d\u5e30\u7684\u306b\u524a\u9664(\u7531\u6765: recursive) $ rm -f *.txt # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066txt\u30d5\u30a1\u30a4\u30eb\u3092\u5168\u524a\u9664 $ rm -rf dir* # \u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9(*)\u3092\u4f7f\u3063\u3066\u4e00\u62ec\u524a\u9664","title":"rm"},{"location":"linux_command/#tar","text":"1 2 3 4 5 # tar -czvf xxx.tgz file1 file2 dir1 : \u5727\u7e2e(file1 file2 dir1\u3092\u30a2\u30fc\u30ab\u30a4\u30d6\u3057\u305f\u5727\u7e2e\u30d5\u30a1\u30a4\u30ebxxx.tgz\u3092\u4f5c\u6210) # tar -tzvf xxx.tgz: \u5727\u7e2e\u30d5\u30a1\u30a4\u30eb\u306b\u542b\u307e\u308c\u308b\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u8868\u793a(=\u5c55\u958b\u306e\u30c6\u30b9\u30c8) # tar -xzvf xxx.tgz: \u5c55\u958b # * c(create), t(test), x(extract) + zvf\u3068\u899a\u3048\u308b tar czvf something.tgz dir* file*","title":"tar"},{"location":"linux_command/#zip-unzip","text":"1 2 3 $ zip -r \u00abZIP\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb $ tar cvzf \u00abTARGZ\u30d5\u30a1\u30a4\u30eb\u540d\u00bb \u00ab\u5bfe\u8c61\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u00bb unzip -q foo.zip -d bar","title":"zip, unzip"},{"location":"linux_command/#_3","text":"","title":"\u30c6\u30ad\u30b9\u30c8\u51e6\u7406"},{"location":"linux_command/#cat-concatenate","text":"1 2 access.log error1.log error2.log $ cat error*.log # error1.log\u3068error2.log\u3092\u307e\u3068\u3081\u3066\u78ba\u8a8d","title":"cat (concatenate)"},{"location":"linux_command/#tail","text":"1 2 3 4 5 # \u91cd\u305f\u3044\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u6700\u5f8c\u306e\u65b9\u3060\u3051\u898b\u308b tail -n 3 file1.txt 11 kkk KKK 12 lll LLL 13 mmm MMM","title":"tail"},{"location":"linux_command/#less","text":"1 2 3 4 5 6 7 8 9 10 # less file1: file1\u3092\u898b\u308b(read only) # cat file1 | cmd1 | cmd2 | less: file1\u3092\u3044\u308d\u3044\u308d\u52a0\u5de5\u3057\u305f\u7d50\u679c\u3092\u898b\u308b # command | less - # grep 080 testData | less -N # less +F output # * \u30bf\u30fc\u30df\u30ca\u30eb\u306b\u51fa\u529b\u305b\u305a\u3001\u4f55\u304b\u3092\u898b\u305f\u3044\u3068\u304d\u306b\u3068\u308a\u3042\u3048\u305a\u4f7f\u3046\u30b3\u30de\u30f3\u30c9 # gg: \u5148\u982d\u884c\u3078\u79fb\u52d5 # G: \u6700\u7d42\u884c\u3078\u79fb\u52d5 # /pattern: pattern\u3067\u30d5\u30a1\u30a4\u30eb\u5185\u691c\u7d22 # q: \u9589\u3058\u308b","title":"less"},{"location":"linux_command/#wc-word-count","text":"1 2 3 4 5 6 7 $ wc -l error.log # \u884c\u6570\u30ab\u30a6\u30f3\u30c8(1) 7 error.log # \u30d5\u30a1\u30a4\u30eb\u6570\u30ab\u30a6\u30f3\u30c8 ls | wc -w find . -name \"*.jpg\" | wc -l ls -F | grep -v / | wc -l","title":"wc (word count)"},{"location":"linux_command/#sort-uniq","text":"1 2 3 4 5 6 7 # sort file1: file1\u3092\u884c\u5358\u4f4d\u3067\u30bd\u30fc\u30c8 # uniq file1: file1\u306e\u91cd\u8907\u696d\u3092\u524a\u9664 # cat file1 | sort | uniq: file1\u3092\u30bd\u30fc\u30c8\u3057\u3066\u3001\u91cd\u8907\u696d\u3092\u6392\u9664 # * sort\u3068uniq\u306f\u30ef\u30f3\u30bb\u30c3\u30c8\u7684\u306a\u3068\u3053\u308d\u304c\u3042\u308b\u306e\u3067\u307e\u3068\u3081\u3066\u7d39\u4ecb # # * sort\u306f-r\u3067\u9006\u9806\u30bd\u30fc\u30c8\u3001-R\u3067\u30e9\u30f3\u30c0\u30e0\u30bd\u30fc\u30c8\u3001\u307f\u305f\u3044\u306b\u7d50\u69cb\u30aa\u30d7\u30b7\u30e7\u30f3\u304c\u591a\u5f69 # * ls -l\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba\u9806\u3067sort\u3059\u308b\u3001\u307f\u305f\u3044\u306b","title":"sort, uniq"},{"location":"linux_command/#grep","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # grep ERROR *.log: \u62e1\u5f35\u5b50\u304clog\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u3001ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3060\u3051\u62bd\u51fa # cat error.log | grep -2 ERROR: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3068\u305d\u306e\u524d\u5f8c2\u884c\u3092\u51fa\u529b # cat error.log | grep -e ERROR -e WARN: error.log\u304b\u3089ERROR\u307e\u305f\u306fWARN\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa # cat error.log | grep ERROR | grep -v 400: error.log\u304b\u3089ERROR\u3092\u542b\u3080\u884c\u3092\u62bd\u51fa\u3057\u3066\u3001400\u3092\u542b\u3080\u884c\u3092\u6392\u9664\u3057\u305f\u7d50\u679c\u3092\u8868\u793a # * -e: \u8907\u6570\u30ad\u30fc\u30ef\u30fc\u30c9\u3092AND\u6761\u4ef6\u3067\u6307\u5b9a(\u7531\u6765: ?? \u305f\u3076\u3093\u9055\u3046\u3051\u3069\u3001\u500b\u4eba\u7684\u306b\u306f\u30d5\u30e9\u30f3\u30b9\u8a9e\u306eet(=and)\u3060\u3068\u89e3\u91c8\u3057\u3066\u308b) # * -v: \u30ad\u30fc\u30ef\u30fc\u30c9\u3092\u542b\u3080\u884c\u3092\u6392\u9664(\u7531\u6765: verbose??) # \u3069\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u5206\u304b\u3089\u306a\u3044\u3051\u3069\u3001Hoge\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u3067\u3001piyo\u3068\u3044\u3046\u6587\u5b57\u5217\u3092\u542b\u3093\u3067\u3044\u308b\u90e8\u5206\u3068\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3092\u77e5\u308a\u305f\u3044 find ~/Hoge -name '*.txt' | xargs grep piyo $ pgrep -f vagrant # \u59cb\u672b $ pkill -f vagrant # \u30b7\u30b0\u30ca\u30eb\u3092\u6307\u5b9a $ pkill -SIGKILL -f vagrant https://qiita.com/uraura/items/12ff6112fd392f1be424","title":"grep"},{"location":"linux_command/#find","text":"1 2 3 4 5 6 7 8 9 10 # find dir1 -type f: dir1\u4ee5\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type f -name \"*.js\": dir1\u4ee5\u4e0b\u306ejs\u30d5\u30a1\u30a4\u30eb\u306e\u4e00\u89a7\u3092\u8868\u793a # find dir1 -type d: dir1\u4ee5\u4e0b\u306e\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4e00\u89a7\u3092\u8868\u793a # * ls\u3068\u9055\u3063\u3066\u30d5\u30a1\u30a4\u30eb\u30d1\u30b9\u304c\u51fa\u529b\u3055\u308c\u308b\u305f\u3081\u3001find xxx | xargs rm -rf \u307f\u305f\u3044\u306b\u4e00\u62ec\u64cd\u4f5c\u306b\u5411\u3044\u3066\u3044\u308b $ find src/ -type f find . -name '*.php' find . -name '???.txt' find . -name \"*.jpg\" | wc -l https://uguisu.skr.jp/Windows/find_xargs2.html","title":"find"},{"location":"linux_command/#sed","text":"1 2 3 4 5 6 7 # cat file1 | sed 's/BEFORE/AFTER/g': file1\u4e2d\u306eBEFORE\u3092AFTER\u306b\u4e00\u62ec\u7f6e\u63db # * s/BEFORE/AFTER/g: BEFORE\u3092AFTER\u306b\u7f6e\u63db(\u7531\u6765: substitute\u3068global?) #\u30b5\u30d6\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u542b\u3081\u305f\u6587\u5b57\u5217\u306e\u4e00\u62ec\u5909\u63db find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} \\; find ./ -name '*.php' -exec sed -i 's/TYPO/TYPE/g' {} + find ./ -type f | xargs sed -i \"s/hoge/fuga/g\"","title":"sed"},{"location":"linux_command/#xargs","text":"1 2 3 4 5 6 7 8 9 10 11 12 # cmd1 | xargs cmd2: cmd1\u306e\u5b9f\u884c\u7d50\u679c\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u5f15\u6570\u3068\u3057\u3066\u53d7\u3051\u53d6\u3063\u3066\u3001cmd2\u3092\u5b9f\u884c find . -name \"*.log\" | xargs rm -fv find TARGET -type d -empty | xargs rm -r find . -name \"*.log\" | xargs -i cp {} /tmp/. #\u6587\u5b57\u5217\u5909\u63db grep -rl 'hogehoge' ./* | xargs perl -i -pe \"s/hogehoge/fugafuga/g\" #100\u30d5\u30a1\u30a4\u30eb\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3093\u3067\u30b3\u30d4\u30fc\uff0e find /some/dir -type f -name \"*.jpg\" | shuf -n 100 | xargs cp -vt /target/dir/ #\u691c\u7d22\u3057\u3066\u898b\u3064\u304b\u3063\u305f\u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5 find . -type f -print0 | xargs -0 mv -t /var/tmp/ https://uguisu.skr.jp/Windows/find_xargs2.html","title":"xargs"},{"location":"linux_command/#_4","text":"1 2 3 4 5 6 7 8 9 10 11 $ echo \"4 ddd DDD\" >> file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u8ffd\u8a18) $ cat file1.txt $ echo \"4 ddd DDD\" > file1.txt # \u30ea\u30c0\u30a4\u30ec\u30af\u30c8(\u4e0a\u66f8\u304d) #Python\u30b9\u30af\u30ea\u30d7\u30c8\u3078\u306e\u5165\u529b\u3092\u30d5\u30a1\u30a4\u30ebinput_data\u3078\u5909\u66f4\u3057\u3001 #\u5b9f\u884c\u7d50\u679c\u3092\u30d5\u30a1\u30a4\u30ebresult01\u306b\u51fa\u529b $ python3 sample02.py < input_data > result01 #sample02.py\u306e\u5b9f\u884c\u7d50\u679c\u3068\u30a8\u30e9\u30fc\u51fa\u529b\u3092\u30d5\u30a1\u30a4\u30ebresult02\u306b\u51fa\u529b $ python3 sample02.py > result02 2 > & 1","title":"&lt;, &gt; , &gt;&gt; (\u30ea\u30c0\u30a4\u30ec\u30af\u30c8)"},{"location":"linux_command/#_5","text":"","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"linux_command/#echo","text":"1 2 3 # echo abc: \u6587\u5b57\u5217abc\u3092\u51fa\u529b # echo $PATH: \u74b0\u5883\u5909\u6570PATH\u3092\u51fa\u529b # print\u3068\u4e00\u7dd2","title":"echo"},{"location":"linux_command/#env","text":"1 2 # env | less: \u74b0\u5883\u5909\u6570\u3092\u78ba\u8a8d # * env\u3060\u3051\u3067\u3082\u898b\u308c\u308b\u304c\u3001\u74b0\u5883\u5909\u6570\u304c\u591a\u3044\u5834\u5408\u898b\u5207\u308c\u3066\u3057\u307e\u3046\u305f\u3081less\u3067\u78ba\u8a8d","title":"env"},{"location":"linux_command/#which","text":"1 # which cmd: cmd\u306e\u5b9f\u4f53\u304c\u7f6e\u304b\u308c\u3066\u3044\u308b\u5834\u6240\u3092\u8868\u793a","title":"which"},{"location":"linux_command/#source","text":"1 2 3 # source ~/.bashrc: .bashrc\u3092\u518d\u8aad\u307f\u8fbc\u307f # . ~/.bashrc: \u2191\u3068\u540c\u3058(.\u306fsource\u306e\u30a8\u30a4\u30ea\u30a2\u30b9) # * \u30b7\u30a7\u30eb\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u5909\u66f4\u3057\u305f\u5f8c\u306e\u518d\u8aad\u307f\u8fbc\u307f\u3067\u4f7f\u3046\u30b1\u30fc\u30b9\u304c100%","title":"source"},{"location":"linux_command/#chmod","text":"1 2 3 # chmod 755 *.sh: sh\u30d5\u30a1\u30a4\u30eb\u306b\u5b9f\u884c\u6a29\u9650\u3092\u4ed8\u4e0e # chmod 644 *.js: js\u30d5\u30a1\u30a4\u30eb\u3092\u666e\u901a\u306b\u8aad\u307f\u66f8\u304d\u3067\u304d\u308b\u8a2d\u5b9a\u306b\u3059\u308b # * \u8b0e\u306e\u6570\u5b57\u306b\u3082\u3061\u3083\u3093\u3068\u610f\u5473\u304c\u3042\u308b\u3093\u3060\u3051\u3069\u3001\u6b63\u76f4644\u3068755\u3057\u304b\u4f7f\u308f\u306a\u3044","title":"chmod"},{"location":"linux_command/#os","text":"","title":"OS\u95a2\u9023"},{"location":"linux_command/#df","text":"1 2 # df -h: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # df: \u30c7\u30a3\u30b9\u30af\u306e\u4f7f\u7528\u91cf/\u7a7a\u304d\u5bb9\u91cf\u3092\u8868","title":"df"},{"location":"linux_command/#du","text":"1 2 3 4 5 # du -h: \u5404\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable) # \u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5bb9\u91cf\u3092\u8868\u793a\uff0e\u6df1\u30551 du -h -d1 . du -h -d 1 | sort -h","title":"du"},{"location":"linux_command/#free","text":"1 # free -h: \u30e1\u30e2\u30ea\u4f7f\u7528\u72b6\u6cc1\u3092\u5358\u4f4d\u4ed8\u304d\u3067\u8868\u793a(\u7531\u6765: human readable)","title":"free"},{"location":"linux_command/#top-ps-kill","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 # top: CPU\u3084\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u72b6\u6cc1\u3092\u78ba\u8a8d # * \u30c7\u30d5\u30a9\u30eb\u30c8\u3060\u3068CPU\u4f7f\u7528\u7387\u306e\u591a\u3044\u30d7\u30ed\u30bb\u30b9\u304c\u4e0a\u306b\u6765\u308b # * %CPU\u304cCPU\u4f7f\u7528\u7387\u3002\u3069\u306e\u30d7\u30ed\u30bb\u30b9\u304c\u9ad8\u8ca0\u8377\u304b\u3092\u78ba\u8a8d\u3067\u304d\u308b\u3002 # ps -ef: \u5168\u3066\u306e\u30d7\u30ed\u30bb\u30b9\u306e\u8a73\u7d30\u306a\u60c5\u5831\u3092\u898b\u308b(\u7531\u6765: every, full) # * \u7528\u90141: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u304c\u751f\u304d\u3066\u308b\u304b\u3069\u3046\u304b\u30c1\u30a7\u30c3\u30af (web\u30b5\u30fc\u30d0\u8d77\u52d5\u3057\u3066\u308b?) # * \u7528\u90142: \u3042\u308b\u30d7\u30ed\u30bb\u30b9\u306ePID(\u30d7\u30ed\u30bb\u30b9ID)\u3092\u30c1\u30a7\u30c3\u30af -> kill ${PID} # kill 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u505c\u6b62\u3055\u305b\u308b(SIGTERM\u3092\u9001\u308b) # kill -9 123: \u30d7\u30ed\u30bb\u30b9ID\u304c123\u306e\u30d7\u30ed\u30bb\u30b9\u3092\u554f\u7b54\u7121\u7528\u3067\u6bba\u3059(9\u306fSIGKILL\u306e\u30b7\u30b0\u30ca\u30eb\u756a\u53f7) # kill -KILL 123: -9\u3068\u540c\u3058 # pkill process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u7d42\u4e86\u3055\u305b\u308b # pkill -9 process_name_prefix: process_name_prefix\u3067\u59cb\u307e\u308b\u30d7\u30ed\u30bb\u30b9\u3059\u3079\u3066\u3092\u554f\u7b54\u7121\u7528\u3067\u7d42\u4e86\u3055\u305b\u308b","title":"top, ps, kill"},{"location":"linux_command/#_6","text":"1 2 3 4 # cmd1: cmd1\u3092\u30d5\u30a9\u30a2\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c # cmd1 &: cmd1\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c # * \u91cd\u305f\u3044\u30d0\u30c3\u30c1\u51e6\u7406\u3084\u3001\u4e00\u6642\u7684\u306bweb\u30b5\u30fc\u30d0\u3092\u52d5\u304b\u3057\u305f\u3044\u3068\u304d\u306f\u3001 # \u30b3\u30de\u30f3\u30c9\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c\u3059\u308b\u3068\u4fbf\u5229","title":"\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c"},{"location":"linux_command/#_7","text":"1 2 3 4 # cmd1 && cmd2: cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # cmd1 || cmd2: cmd1\u304c\u5931\u6557\u3057\u305f\u3089\u3001cmd2\u3092\u5b9f\u884c(cmd1\u304c\u6210\u529f\u3057\u305f\u3089\u305d\u3053\u3067\u7d42\u308f\u308a) # * \u7528\u90141: \u30ef\u30f3\u30e9\u30a4\u30ca\u30fc\u3067\u3061\u3087\u3063\u3068\u3057\u305f\u9010\u6b21\u51e6\u7406\u3092\u66f8\u304f # * \u7528\u90142: cmd1 || echo \"error message\"","title":"&amp;&amp;, ||"},{"location":"linux_command/#_8","text":"","title":"\u30ea\u30e2\u30fc\u30c8"},{"location":"linux_command/#ssh","text":"1 2 3 4 5 6 7 8 9 10 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -L: \u30dd\u30fc\u30c8\u30d5\u30a9\u30ef\u30fc\u30c7\u30a3\u30f3\u30b0 ssh -L <host port>:localhost:<remote port> user@remote #https://qiita.com/wnoguchi/items/a72a042bb8159c35d056 # ECDSA521 bit ssh-keygen -t ecdsa -b 521 -C \"wnoguchi-mbp\" # \u5727\u5012\u7684\u306b\u610f\u8b58\u9ad8\u3044 Ed25519 ssh-keygen -t ed25519 -P \"\" -f serial-server.pem ssh-keygen -t ed25519","title":"ssh"},{"location":"linux_command/#scp","text":"1 2 # -i : \u9375\u30d5\u30a1\u30a4\u30eb # -r : \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u79fb\u52d5","title":"scp"},{"location":"linux_command/#tmux","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # \u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux # \u540d\u524d\u3092\u3064\u3051\u3066\u65b0\u898f\u30bb\u30c3\u30b7\u30e7\u30f3\u958b\u59cb tmux new -s <\u30bb\u30c3\u30b7\u30e7\u30f3\u540d> # \u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u4e00\u89a7\u8868\u793a tmux ls # \u63a5\u7d9a\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u4e00\u89a7\u8868\u793a tmux lsc # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u518d\u958b \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux a -t [ session-name ] # \u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u7d42\u4e86 \u203b-t <\u5bfe\u8c61\u30bb\u30c3\u30b7\u30e7\u30f3\u540d>\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u540d\u306e\u6307\u5b9a\u3082\u53ef\u80fd tmux kill-session -t [ session-name ] # tmux\u5168\u4f53\u3092\u7d42\u4e86 tmux kill-server # \u305d\u306e\u4ed6\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c tmux [ command [ flags ]] ctrl-b c : \u65b0\u3057\u3044\u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u8ffd\u52a0 ctrl-b b : \u30a6\u30a4\u30f3\u30c9\u30a6\u3092\u79fb\u52d5 ctrl-b & : \u30a2\u30af\u30c6\u30a3\u30d6\u306a\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u524a\u9664 ctrl-b % : \u5de6\u53f3\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 ctrl-b \" : \u4e0a\u4e0b\u306e\u30da\u30a4\u30f3\u306b\u79fb\u52d5 : ctrl-b o or \u30ab\u30fc\u30bd\u30eb\uff1a\u30da\u30a4\u30f3\u9593\u306e\u79fb\u52d5 ctrl-b x : \u30da\u30a4\u30f3\u5206\u5272\u306e\u89e3\u9664 ctrl-b d: detach","title":"tmux"},{"location":"linux_command_old/","text":"\u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u00b6 pwd \u00b6 pwd \u00b6 ls \u00b6 1 cd \u00b6 cat, less, tail \u00b6 du, df \u00b6 mv, cp \u00b6 ssh, scp \u00b6 wget, curl \u00b6 touch \u00b6 find, rm \u00b6 kill, ps, pkill \u00b6 export \u00b6 \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c \u00b6 \u30ea\u30c0\u30a4\u30ec\u30af\u30c8> , >>, >&, 2>, 2>> \u00b6 \u30d7\u30ed\u30bb\u30b9\u7f6e\u63db \u00b6 \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3 \u00b6 https://webkaru.net/linux/redirect-output-to-file/ https://eng-entrance.com/linux-redirect https://shellscript.sunone.me/input_output.html python3 sample02.py < input_data > result01 tmux\u3067\u5b9f\u884c\u4e2d\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u898b\u308b ps aux | grep sleep tmux \u30b7\u30a7\u30eb\u306f\u30ab\u30fc\u30cd\u30eb\u306b\u547d\u4ee4\u3092\u51fa\u3057\u3066\u30ab\u30fc\u30cd\u30eb\u304b\u3089\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u305f\u3081\u306e\u3082\u306e bash \u306f\u30b7\u30a7\u30eb\u306e\uff11\u3064\u3000echo $SHELL\u3067\u78ba\u8a8d sh\u306e1\u3064 \u30bf\u30fc\u30df\u30ca\u30eb\uff1a\u5165\u51fa\u529b\u3000\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\uff08Linux\u306e\u5834\u5408\u3067\u306f\u5165\u51fa\u529b\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u6307\u3059\u3002\u30b3\u30de\u30f3\u30c9\u3092\u3046\u3051\u3068\u3063\u305f\u308a\u51fa\u529b\uff09 \u30d7\u30ed\u30f3\u30d7\u30c8\uff1a\u30ab\u30fc\u30bd\u30eb\u306e\u5de6\u5074[ ^^^^@ ~]\u3000\u30b3\u30de\u30f3\u30c9\u5165\u529b\u3092\u4fc3\u3059 \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3: \u30d7\u30ed\u30f3\u30d7\u30c8\u306e\u53f3\u5074 \u57fa\u672c\u64cd\u4f5c ctl + a: atama\u306b\u79fb\u52d5 ctrl + e: end\u306b\u79fb\u52d5 ctrl + w:\u3000 word: \u5358\u8a9e\u5358\u4f4d\u3067\u524a\u9664 \u30ab\u30c3\u30c8\u3000\u30a2\u30f3\u30c9\u3000\u30e4\u30f3\u30af ctrl + u \u884c\u982d\u307e\u3067\u30ab\u30c3\u30c8 ctrl + k:\u884c\u672b\u307e\u3067\u30ab\u30c3\u30c8 ctrl + y (yank) \u30bf\u30d6\u3067\u30aa\u30fc\u30c8\u30b3\u30f3\u30d7\u30ea\u30fc\u30c8 du -h --total df -h --total rm find\u9055\u3044 outout\u30d5\u30a1\u30a4\u30eb\u306b\u683c\u7d0d \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3 python -m logger conda venv \u74b0\u5883\u69cb\u7bc9 ls ls -a echo \u30b3\u30de\u30f3\u30c9 cat/less/tail cat >> cat > wget curl ping host ps kill grep \u30b3\u30de\u30f3\u30c9 space(\u4e00\u753b\u9762\u4e0b)\u3000b\uff08\u4e00\u753b\u9762\u4e0a\uff09 j\uff08\u4e00\u884c\u305a\u3064\u4e0b\uff09 k\uff08\u4e00\u884c\u305a\u3064\u4e0a\uff09 q \u306f\u3082\u3068\u306e\u753b\u9762\u306b\u623b\u308b\uff08quit\uff09 wget \u30b3\u30de\u30f3\u30c9 unzip touch \u30b3\u30de\u30f3\u30c9 rm \u3068rm -r echo \\(PATH export PATH = /path/to/something:\\) PATH\u3092\u8ffd\u52a0\u3059\u308b\u3002 cp /etc/crontab file2 cp file1 directory\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30b3\u30d4\u30fc\u53ef\u80fd \u6307\u5b9a\u3057\u305f\u30b3\u30d4\u30fc\u5148\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4e2d\u306b\u306a\u308b \u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u65b0\u3057\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308b\u3002 cp -r dir1 dir2\u3067\u518d\u5e30\u7684\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b3\u30d4\u30fc\u53ef\u80fd mv\u30b3\u30de\u30f3\u30c9\u3067\u540d\u524d\u3092\u5909\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3057\u3001\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5\u3067\u304d\u308b\u3002 sh -x \u3067\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u78ba\u8a8d docker \u30d5\u30a1\u30a4\u30eb\u306e\u3068\u304d\u306f-b -p\u3092\u3064\u3051\u308b\u3002 \u30cf\u30fc\u30c9\u30ea\u30f3\u30af \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af ln file1 file2 (file1\u306bfile2\u3068\u3044\u3046\u30cf\u30fc\u30c9\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b\u3002file1\u3092\u4f5c\u6210\u3057\u3066\u3082file2\u304c\u6b8b\u308b) ln -s file1 file2\u3067\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b mkdir -p dir1/dir2/dir3/target touch p dir1/dir2/dir3/target/file ln -s dir1/dir2/dir3/target/ target \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(/)\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(~)\u306e\u3061\u3083\u3093\u3068\u3057\u305f\u7406\u89e3 - Qiita \u3082\u306e\u3059\u3054\u3044\u7d30\u304b\u3044\u3053\u3068\u3060\u3051\u3069\u3001\u30d1\u30b9\u306e\u6307\u5b9a\u65b9\u6cd5\u3067\u306e ~ \uff08\u30c1\u30eb\u30c0\uff09\u3068 / \uff08\u30b9\u30e9\u30c3\u30b7\u30e5\uff09\u306e\u7406\u89e3\u304c\u66d6\u6627\u3067\u6c17\u6301\u3061\u60aa\u3044\u601d\u3044\u3092\u3057\u305f\u306e\u3067\u30e1\u30e2\u3002 / : \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~ \uff1a\u4eca\u306e\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~taro : taro\u3068\u3044\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea \u30b9\u30e9\u30c3\u30b7\u30e5\u306e\u610f\u5473\u5408\u3044 \u00b6 \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e / \u3068\u3001\u5404\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u524d\u306b\u3064\u304f / \u306f\u610f\u5473\u5408\u3044\u304c\u9055\u3063\u3066\u3044\u308b\u6a21\u69d8\u3002 \u524d\u8005\uff1a\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u305d\u306e\u3082\u306e \u5f8c\u8005\uff1a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u533a\u5207\u308b\u3082\u306e \u306a\u306e\u3067\u3001\u4e00\u898b\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u305b\u3044\u3067\u300c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u306f\u672b\u5c3e\u306b\u30b9\u30e9\u30c3\u30b7\u30e5\u304c\u4ed8\u3044\u3066\u3044\u308b\u3082\u306e\u300d\u3068\u3044\u3046\u52d8\u9055\u3044\u3092\uff08\u5c11\u306a\u304f\u3082\u7b46\u8005\u306f\uff09\u3057\u3061\u3083\u3046\u304c\u3001 hogehoge/ \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3067\u306f\u306a\u304f hogehoge \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3060\u3002\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ~/ \u3060\u3068\u601d\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u4eba\u306f\u591a\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f history !393\u3067\u4f7f\u3048\u308b \u30d4\u30ea\u30aa\u30c9\u3067\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea find . -name '*.txt' -print \u3053\u306e\u30a2\u30b9\u30bf\u30ea\u30b9\u30af\u306f\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3067\u30d1\u30b9\u540d\u5c55\u958b\u3068\u306f\u9055\u3046\u3002\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c6\u30b7\u30e7\u30f3\u304b\u3069\u3046\u304b find . -type d \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u691c\u7d22 find . -type d -a -name share locate \u30b3\u30de\u30f3\u30c9\u306ffind\u30b3\u30de\u30f3\u30c9\u3088\u308a\u3082\u9ad8\u901f\uff08\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u691c\u7d22\uff09 sudo updatedb\u3092\u3057\u3066\u304b\u3089 locate bash -A doc \u3000and \u691c\u7d22 locate bash doc grep bin /etc/crontab \u30d5\u30a3\u30eb\u30bf history | head wc:\u6587\u5b57\u6570\u3092\u6570\u3048\u308b wc -l ls / | wc -l \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u500b\u6570\u884c\u6570 \u30bd\u30fc\u30c8\u30b3\u30de\u30f3\u30c9 sort word.txt sort -r word.txt sort -n number.txt \u91cd\u8907\u3092\u53d6\u308a\u51fa\u3059 uniq number.txt sort -n number.txt | uniq sort -n number.txt | uniq -c | sort -nr | head -n 3 \u30d5\u30a1\u30a4\u30eb\u3092\u76e3\u8996\u3059\u308b tail -f log.txt \u30e1\u30e2\u30ea\u304b\u3089\u898b\u305f\u5b9f\u884c\u72b6\u614b\u306b\u3042\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30d7\u30ed\u30bb\u30b9\u3068\u3044\u3046 \u30b8\u30e7\u30d6\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306b\u5165\u529b\u3055\u308c\u305f\u884c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3068\u304d\u306f\u8907\u6570\u306b\u306a\u308b\u3002\uff09 ps\u30b3\u30de\u30f3\u30c9 ps -x ps -u sleep\u30b3\u30de\u30f3\u30c9 jobs\u30b3\u30de\u30f3\u30c9 fg\u30b3\u30de\u30f3\u30c9 bg\u30b3\u30de\u30f3\u30c9","title":"Linux command old"},{"location":"linux_command_old/#_1","text":"","title":"\u30c7\u30a3\u30ec\u30af\u30c8\u30ea"},{"location":"linux_command_old/#pwd","text":"","title":"pwd"},{"location":"linux_command_old/#pwd_1","text":"","title":"pwd"},{"location":"linux_command_old/#ls","text":"1","title":"ls"},{"location":"linux_command_old/#cd","text":"","title":"cd"},{"location":"linux_command_old/#cat-less-tail","text":"","title":"cat, less, tail"},{"location":"linux_command_old/#du-df","text":"","title":"du, df"},{"location":"linux_command_old/#mv-cp","text":"","title":"mv, cp"},{"location":"linux_command_old/#ssh-scp","text":"","title":"ssh, scp"},{"location":"linux_command_old/#wget-curl","text":"","title":"wget, curl"},{"location":"linux_command_old/#touch","text":"","title":"touch"},{"location":"linux_command_old/#find-rm","text":"","title":"find, rm"},{"location":"linux_command_old/#kill-ps-pkill","text":"","title":"kill, ps, pkill"},{"location":"linux_command_old/#export","text":"","title":"export"},{"location":"linux_command_old/#_2","text":"","title":"\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u5b9f\u884c"},{"location":"linux_command_old/#2-2","text":"","title":"\u30ea\u30c0\u30a4\u30ec\u30af\u30c8&gt; , &gt;&gt;, &gt;&amp;, 2&gt;, 2&gt;&gt;"},{"location":"linux_command_old/#_3","text":"","title":"\u30d7\u30ed\u30bb\u30b9\u7f6e\u63db"},{"location":"linux_command_old/#_4","text":"https://webkaru.net/linux/redirect-output-to-file/ https://eng-entrance.com/linux-redirect https://shellscript.sunone.me/input_output.html python3 sample02.py < input_data > result01 tmux\u3067\u5b9f\u884c\u4e2d\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u898b\u308b ps aux | grep sleep tmux \u30b7\u30a7\u30eb\u306f\u30ab\u30fc\u30cd\u30eb\u306b\u547d\u4ee4\u3092\u51fa\u3057\u3066\u30ab\u30fc\u30cd\u30eb\u304b\u3089\u7d50\u679c\u3092\u53d7\u3051\u53d6\u308b\u305f\u3081\u306e\u3082\u306e bash \u306f\u30b7\u30a7\u30eb\u306e\uff11\u3064\u3000echo $SHELL\u3067\u78ba\u8a8d sh\u306e1\u3064 \u30bf\u30fc\u30df\u30ca\u30eb\uff1a\u5165\u51fa\u529b\u3000\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\uff08Linux\u306e\u5834\u5408\u3067\u306f\u5165\u51fa\u529b\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u30a8\u30df\u30e5\u30ec\u30fc\u30bf\u3092\u6307\u3059\u3002\u30b3\u30de\u30f3\u30c9\u3092\u3046\u3051\u3068\u3063\u305f\u308a\u51fa\u529b\uff09 \u30d7\u30ed\u30f3\u30d7\u30c8\uff1a\u30ab\u30fc\u30bd\u30eb\u306e\u5de6\u5074[ ^^^^@ ~]\u3000\u30b3\u30de\u30f3\u30c9\u5165\u529b\u3092\u4fc3\u3059 \u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3: \u30d7\u30ed\u30f3\u30d7\u30c8\u306e\u53f3\u5074 \u57fa\u672c\u64cd\u4f5c ctl + a: atama\u306b\u79fb\u52d5 ctrl + e: end\u306b\u79fb\u52d5 ctrl + w:\u3000 word: \u5358\u8a9e\u5358\u4f4d\u3067\u524a\u9664 \u30ab\u30c3\u30c8\u3000\u30a2\u30f3\u30c9\u3000\u30e4\u30f3\u30af ctrl + u \u884c\u982d\u307e\u3067\u30ab\u30c3\u30c8 ctrl + k:\u884c\u672b\u307e\u3067\u30ab\u30c3\u30c8 ctrl + y (yank) \u30bf\u30d6\u3067\u30aa\u30fc\u30c8\u30b3\u30f3\u30d7\u30ea\u30fc\u30c8 du -h --total df -h --total rm find\u9055\u3044 outout\u30d5\u30a1\u30a4\u30eb\u306b\u683c\u7d0d \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3 python -m logger conda venv \u74b0\u5883\u69cb\u7bc9 ls ls -a echo \u30b3\u30de\u30f3\u30c9 cat/less/tail cat >> cat > wget curl ping host ps kill grep \u30b3\u30de\u30f3\u30c9 space(\u4e00\u753b\u9762\u4e0b)\u3000b\uff08\u4e00\u753b\u9762\u4e0a\uff09 j\uff08\u4e00\u884c\u305a\u3064\u4e0b\uff09 k\uff08\u4e00\u884c\u305a\u3064\u4e0a\uff09 q \u306f\u3082\u3068\u306e\u753b\u9762\u306b\u623b\u308b\uff08quit\uff09 wget \u30b3\u30de\u30f3\u30c9 unzip touch \u30b3\u30de\u30f3\u30c9 rm \u3068rm -r echo \\(PATH export PATH = /path/to/something:\\) PATH\u3092\u8ffd\u52a0\u3059\u308b\u3002 cp /etc/crontab file2 cp file1 directory\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30b3\u30d4\u30fc\u53ef\u80fd \u6307\u5b9a\u3057\u305f\u30b3\u30d4\u30fc\u5148\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4e2d\u306b\u306a\u308b \u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u65b0\u3057\u3044\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u306a\u308b\u3002 cp -r dir1 dir2\u3067\u518d\u5e30\u7684\u306b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u30b3\u30d4\u30fc\u53ef\u80fd mv\u30b3\u30de\u30f3\u30c9\u3067\u540d\u524d\u3092\u5909\u3048\u308b\u3053\u3068\u3082\u3067\u304d\u308b\u3057\u3001\u30d5\u30a1\u30a4\u30eb\u3092\u79fb\u52d5\u3067\u304d\u308b\u3002 sh -x \u3067\u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u78ba\u8a8d docker \u30d5\u30a1\u30a4\u30eb\u306e\u3068\u304d\u306f-b -p\u3092\u3064\u3051\u308b\u3002 \u30cf\u30fc\u30c9\u30ea\u30f3\u30af \u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af ln file1 file2 (file1\u306bfile2\u3068\u3044\u3046\u30cf\u30fc\u30c9\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b\u3002file1\u3092\u4f5c\u6210\u3057\u3066\u3082file2\u304c\u6b8b\u308b) ln -s file1 file2\u3067\u30b7\u30f3\u30dc\u30ea\u30c3\u30af\u30ea\u30f3\u30af\u3092\u4f5c\u6210\u3059\u308b mkdir -p dir1/dir2/dir3/target touch p dir1/dir2/dir3/target/file ln -s dir1/dir2/dir3/target/ target \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(/)\u3068\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea(~)\u306e\u3061\u3083\u3093\u3068\u3057\u305f\u7406\u89e3 - Qiita \u3082\u306e\u3059\u3054\u3044\u7d30\u304b\u3044\u3053\u3068\u3060\u3051\u3069\u3001\u30d1\u30b9\u306e\u6307\u5b9a\u65b9\u6cd5\u3067\u306e ~ \uff08\u30c1\u30eb\u30c0\uff09\u3068 / \uff08\u30b9\u30e9\u30c3\u30b7\u30e5\uff09\u306e\u7406\u89e3\u304c\u66d6\u6627\u3067\u6c17\u6301\u3061\u60aa\u3044\u601d\u3044\u3092\u3057\u305f\u306e\u3067\u30e1\u30e2\u3002 / : \u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~ \uff1a\u4eca\u306e\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea ~taro : taro\u3068\u3044\u3046\u30e6\u30fc\u30b6\u30fc\u306e\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea","title":"\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3"},{"location":"linux_command_old/#_5","text":"\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e / \u3068\u3001\u5404\u30d5\u30a1\u30a4\u30eb\u3084\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u524d\u306b\u3064\u304f / \u306f\u610f\u5473\u5408\u3044\u304c\u9055\u3063\u3066\u3044\u308b\u6a21\u69d8\u3002 \u524d\u8005\uff1a\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u305d\u306e\u3082\u306e \u5f8c\u8005\uff1a\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u533a\u5207\u308b\u3082\u306e \u306a\u306e\u3067\u3001\u4e00\u898b\u30eb\u30fc\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u305b\u3044\u3067\u300c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u306f\u672b\u5c3e\u306b\u30b9\u30e9\u30c3\u30b7\u30e5\u304c\u4ed8\u3044\u3066\u3044\u308b\u3082\u306e\u300d\u3068\u3044\u3046\u52d8\u9055\u3044\u3092\uff08\u5c11\u306a\u304f\u3082\u7b46\u8005\u306f\uff09\u3057\u3061\u3083\u3046\u304c\u3001 hogehoge/ \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3067\u306f\u306a\u304f hogehoge \u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306a\u306e\u3060\u3002\u30db\u30fc\u30e0\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092 ~/ \u3060\u3068\u601d\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u4eba\u306f\u591a\u3044\u306e\u3067\u306f\u306a\u3044\u304b\uff1f history !393\u3067\u4f7f\u3048\u308b \u30d4\u30ea\u30aa\u30c9\u3067\u30ab\u30ec\u30f3\u30c8\u30c7\u30a3\u30ec\u30af\u30c8\u30ea find . -name '*.txt' -print \u3053\u306e\u30a2\u30b9\u30bf\u30ea\u30b9\u30af\u306f\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9\u3067\u30d1\u30b9\u540d\u5c55\u958b\u3068\u306f\u9055\u3046\u3002\u30c0\u30d6\u30eb\u30af\u30aa\u30fc\u30c6\u30b7\u30e7\u30f3\u304b\u3069\u3046\u304b find . -type d \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u691c\u7d22 find . -type d -a -name share locate \u30b3\u30de\u30f3\u30c9\u306ffind\u30b3\u30de\u30f3\u30c9\u3088\u308a\u3082\u9ad8\u901f\uff08\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u691c\u7d22\uff09 sudo updatedb\u3092\u3057\u3066\u304b\u3089 locate bash -A doc \u3000and \u691c\u7d22 locate bash doc grep bin /etc/crontab \u30d5\u30a3\u30eb\u30bf history | head wc:\u6587\u5b57\u6570\u3092\u6570\u3048\u308b wc -l ls / | wc -l \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u500b\u6570\u884c\u6570 \u30bd\u30fc\u30c8\u30b3\u30de\u30f3\u30c9 sort word.txt sort -r word.txt sort -n number.txt \u91cd\u8907\u3092\u53d6\u308a\u51fa\u3059 uniq number.txt sort -n number.txt | uniq sort -n number.txt | uniq -c | sort -nr | head -n 3 \u30d5\u30a1\u30a4\u30eb\u3092\u76e3\u8996\u3059\u308b tail -f log.txt \u30e1\u30e2\u30ea\u304b\u3089\u898b\u305f\u5b9f\u884c\u72b6\u614b\u306b\u3042\u308b\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u30d7\u30ed\u30bb\u30b9\u3068\u3044\u3046 \u30b8\u30e7\u30d6\u306f\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u306b\u5165\u529b\u3055\u308c\u305f\u884c\uff08\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u3068\u304d\u306f\u8907\u6570\u306b\u306a\u308b\u3002\uff09 ps\u30b3\u30de\u30f3\u30c9 ps -x ps -u sleep\u30b3\u30de\u30f3\u30c9 jobs\u30b3\u30de\u30f3\u30c9 fg\u30b3\u30de\u30f3\u30c9 bg\u30b3\u30de\u30f3\u30c9","title":"\u30b9\u30e9\u30c3\u30b7\u30e5\u306e\u610f\u5473\u5408\u3044"},{"location":"mkdocs/","text":"\u30c7\u30d7\u30ed\u30a4 \u00b6","title":"Mkdocs"},{"location":"mkdocs/#_1","text":"","title":"\u30c7\u30d7\u30ed\u30a4"},{"location":"mlutils/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def randomname ( n ): return \"\" . join ( random . choices ( string . ascii_letters + string . digits , k = n )) JOBNAME = datetime . datetime . now () . strftime ( \"%Y%m %d %H%M\" ) + \"_\" + randomname ( 10 ) with open ( file = \"./src/conf/config.yaml\" , mode = \"r\" , encoding = \"utf-8\" , ) as f : obj = yaml . safe_load ( f ) obj [ \"hydra\" ][ \"run\" ][ \"dir\" ] = \"./logs/${\" + \"now:\" + f \" { JOBNAME } \" + \"}\" with open ( \"./src/conf/config.yaml\" , \"w\" ) as f : yaml . safe_dump ( obj , f ) def seed_everything ( seed = 2021 ): random . seed ( seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False def git_commits ( rand ): def func_decorator ( my_func ): repo = git . Repo ( \"/work\" ) repo . config_writer () . set_value ( \"user\" , \"name\" , \"yseeker\" ) . release () repo . config_writer () . set_value ( \"user\" , \"email\" , \"maxwell8313@gmail.com\" ) . release () # os.system(\"git config --global user.name \\\"yseeker\\\"\") # os.system(\"git config --global user.email \\\"maxwell8313@gmail.com\\\"\") repo . git . diff ( \"HEAD\" ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _running\" ) repo . git . push ( \"origin\" , \"main\" ) def decorator_wrapper ( * args , ** kwargs ): my_func ( * args , ** kwargs ) repo . git . add ( \".\" ) repo . index . commit ( f \" { rand } _done\" ) repo . git . push ( \"origin\" , \"main\" ) return decorator_wrapper return func_decorator","title":"ML utils"},{"location":"pandas/","text":"1 df = df . append ({ 'open' : candle_stick [ 'open' ], 'high' : candle_stick [ 'high' ], 'low' : candle_stick [ 'low' ], 'close' : candle_stick [ 'close' ], }, ignore_index = True )","title":"Pandas"},{"location":"parallel/","text":"","title":"Parallel"},{"location":"pararell-async/","text":"https://qiita.com/icoxfog417/items/07cbf5110ca82629aca0 https://qiita.com/takey/items/917a48aa5d1882e125f4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from time import time import subprocess start = time () #\u4e26\u5217\u30d7\u30ed\u30bb\u30b9\u5b9f\u884c\u6570\u306e\u6700\u5927\u5024 max_process = 16 proc_list = [] loop_num = 64 for i in range ( loop_num ): proc = subprocess . Popen ([ 'python' , r \".\\fib_sample.py\" , str ( 500000 + i )]) proc_list . append ( proc ) if ( i + 1 ) % max_process == 0 or ( i + 1 ) == loop_num : #max_process\u6bce\u306b\u3001\u5168\u30d7\u30ed\u30bb\u30b9\u306e\u7d42\u4e86\u3092\u5f85\u3064 for subproc in proc_list : subproc . wait () proc_list = [] end = time () print ( \" %f sec\" % ( end - start )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import subprocess import time def run_multisleep (): \"\"\" \u8907\u6570\u306e\u5b50\u30d7\u30ed\u30bb\u30b9\u3067run_sleep.py\u3092\u5b9f\u884c\u3059\u308b \"\"\" def run_sleep (): proc = subprocess . Popen ([ \"python\" , \"run_sleep.py\" ]) return proc start = time . time () procs = [] for _ in range ( 10 ): procs . append ( run_sleep ()) for proc in procs : proc . communicate () end = time . time () print ( \"Finished in {} seconds.\" . format ( end - start )) if __name__ == \"__main__\" : run_multisleep () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def task ( v ): getLogger () . info ( \" %s start\" , v ) time . sleep ( 1.0 ) getLogger () . info ( \" %s end\" , v ) return v * 2 def main (): init_logger () getLogger () . info ( \"main start\" ) with ThreadPoolExecutor ( max_workers = 2 , thread_name_prefix = \"thread\" ) as executor : futures = [] for i in range ( 5 ): futures . append ( executor . submit ( task , i )) getLogger () . info ( \"submit end\" ) getLogger () . info ([ f . result () for f in futures ]) getLogger () . info ( \"main end\" ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import cv2 from pathlib import Path import time from concurrent.futures import ThreadPoolExecutor , ProcessPoolExecutor IMAGE_PATH = \"../../../tmp/image\" def main ( file ): \"\"\" \u4e0e\u3048\u3089\u308c\u305f\u753b\u50cf\u304b\u3089\u3001\u30e2\u30b6\u30a4\u30af\u51e6\u7406\u3057\u305f\u753b\u50cf\u3092\u751f\u6210\u3059\u308b \"\"\" src = cv2 . imread ( str ( file )) dst = img2mosaic ( src , ratio = 0.01 ) cv2 . imwrite ( IMAGE_PATH + \"/mosaic/\" + file . name , dst ) if __name__ == \"__main__\" : files = Path ( IMAGE_PATH + \"/origin/\" ) . glob ( \"*\" ) start = time . time () #pool = ThreadPoolExecutor(max_workers=6) # CPU\u30b3\u30a2\u65706\u306a\u306e\u3067 pool = ProcessPoolExecutor ( max_workers = 6 ) results = list ( pool . map ( main , files )) # list()\u3067\u56f2\u307e\u306a\u3044\u3068\u3059\u3050\u306b\u7d42\u4e86\u3059\u308b\u306e\u3067\u6ce8\u610f end = time . time () print ( \"Finished in {} seconds.\" . format ( end - start )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def task ( params ): ( v , num_calc ) = params a = float ( v ) for _ in range ( num_calc ): a = pow ( a , a ) return a def main (): init_logger () if len ( sys . argv ) != 5 : print ( \"usage: 05_process.py max_workers chunk_size num_tasks num_calc\" ) sys . exit ( 1 ) ( max_workers , chunk_size , num_tasks , num_calc ) = map ( int , sys . argv [ 1 :]) start = time () with ProcessPoolExecutor ( max_workers = max_workers ) as executor : params = map ( lambda _ : ( random (), num_calc ), range ( num_tasks )) results = executor . map ( task , params , chunksize = chunk_size ) getLogger () . info ( sum ( results )) getLogger () . info ( \" {:.3f} \" . format ( time () - start )) https://qiita.com/icoxfog417/items/07cbf5110ca82629aca0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 async def parallel_by_gather (): # execute by parallel def notify ( order ): print ( order + \" has just finished.\" ) cors = [ sleeping ( s [ 0 ], s [ 1 ], hook = notify ) for s in Seconds ] results = await asyncio . gather ( * cors ) return results if __name__ == \"__main__\" : loop = asyncio . get_event_loop () results = loop . run_until_complete ( parallel_by_gather ()) for r in results : print ( \"asyncio.gather result: {0} \" . format ( r )) async def parallel_by_wait (): # execute by parallel def notify ( order ): print ( order + \" has just finished.\" ) cors = [ sleeping ( s [ 0 ], s [ 1 ], hook = notify ) for s in Seconds ] done , pending = await asyncio . wait ( cors ) return done , pending if __name__ == \"__main__\" : loop = asyncio . get_event_loop () done , pending = loop . run_until_complete ( parallel_by_wait ()) for d in done : dr = d . result () print ( \"asyncio.wait result: {0} \" . format ( dr ))","title":"Pararell async"},{"location":"pillow_openCV/","text":"Dataset\u3067\u306e\u6271\u3044\u3067\u306e\u9055\u3044 \u753b\u50cf\u8868\u793a\u65b9\u6cd5\uff08\u30ed\u30fc\u30ab\u30eb\u3001torch\uff09 Truncated \u900f\u660e\u5316\u306e\u524a\u9664 \u80cc\u666f\u524a\u9664","title":"pillow openCV"},{"location":"plot/","text":"","title":"Plot"},{"location":"python_basis/","text":"Python basis \u00b6 \u57fa\u790e \u00b6 Python\u306f\u3059\u3079\u3066\u304c\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002int, str, \u95a2\u6570 Python\u306f\u52d5\u7684\u578b\u4ed8\u3051\u8a00\u8a9e\u3001\u578b\u3088\u308a\u3082\u632f\u308b\u821e\u3044\u306b\u8208\u5473\u304c\u3042\u308b\u3002 \u4fbf\u5229\u306a\u30d3\u30eb\u30c9\u30a4\u30f3\u95a2\u6570 id():\u5909\u6570\u306e\u5834\u6240\u306eid\u3092\u8fd4\u3059 dir:attribute\u3092\u8fd4\u3059 is\u6f14\u7b97\u5b50\uff1a\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304b\u3069\u3046\u304b\u3092\u5224\u65ad\u3059\u308b isinstance:\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30bf\u30a4\u30d7\u3092\u78ba\u8a8d copy\u3068deepcopy \u578b\u5909\u63db\uff08casting\uff09 \u30a4\u30df\u30e5\u30fc\u30bf\u30d6\u30eb\u3068\u30df\u30e5\u30fc\u30bf\u30d6\u30eb\uff1a\u95a2\u6570\u306e\u4e2d\u3067\u65b0\u3057\u3044\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09ID\u304c\u4f5c\u3089\u308c\u308b\u3002for\u6587\u3067\u306f\u8db3\u3057\u3066\u3044\u304f\u3068\u304d\u306f\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\u307b\u3046\u304c\u826f\u3044\u3002\uff09 _\u306f\u76f4\u524d\u306e\u5b9f\u884c\u3057\u305f\u623b\u308a\u5024\u3092\u683c\u7d0d\u3059\u308b\u3002 \u30d6\u30fc\u30ea\u30a2\u30f3\u306b\u6bd4\u8f03\u6f14\u7b97\u5b50\u3092\u4f7f\u308f\u306a\u3044\u3002 tuple\u306f\u300c\u4e38\u62ec\u5f27\u3067\u4f5c\u6210\u3055\u308c\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u30ab\u30f3\u30de\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u300d\u307e\u3059 \u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u66f8\u304d\u306b\u306f with open \u3092\u4f7f\u3046\u3002 namedtuple getattr:\u3092\u4f7f\u3046\u3002 https://qiita.com/ganyariya/items/3b8861788ec30238a8a9 \u4e09\u9023\u30af\u30aa\u30fc\u30c8\u306b\u3088\u308b\u8907\u6570\u884c\u6587\u5b57\u5217 shutil\u30e2\u30b8\u30e5\u30fc\u30eb 1 2 method = getattr ( animal , 'walk' , None ) if callable ( method ) vscode \u00b6 vscode\u3067\u81ea\u52d5\u3067\u5909\u6570\u3092\u5236\u5fa1\u3002\u66f8\u304d\u63db\u3048 vscode\u3067linter\u3068formatter\u3092\u8a2d\u5b9a black: https://github.com/psf/black flake8: https://github.com/PyCQA/flake8 isort: https://github.com/PyCQA/isort mypy \u30b3\u30e1\u30f3\u30c8\u3067# TODO \u74b0\u5883\u69cb\u7bc9 \u00b6 1 2 3 4 5 6 7 pip freeze pip freeze > requirements.txt pip install -r requirements.txt #or pipenv --python 3 pipenv install -r ./requirements.txt #\u81ea\u52d5\u3067pipfile\u304c\u4f5c\u6210 pipenv lock ) git\u304b\u3089install \u00b6 1 !pip install git+https://github.com/yseeker/tez_custom \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30b9\u30bf\u30a4\u30eb \u00b6 pep8 \u00b6 =\u3068\u30aa\u30da\u30ec\u30fc\u30bf\u30fc\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9 \u95a2\u6570\u306e\u5f15\u6570\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9\u306f\u4e0d\u8981 \u30d7\u30e9\u30a4\u30aa\u30ea\u30c6\u30a3\u304c\u3042\u308b\u5834\u5408\u306f\u30b9\u30da\u30fc\u30b9\u3092\u7121\u304f\u3059 \u30ab\u30f3\u30de\u306e\u3042\u3068\u306b\u30b9\u30da\u30fc\u30b9\u3092\u5165\u308c\u308b\u3002 \u6700\u5f8c\u306e\u8981\u7d20\u306b\u30ab\u30f3\u30de\u3082\u3064\u3051\u308b\uff08\u62ec\u5f27\u9589\u3058\u3092\u6b21\u306e\u884c\u306b\u3059\u308b\u3002\uff09 \u95a2\u6570\u306e\u5f15\u6570\u306e\u982d\u3092\u63c3\u3048\u3066\u6539\u884c\u3059\u308b\u3002 \u95a2\u6570\u9593\u306f\u4e8c\u884c\u3042\u3051\u308b\u3002 \u30af\u30e9\u30b9\u306e\u30e1\u30bd\u30c3\u30c9\u9593\u306f1\u884c import \u306e\u9806\u756a standrd library third party our library local library \u74b0\u5883 \u00b6 pyenv + pipenv\u3092\u4f7f\u3046\u3002 \u95a2\u6570 \u00b6 Python\u3067\u306f \u5168\u3066\u53c2\u7167\u6e21\u3057 \u3002 constant variable \u5927\u6587\u5b57\u3067\u66f8\u304f \u30e2\u30b8\u30e5\u30fc\u30eb \uff08\u30d5\u30a1\u30a4\u30eb\u5358\u4f4d\u3067\u5206\u3051\u3066\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8f09\u3057\u305f\u3082\u306e\uff09\uff1c \u30d1\u30c3\u30b1\u30fc\u30b8 \uff08\u8907\u6570\u306e\u30d5\u30a1\u30a4\u30eb\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u304b\u3089\u306a\u308a\u3001\u30eb\u30fc\u30eb\u306b\u5f93\u3063\u3066\u305d\u308c\u3089\u3092\u3072\u3068\u56fa\u307e\u308a\u306b\u3057\u305f\u3082\u306e\uff09\uff1c \u30e9\u30a4\u30d6\u30e9\u30ea \u3002 \u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u968e\u5c64\u5316\u3055\u305b\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u8aad\u307f\u8fbc\u307e\u305b\u308b\u305f\u3081\u306b init .py \u304c\u5fc5\u8981\uff08\u3053\u306e\u3068\u304d\u76f8\u5bfeimport\u3082\u884c\u3046\uff09 \u5f15\u6570\uff08arguments\uff09 \uff1a\u5b9f\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024 \u30d1\u30e9\u30e1\u30fc\u30bf\uff08parameters\uff09 \uff1a\u4eee\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024\u306e\u30d7\u30ec\u30fc\u30b9\u30db\u30eb\u30c0\u3002 https://qiita.com/raviqqe/items/ee2bcb6bef86502f8cc6#%E5%BC%95%E6%95%B0%E3%81%AF-2-x-2--4-%E7%A8%AE%E9%A1%9E positional paremeters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\uff08arguments\uff09\u306a\u3057\u3002 keyword parameters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\u3042\u308a\u3002 \u53ef\u5909\u9577\u5f15\u6570 : args, *kwargs, \u69d8\u3005\u306a\u9577\u3055\u306e\u5f15\u6570\u3092\u53d7\u3051\u53d6\u308c\u308b\u3002 global \u3068 nonlocal \uff08nested\u95a2\u6570\u306e\u3068\u304d\u306b\u5b9a\u7fa9\uff09 \u30e9\u30e0\u30c0\u95a2\u6570 \uff08\u95a2\u6570\u540d\u304c\u7121\u3044\u95a2\u6570\uff09:\u95a2\u6570\u540d\u3068\"return\"\u3092\u7121\u304f\u3059\u3002filter \u95a2\u6570\u306e\u969b\u306b\u4f7f\u3046\u3002 1 lm_add = lambda x , y : x + y \u95a2\u6570\u3082\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3001\u95a2\u6570\u3092\u5f15\u6570\u3067\u3068\u308c\u308b\u3001\u95a2\u6570\u3082return\u3067\u304d\u308b\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u3066\u8fd4\u3059\u3002\uff09Closure:\u72b6\u614b\u3092\u30ad\u30fc\u30d7\u3057\u305f\u95a2\u6570\u3002\uff08\u72b6\u614b\u3092\u52d5\u7684\u30fb\u9759\u7684\uff09 sys.path :\u306b\u5165\u308c\u308b\u3068\u30ab\u30b9\u30bf\u30e0\u3067\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3048\u308b\u3002pip\u3092\u4f7f\u3046\u3068site-packages\u306e\u4e2d\u3067\u7ba1\u7406\u3055\u308c\u308b\u3002 \u6b63\u898f\u8868\u73fe \u00b6 re.search('[0-9]', string) re.search('^[0-9]', string):\u6700\u521d\u306e\u6587\u5b57 re.search('^[0-9]{4}', string):\u6700\u521d\u306e\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}', string):\u6700\u521d\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}$', string):\u6700\u5f8c\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('a*b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('a+b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30921\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('ab?c', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u304b1\u56de\u7e70\u308a\u8fd4\u3059 abc|012 or te(s|x)t \u30b0\u30eb\u30fc\u30d7 'h.t'\u4efb\u610f\u306e\u4e00\u6587\u5b57 \u30a8\u30b9\u30b1\u30fc\u30d7'h.t' \\w [a-zA-Z0-9_]\u306b\u30de\u30c3\u30c1 \u30af\u30e9\u30b9 \u00b6 \u5c5e\u6027\uff08\u30e1\u30f3\u30d0\u5909\u6570\u3001\u30e1\u30f3\u30d0\u95a2\u6570\uff09, \u30e1\u30bd\u30c3\u30c9\uff08instancemethod, static method, class method\uff09, \u30d7\u30ed\u30d1\u30c6\u30a3 * \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3068\u30af\u30e9\u30b9\u5909\u6570 https://docs.python.org/ja/3/library/functions.html https://qiita.com/ichi_taro3/items/cd71a8e43040abb446a1 \u6163\u7fd2\u7684\u306a\u547d\u540d\u898f\u5247\u3068\u3057\u3066\u306e\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8(non public)\u5316\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc\uff09\u3002\u6226\u95d8\u306b\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc\u3092\u3064\u3051\u3066_\u540d\u524d\u3068\u3059\u308b \u30cd\u30fc\u30e0\u30de\u30f3\u30b0\u30ea\u30f3\u30b0\uff08\u96e3\u53f7\u5316\uff09\u8981\u7d20\u540d\u306e\u524d\u306b\"__\"\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc2\u3064\uff09\u3092\u3064\u3051\u307e\u3059\u3002 \u7d99\u627f\u6642\u306e\u540d\u524d\u4fee\u98fe\u306f__\u3092\u4f7f\u3044\u3053\u306a\u3059\u3002 \u30dd\u30ea\u30e2\u30fc\u30d5\u30a3\u30ba\u30e0\u306f\u7d99\u627f\u3092\u3057\u3066\u3044\u308bint\u3082str\u3082print\u3092\u3059\u308b\u3068\u540c\u3058\u3088\u3046\u306b\u632f\u308b\u821e\u3046 \u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\uff1a\u30b5\u30d6\u30af\u30e9\u30b9\u306e\u3067\u540c\u3058\u540d\u524d\u306e\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3002 .\u3068\u304b..\u3067\u76f8\u5bfe\u30a4\u30f3\u30dd\u30fc\u30c8\u3067\u304d\u308b\u3002 \u30c7\u30b3\u30ec\u30fc\u30bf \u00b6 https://qiita.com/koshigoe/items/848ddc0272b3cee92134 @staticmethod \uff1a\u307b\u3068\u3093\u3069\u30af\u30e9\u30b9\u5916\u306e\u95a2\u6570\u3068\u3057\u3066\u6271\u3046\u3002\uff08self\u306f\u3044\u3089\u306a\u3044\uff09 @classmethod \uff1acls\u306b\u5f15\u6570\u3092\u3068\u3063\u3066\u3001class\u306e\u60c5\u5831\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002\u7d99\u627f\u3059\u308b\u3068\u304d\u306fstaticmethod\u3067\u306f\u547c\u3076\u3068\u304d\u306b\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3002classmethod\u3092\u4f7f\u3046\u3002 @property \uff1a\u5909\u6570\u3092\u30ab\u30d7\u30bb\u30eb\u5316\u3057\u3001\u5909\u6570\u3060\u3051\u5916\u306b\u8fd4\u305b\u308b\u3088\u3046\u306b\u3059\u308b\u3002setter\u3068\u30bb\u30c3\u30c8\u3067\u4f7f\u3046\u3002\u5916\u304b\u3089\u5185\u90e8\u306e\u5024\u3092\u30bb\u30c3\u30c8\u3067\u304d\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 class Example : def __init__ ( self , x , y ): self . _x = x self . _y = y @property def x ( self ): return self . _x @x . setter def x ( self , dx ): self . _x += dx self . _x = max ( 0 , min ( self . MAX_X , self . _x )) @dataclass \uff1a\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308b\u30af\u30e9\u30b9\u3092\u7c21\u5358\u306b\u4f5c\u6210\u3059\u308b\u30c7\u30b3\u30ec\u30fc\u30bf\u306a\u3069\u3092\u63d0\u4f9b 1 2 3 4 5 6 7 8 @dataclass class InventoryItem : name : str price : float quantity : int = 0 def total_cost ( self ) -> float : return self . price * self . quantity \u30de\u30b8\u30c3\u30af\u30e1\u30bd\u30c3\u30c9\uff08\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9\uff09 \u00b6 http://diveintopython3-ja.rdy.jp/special-method-names.html __init__ \uff1a\u30af\u30e9\u30b9\u306e\u521d\u671f\u5316\u3002\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3092\u547c\u3076\u3002\u89aa\u30af\u30e9\u30b9\u306e\u30b3\u30f3\u30b9\u30bf\u30af\u30bf\u3092\u547c\u3076\u3068\u304d\u306f\u3001super. init ()\u3068\u3059\u308b\u3002 __del__ \uff1a\u30c7\u30b9\u30c8\u30e9\u30af\u30bf\u3002\u57fa\u672c\u7684\u306b\u306f\u4f7f\u308f\u306a\u3044\u3002\u4ee3\u308f\u308a\u306bwith\u69cb\u6587\u3092\u4f7f\u3046\u3002 __call__ \uff1a\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u95a2\u6570\u306e\u3088\u3046\u306b\u6271\u3048\u308b\u3002 __len__ \uff1a len\u30e1\u30bd\u30c3\u30c9\u306b\u5bfe\u3059\u308bint\u578b\u306e\u5024\u3092\u8fd4\u3059 __getitem__ :\u30a4\u30f3\u30c7\u30af\u30b7\u30f3\u30b0\uff08\u914d\u5217\u306e\u3088\u3046\u306b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a2\u30af\u30bb\u30b9\u3059\u308b\uff09\u3002\u30b7\u30fc\u30b1\u30f3\u30b9\u578b\uff08list, tuple, str, range\uff09 __iter__ :iterator\u3092\u8fd4\u3059\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9 __next__ :\u8981\u7d20\u3092\u53cd\u5fa9\u3057\u3066\u53d6\u308a\u51fa\u3059\u3053\u3068\u306e\u3067\u304d\u308b\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9\u3067\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class FooIterator (): def __init__ ( self , foo ): # foo\u306fiterable\u306a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8 self . _i = 0 self . _foo = foo def __iter__ ( self ): return self def __next__ ( self ): try : v = self . _foo . _L [ self . _i ] self . _i += 1 return v except IndexError : raise StopIteration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Odd : def __init__ ( self ): self . i = 1 def __contains__ ( self , x ): return x % 2 == 1 def __str__ ( self ): return \"odd numbers\" def __iter__ ( self ): return self def __next__ ( self ): result = self . i self . i += 2 return result def __getitem__ ( self , i ): return 2 * i + 1 def __len__ ( self ): return 0 __contains__ \uff1a\u30b3\u30f3\u30c6\u30ca\u578b\u3092\u5b9a\u7fa9\u3002\u72ec\u81ea\u30af\u30e9\u30b9\u306bin\u3092\u5b9a\u7fa9\u3067\u304d\u308b\u3002\uff08list, dict, tuple, str, collections.defaultdict\uff09 __str__ \uff1aprint\u95a2\u6570\u306e\u969b\u306b\u547c\u3070\u308c\u308b\u3002 __repr__ \uff1a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u518d\u751f\u6210\u3059\u308b\u305f\u3081\u306b\u4f7f\u3048\u308b\u3088\u3046\u306a\u3088\u308a\u6b63\u78ba\u306a\u60c5\u5831\u3092\u8fd4\u3059 __name__ :\u30e2\u30b8\u30e5\u30fc\u30eb\u5185\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u304a\u3044\u3066\u306f\u30e2\u30b8\u30e5\u30fc\u30eb\u540d\u3002\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u304a\u3044\u3066\u306f\u95a2\u6570\u540d\u306b\u306a\u308b\u3002 __doc__ : __buidins__ :\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002 set or get members getitem (), setitem () and iterate over items iter () and to get the number of items - method len () The behaviour of sorted() built-in function is to iterate over elements of your container and compare them using methods you mentioned cmp (), ge (), le () This is because reversing a collection doesn't care about values of items but sorting it depends on these values. \u2013 ElmoVanKielmo Feb 19 '18 at 15:12 it seems to me the only important difference is that reversed() looks for a reversed () method in the container its reversing whereas sorted() doesn't look for a sorted () method. \u2013 gregrf Feb 19 '18 at 15:17 This is the technical difference https://stackoverflow.com/questions/48868228/is-there-a-magic-method-for-sorted-in-python generetor \u5f0f \u00b6 \u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u53d6\u308a\u306a\u3069\u90e8\u5206\u7684\u306b\u30e1\u30e2\u30ea\u306b\u8f09\u305b\u3066\u3044\u304f\u5834\u5408\u306b\u4f7f\u3046\u3002 generator\u306f\u30e9\u30f3\u30c0\u30e0\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044 ex. range(10) https://www.atmarkit.co.jp/ait/articles/1908/20/news024.html https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee 1 2 3 4 5 6 7 8 # generator expression ( x * x for x in numbers ) # generator function # \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u3063\u3066yield\u3092\u8fd4\u3059\u3002 # next\u3067\u5024\u3092\u53d6\u5f97\u3067\u304d\u308b\u3002 def gen_func (): for x in numbers : yield x * x \u30a8\u30e9\u30fc \u00b6 https://note.nkmk.me/python-error-message/ https://note.nkmk.me/python-try-except-else-finally/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 try : print ( a / b ) #\u4f8b\u5916\u3092\u610f\u56f3\u7684\u306b\u767a\u751f\u3055\u305b\u308b raise ZeroDivisionError except ZeroDivisionError as e : print ( 'catch ZeroDivisionError:' , e ) except ValueError as e : print ( '\u6570\u5b57\u4ee5\u5916\u304c\u5165\u529b\u3055\u308c\u307e\u3057\u305f\u3002\u6570\u5b57\u306e\u307f\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044' ) print ( 'catch ValueError:' , e ) else : \u4f8b\u5916\u304c\u8d77\u304d\u305f\u3068\u304d\u306f\u5b9f\u884c\u3057\u306a\u3044\u30b3\u30fc\u30c9 finally : \u5e38\u306b\u5b9f\u884c\u3059\u308b\u30b3\u30fc\u30c9 finally \u306f\u30ad\u30e3\u30c3\u30c1\u3055\u308c\u306a\u304f\u3066\u3082\u30a8\u30e9\u30fc\u306e\u524d\u306b\u5b9f\u884c\u3055\u308c\u308b raise \u306f\u30a8\u30e9\u30fc\u3092\u767a\u751f\u3055\u305b\u308b\u3002 \u4f8b\u5916\u306e\u81ea\u4f5c Exception \u30af\u30e9\u30b9\u3092\u7d99\u627f\u3059\u308b traceback.print_exc() tracebackmodudle \u30c6\u30b9\u30c8 \u00b6 unittest \u00b6 assert\u30b3\u30fc\u30c9\u3067\u30c6\u30b9\u30c8\u3057\u3066\u3044\u304f\u3002 \u30c6\u30b9\u30c8\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u304f\u3002 Test runnner: unittest, self.assertEqual(power(base, exp), 8) python -m unittest test.py\u3092\u4f7f\u3046 \u4f8b\u5916\u30b1\u30fc\u30b9\u306fwith \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u3063\u3066\u66f8\u304f\u3002 with self.assertRaise(Typeerror) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #https://qiita.com/phorizon20/items/acb929772aaae4f52101 def fizzbuzz ( number ): if number % 15 == 0 : return \"FizzBuzz\" if number % 5 == 0 : return \"Buzz\" if number % 3 == 0 : return \"Fizz\" return number import unittest import fizzbuzz as fb class FizzBuzzTest ( unittest . TestCase ): def setUp ( self ): # \u521d\u671f\u5316\u51e6\u7406 pass def tearDown ( self ): # \u7d42\u4e86\u51e6\u7406 pass def test_normal ( self ): self . assertEqual ( 1 , fb . fizzbuzz ( 1 )) def test_fizz ( self ): self . assertEqual ( \"Fizz\" , fb . fizzbuzz ( 3 )) def test_buzz ( self ): self . assertEqual ( \"Buzz\" , fb . fizzbuzz ( 5 )) def test_fizzbuzz ( self ): self . assertEqual ( \"FizzBuzz\" , fb . fizzbuzz ( 15 )) pytest \u00b6 pytest assert\u3067\u7c21\u6f54\u306b\u66f8\u3051\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #https://kazuhira-r.hatenablog.com/entry/2020/03/14/173536 class Calc : def add ( self , x , y ): return x + y def minus ( self , x , y ): return x - y def multiply ( self , x , y ): return x * y def divide ( self , x , y ): return x / y from sample.calc import Calc def test_add (): calc = Calc () assert calc . add ( 1 , 3 ) == 4 def test_minus (): calc = Calc () assert calc . minus ( 5 , 3 ) == 2 def test_multiply (): calc = Calc () assert calc . multiply ( 2 , 3 ) == 6 def test_divide (): calc = Calc () assert calc . divide ( 10 , 2 ) == 5 \u30c6\u30b9\u30c8\u30ab\u30d0\u30ec\u30c3\u30b8 pytest-cov:\u30ab\u30d0\u30fc\u7387\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002 html\u3084xml\u3067\u51fa\u529b\u3067\u304d\u308b\u3002--cov-append \u30ea\u30f3\u30af \u00b6 https://qiita.com/ganyariya/items/fb3f38c2f4a35d1ee2e8 https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee http://diveintopython3-ja.rdy.jp/special-method-names.html utils \u00b6 \u30d5\u30a1\u30a4\u30eb\u53d6\u5f97 \u00b6 1 2 import glob glob . glob ( '/folder/* /*.dcm' ) \u8f9e\u66f8\u306a\u3069\u306e\u4fdd\u5b58 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import pickle with open ( \"data.pkl\" , \"wb\" ) as pkl_handle : pickle . dump ( dictionary_data , pkl_handle ) # LOAD with open ( \"data.pkl\" , \"rb\" ) as pkl_handle : output = pickle . load ( pkl_handle ) import mpu your_data = { 'foo' : 'bar' } mpu . io . write ( 'filename.pickle' , data ) unserialized_data = mpu . io . read ( 'filename.pickle' ) # https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict CSV : Super simple format (read & write) JSON : Nice for writing human-readable data; VERY commonly used (read & write) YAML : YAML is a superset of JSON, but easier to read (read & write, comparison of JSON and YAML) pickle : A Python serialization format (read & write) MessagePack (Python package): More compact representation (read & write) HDF5 (Python package): Nice for matrices (read & write) XML : exists too sigh (read & write) dicom\u30d5\u30a1\u30a4\u30eb \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def dicom2array ( path , voi_lut = True , fix_monochrome = True ): # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way dicom = pydicom . read_file ( path ) # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to # \"human-friendly\" view if voi_lut : data = apply_voi_lut ( dicom . pixel_array , dicom ) else : data = dicom . pixel_array # depending on this value, X-ray may look inverted - fix that: if fix_monochrome and dicom . PhotometricInterpretation == \"MONOCHROME1\" : data = np . amax ( data ) - data data = data - np . min ( data ) data = data / np . max ( data ) data = ( data * 255 ) . astype ( np . uint8 ) return data def resize ( array , size , keep_ratio = False , resample = Image . LANCZOS ): # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image im = Image . fromarray ( array ) if keep_ratio : im . thumbnail (( size , size ), resample ) else : im = im . resize (( size , size ), resample ) return im def resize_and_save ( file_path ): split = 'train' if 'train' in file_path else 'test' base_dir = f '/kaggle/working/ { split } ' img = dicom2array ( file_path ) h , w = img . shape [: 2 ] # orig hw if aspect_ratio : r = dim / max ( h , w ) # resize image to img_size interp = cv2 . INTER_AREA if r < 1 else cv2 . INTER_LINEAR if r != 1 : # always resize down, only resize up if training with augmentation img = cv2 . resize ( img , ( int ( w * r ), int ( h * r )), interpolation = interp ) else : img = cv2 . resize ( img , ( dim , dim ), cv2 . INTER_AREA ) filename = file_path . split ( '/' )[ - 1 ] . split ( '.' )[ 0 ] cv2 . imwrite ( os . path . join ( base_dir , f ' { filename } .jpg' ), img ) return filename . replace ( 'dcm' , '' ) + '_image' , w , h","title":"Basics"},{"location":"python_basis/#python-basis","text":"","title":"Python basis"},{"location":"python_basis/#_1","text":"Python\u306f\u3059\u3079\u3066\u304c\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002int, str, \u95a2\u6570 Python\u306f\u52d5\u7684\u578b\u4ed8\u3051\u8a00\u8a9e\u3001\u578b\u3088\u308a\u3082\u632f\u308b\u821e\u3044\u306b\u8208\u5473\u304c\u3042\u308b\u3002 \u4fbf\u5229\u306a\u30d3\u30eb\u30c9\u30a4\u30f3\u95a2\u6570 id():\u5909\u6570\u306e\u5834\u6240\u306eid\u3092\u8fd4\u3059 dir:attribute\u3092\u8fd4\u3059 is\u6f14\u7b97\u5b50\uff1a\u540c\u3058\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u304b\u3069\u3046\u304b\u3092\u5224\u65ad\u3059\u308b isinstance:\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u30bf\u30a4\u30d7\u3092\u78ba\u8a8d copy\u3068deepcopy \u578b\u5909\u63db\uff08casting\uff09 \u30a4\u30df\u30e5\u30fc\u30bf\u30d6\u30eb\u3068\u30df\u30e5\u30fc\u30bf\u30d6\u30eb\uff1a\u95a2\u6570\u306e\u4e2d\u3067\u65b0\u3057\u3044\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09ID\u304c\u4f5c\u3089\u308c\u308b\u3002for\u6587\u3067\u306f\u8db3\u3057\u3066\u3044\u304f\u3068\u304d\u306f\u30ea\u30b9\u30c8\u3092\u4f7f\u3046\u307b\u3046\u304c\u826f\u3044\u3002\uff09 _\u306f\u76f4\u524d\u306e\u5b9f\u884c\u3057\u305f\u623b\u308a\u5024\u3092\u683c\u7d0d\u3059\u308b\u3002 \u30d6\u30fc\u30ea\u30a2\u30f3\u306b\u6bd4\u8f03\u6f14\u7b97\u5b50\u3092\u4f7f\u308f\u306a\u3044\u3002 tuple\u306f\u300c\u4e38\u62ec\u5f27\u3067\u4f5c\u6210\u3055\u308c\u308b\u306e\u3067\u306f\u306a\u304f\u3001\u30ab\u30f3\u30de\u306b\u3088\u3063\u3066\u4f5c\u6210\u3055\u308c\u300d\u307e\u3059 \u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u66f8\u304d\u306b\u306f with open \u3092\u4f7f\u3046\u3002 namedtuple getattr:\u3092\u4f7f\u3046\u3002 https://qiita.com/ganyariya/items/3b8861788ec30238a8a9 \u4e09\u9023\u30af\u30aa\u30fc\u30c8\u306b\u3088\u308b\u8907\u6570\u884c\u6587\u5b57\u5217 shutil\u30e2\u30b8\u30e5\u30fc\u30eb 1 2 method = getattr ( animal , 'walk' , None ) if callable ( method )","title":"\u57fa\u790e"},{"location":"python_basis/#vscode","text":"vscode\u3067\u81ea\u52d5\u3067\u5909\u6570\u3092\u5236\u5fa1\u3002\u66f8\u304d\u63db\u3048 vscode\u3067linter\u3068formatter\u3092\u8a2d\u5b9a black: https://github.com/psf/black flake8: https://github.com/PyCQA/flake8 isort: https://github.com/PyCQA/isort mypy \u30b3\u30e1\u30f3\u30c8\u3067# TODO","title":"vscode"},{"location":"python_basis/#_2","text":"1 2 3 4 5 6 7 pip freeze pip freeze > requirements.txt pip install -r requirements.txt #or pipenv --python 3 pipenv install -r ./requirements.txt #\u81ea\u52d5\u3067pipfile\u304c\u4f5c\u6210 pipenv lock )","title":"\u74b0\u5883\u69cb\u7bc9"},{"location":"python_basis/#gitinstall","text":"1 !pip install git+https://github.com/yseeker/tez_custom","title":"git\u304b\u3089install"},{"location":"python_basis/#_3","text":"","title":"\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30b9\u30bf\u30a4\u30eb"},{"location":"python_basis/#pep8","text":"=\u3068\u30aa\u30da\u30ec\u30fc\u30bf\u30fc\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9 \u95a2\u6570\u306e\u5f15\u6570\u306e\u5468\u308a\u306b\u30b9\u30da\u30fc\u30b9\u306f\u4e0d\u8981 \u30d7\u30e9\u30a4\u30aa\u30ea\u30c6\u30a3\u304c\u3042\u308b\u5834\u5408\u306f\u30b9\u30da\u30fc\u30b9\u3092\u7121\u304f\u3059 \u30ab\u30f3\u30de\u306e\u3042\u3068\u306b\u30b9\u30da\u30fc\u30b9\u3092\u5165\u308c\u308b\u3002 \u6700\u5f8c\u306e\u8981\u7d20\u306b\u30ab\u30f3\u30de\u3082\u3064\u3051\u308b\uff08\u62ec\u5f27\u9589\u3058\u3092\u6b21\u306e\u884c\u306b\u3059\u308b\u3002\uff09 \u95a2\u6570\u306e\u5f15\u6570\u306e\u982d\u3092\u63c3\u3048\u3066\u6539\u884c\u3059\u308b\u3002 \u95a2\u6570\u9593\u306f\u4e8c\u884c\u3042\u3051\u308b\u3002 \u30af\u30e9\u30b9\u306e\u30e1\u30bd\u30c3\u30c9\u9593\u306f1\u884c import \u306e\u9806\u756a standrd library third party our library local library","title":"pep8"},{"location":"python_basis/#_4","text":"pyenv + pipenv\u3092\u4f7f\u3046\u3002","title":"\u74b0\u5883"},{"location":"python_basis/#_5","text":"Python\u3067\u306f \u5168\u3066\u53c2\u7167\u6e21\u3057 \u3002 constant variable \u5927\u6587\u5b57\u3067\u66f8\u304f \u30e2\u30b8\u30e5\u30fc\u30eb \uff08\u30d5\u30a1\u30a4\u30eb\u5358\u4f4d\u3067\u5206\u3051\u3066\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u8a18\u8f09\u3057\u305f\u3082\u306e\uff09\uff1c \u30d1\u30c3\u30b1\u30fc\u30b8 \uff08\u8907\u6570\u306e\u30d5\u30a1\u30a4\u30eb\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u9020\u304b\u3089\u306a\u308a\u3001\u30eb\u30fc\u30eb\u306b\u5f93\u3063\u3066\u305d\u308c\u3089\u3092\u3072\u3068\u56fa\u307e\u308a\u306b\u3057\u305f\u3082\u306e\uff09\uff1c \u30e9\u30a4\u30d6\u30e9\u30ea \u3002 \u30d1\u30c3\u30b1\u30fc\u30b8\u306b\u306f\u3001\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u968e\u5c64\u5316\u3055\u305b\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u8aad\u307f\u8fbc\u307e\u305b\u308b\u305f\u3081\u306b init .py \u304c\u5fc5\u8981\uff08\u3053\u306e\u3068\u304d\u76f8\u5bfeimport\u3082\u884c\u3046\uff09 \u5f15\u6570\uff08arguments\uff09 \uff1a\u5b9f\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024 \u30d1\u30e9\u30e1\u30fc\u30bf\uff08parameters\uff09 \uff1a\u4eee\u5f15\u6570\u3002\u95a2\u6570\u306b\u6e21\u3055\u308c\u308b\u5177\u4f53\u7684\u306a\u5024\u306e\u30d7\u30ec\u30fc\u30b9\u30db\u30eb\u30c0\u3002 https://qiita.com/raviqqe/items/ee2bcb6bef86502f8cc6#%E5%BC%95%E6%95%B0%E3%81%AF-2-x-2--4-%E7%A8%AE%E9%A1%9E positional paremeters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\uff08arguments\uff09\u306a\u3057\u3002 keyword parameters \uff1a\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u5024\u3042\u308a\u3002 \u53ef\u5909\u9577\u5f15\u6570 : args, *kwargs, \u69d8\u3005\u306a\u9577\u3055\u306e\u5f15\u6570\u3092\u53d7\u3051\u53d6\u308c\u308b\u3002 global \u3068 nonlocal \uff08nested\u95a2\u6570\u306e\u3068\u304d\u306b\u5b9a\u7fa9\uff09 \u30e9\u30e0\u30c0\u95a2\u6570 \uff08\u95a2\u6570\u540d\u304c\u7121\u3044\u95a2\u6570\uff09:\u95a2\u6570\u540d\u3068\"return\"\u3092\u7121\u304f\u3059\u3002filter \u95a2\u6570\u306e\u969b\u306b\u4f7f\u3046\u3002 1 lm_add = lambda x , y : x + y \u95a2\u6570\u3082\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3001\u95a2\u6570\u3092\u5f15\u6570\u3067\u3068\u308c\u308b\u3001\u95a2\u6570\u3082return\u3067\u304d\u308b\uff08\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3068\u3057\u3066\u8fd4\u3059\u3002\uff09Closure:\u72b6\u614b\u3092\u30ad\u30fc\u30d7\u3057\u305f\u95a2\u6570\u3002\uff08\u72b6\u614b\u3092\u52d5\u7684\u30fb\u9759\u7684\uff09 sys.path :\u306b\u5165\u308c\u308b\u3068\u30ab\u30b9\u30bf\u30e0\u3067\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3048\u308b\u3002pip\u3092\u4f7f\u3046\u3068site-packages\u306e\u4e2d\u3067\u7ba1\u7406\u3055\u308c\u308b\u3002","title":"\u95a2\u6570"},{"location":"python_basis/#_6","text":"re.search('[0-9]', string) re.search('^[0-9]', string):\u6700\u521d\u306e\u6587\u5b57 re.search('^[0-9]{4}', string):\u6700\u521d\u306e\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}', string):\u6700\u521d\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('^[0-9]{2-4}$', string):\u6700\u5f8c\u306e\u6587\u5b572-4\u6587\u5b57 \u30ea\u30d4\u30fc\u30c8 re.search('a*b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('a+b', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30921\u56de\u4ee5\u4e0a\u7e70\u308a\u8fd4\u3059 re.search('ab?c', 'aaaab')\u5de6\u306e\u30d1\u30bf\u30fc\u30f3\u30920\u56de\u304b1\u56de\u7e70\u308a\u8fd4\u3059 abc|012 or te(s|x)t \u30b0\u30eb\u30fc\u30d7 'h.t'\u4efb\u610f\u306e\u4e00\u6587\u5b57 \u30a8\u30b9\u30b1\u30fc\u30d7'h.t' \\w [a-zA-Z0-9_]\u306b\u30de\u30c3\u30c1","title":"\u6b63\u898f\u8868\u73fe"},{"location":"python_basis/#_7","text":"\u5c5e\u6027\uff08\u30e1\u30f3\u30d0\u5909\u6570\u3001\u30e1\u30f3\u30d0\u95a2\u6570\uff09, \u30e1\u30bd\u30c3\u30c9\uff08instancemethod, static method, class method\uff09, \u30d7\u30ed\u30d1\u30c6\u30a3 * \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5909\u6570\u3068\u30af\u30e9\u30b9\u5909\u6570 https://docs.python.org/ja/3/library/functions.html https://qiita.com/ichi_taro3/items/cd71a8e43040abb446a1 \u6163\u7fd2\u7684\u306a\u547d\u540d\u898f\u5247\u3068\u3057\u3066\u306e\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8(non public)\u5316\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc\uff09\u3002\u6226\u95d8\u306b\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc\u3092\u3064\u3051\u3066_\u540d\u524d\u3068\u3059\u308b \u30cd\u30fc\u30e0\u30de\u30f3\u30b0\u30ea\u30f3\u30b0\uff08\u96e3\u53f7\u5316\uff09\u8981\u7d20\u540d\u306e\u524d\u306b\"__\"\uff08\u30a2\u30f3\u30c0\u30fc\u30d0\u30fc2\u3064\uff09\u3092\u3064\u3051\u307e\u3059\u3002 \u7d99\u627f\u6642\u306e\u540d\u524d\u4fee\u98fe\u306f__\u3092\u4f7f\u3044\u3053\u306a\u3059\u3002 \u30dd\u30ea\u30e2\u30fc\u30d5\u30a3\u30ba\u30e0\u306f\u7d99\u627f\u3092\u3057\u3066\u3044\u308bint\u3082str\u3082print\u3092\u3059\u308b\u3068\u540c\u3058\u3088\u3046\u306b\u632f\u308b\u821e\u3046 \u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\uff1a\u30b5\u30d6\u30af\u30e9\u30b9\u306e\u3067\u540c\u3058\u540d\u524d\u306e\u95a2\u6570\u3092\u5b9a\u7fa9\u3059\u308b\u3002 .\u3068\u304b..\u3067\u76f8\u5bfe\u30a4\u30f3\u30dd\u30fc\u30c8\u3067\u304d\u308b\u3002","title":"\u30af\u30e9\u30b9"},{"location":"python_basis/#_8","text":"https://qiita.com/koshigoe/items/848ddc0272b3cee92134 @staticmethod \uff1a\u307b\u3068\u3093\u3069\u30af\u30e9\u30b9\u5916\u306e\u95a2\u6570\u3068\u3057\u3066\u6271\u3046\u3002\uff08self\u306f\u3044\u3089\u306a\u3044\uff09 @classmethod \uff1acls\u306b\u5f15\u6570\u3092\u3068\u3063\u3066\u3001class\u306e\u60c5\u5831\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002\u7d99\u627f\u3059\u308b\u3068\u304d\u306fstaticmethod\u3067\u306f\u547c\u3076\u3068\u304d\u306b\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3002classmethod\u3092\u4f7f\u3046\u3002 @property \uff1a\u5909\u6570\u3092\u30ab\u30d7\u30bb\u30eb\u5316\u3057\u3001\u5909\u6570\u3060\u3051\u5916\u306b\u8fd4\u305b\u308b\u3088\u3046\u306b\u3059\u308b\u3002setter\u3068\u30bb\u30c3\u30c8\u3067\u4f7f\u3046\u3002\u5916\u304b\u3089\u5185\u90e8\u306e\u5024\u3092\u30bb\u30c3\u30c8\u3067\u304d\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 class Example : def __init__ ( self , x , y ): self . _x = x self . _y = y @property def x ( self ): return self . _x @x . setter def x ( self , dx ): self . _x += dx self . _x = max ( 0 , min ( self . MAX_X , self . _x )) @dataclass \uff1a\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308b\u30af\u30e9\u30b9\u3092\u7c21\u5358\u306b\u4f5c\u6210\u3059\u308b\u30c7\u30b3\u30ec\u30fc\u30bf\u306a\u3069\u3092\u63d0\u4f9b 1 2 3 4 5 6 7 8 @dataclass class InventoryItem : name : str price : float quantity : int = 0 def total_cost ( self ) -> float : return self . price * self . quantity","title":"\u30c7\u30b3\u30ec\u30fc\u30bf"},{"location":"python_basis/#_9","text":"http://diveintopython3-ja.rdy.jp/special-method-names.html __init__ \uff1a\u30af\u30e9\u30b9\u306e\u521d\u671f\u5316\u3002\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3092\u547c\u3076\u3002\u89aa\u30af\u30e9\u30b9\u306e\u30b3\u30f3\u30b9\u30bf\u30af\u30bf\u3092\u547c\u3076\u3068\u304d\u306f\u3001super. init ()\u3068\u3059\u308b\u3002 __del__ \uff1a\u30c7\u30b9\u30c8\u30e9\u30af\u30bf\u3002\u57fa\u672c\u7684\u306b\u306f\u4f7f\u308f\u306a\u3044\u3002\u4ee3\u308f\u308a\u306bwith\u69cb\u6587\u3092\u4f7f\u3046\u3002 __call__ \uff1a\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u95a2\u6570\u306e\u3088\u3046\u306b\u6271\u3048\u308b\u3002 __len__ \uff1a len\u30e1\u30bd\u30c3\u30c9\u306b\u5bfe\u3059\u308bint\u578b\u306e\u5024\u3092\u8fd4\u3059 __getitem__ :\u30a4\u30f3\u30c7\u30af\u30b7\u30f3\u30b0\uff08\u914d\u5217\u306e\u3088\u3046\u306b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a2\u30af\u30bb\u30b9\u3059\u308b\uff09\u3002\u30b7\u30fc\u30b1\u30f3\u30b9\u578b\uff08list, tuple, str, range\uff09 __iter__ :iterator\u3092\u8fd4\u3059\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9 __next__ :\u8981\u7d20\u3092\u53cd\u5fa9\u3057\u3066\u53d6\u308a\u51fa\u3059\u3053\u3068\u306e\u3067\u304d\u308b\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9\u3067\u3059\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class FooIterator (): def __init__ ( self , foo ): # foo\u306fiterable\u306a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8 self . _i = 0 self . _foo = foo def __iter__ ( self ): return self def __next__ ( self ): try : v = self . _foo . _L [ self . _i ] self . _i += 1 return v except IndexError : raise StopIteration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Odd : def __init__ ( self ): self . i = 1 def __contains__ ( self , x ): return x % 2 == 1 def __str__ ( self ): return \"odd numbers\" def __iter__ ( self ): return self def __next__ ( self ): result = self . i self . i += 2 return result def __getitem__ ( self , i ): return 2 * i + 1 def __len__ ( self ): return 0 __contains__ \uff1a\u30b3\u30f3\u30c6\u30ca\u578b\u3092\u5b9a\u7fa9\u3002\u72ec\u81ea\u30af\u30e9\u30b9\u306bin\u3092\u5b9a\u7fa9\u3067\u304d\u308b\u3002\uff08list, dict, tuple, str, collections.defaultdict\uff09 __str__ \uff1aprint\u95a2\u6570\u306e\u969b\u306b\u547c\u3070\u308c\u308b\u3002 __repr__ \uff1a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u518d\u751f\u6210\u3059\u308b\u305f\u3081\u306b\u4f7f\u3048\u308b\u3088\u3046\u306a\u3088\u308a\u6b63\u78ba\u306a\u60c5\u5831\u3092\u8fd4\u3059 __name__ :\u30e2\u30b8\u30e5\u30fc\u30eb\u5185\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u306b\u304a\u3044\u3066\u306f\u30e2\u30b8\u30e5\u30fc\u30eb\u540d\u3002\u95a2\u6570\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u304a\u3044\u3066\u306f\u95a2\u6570\u540d\u306b\u306a\u308b\u3002 __doc__ : __buidins__ :\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3002 set or get members getitem (), setitem () and iterate over items iter () and to get the number of items - method len () The behaviour of sorted() built-in function is to iterate over elements of your container and compare them using methods you mentioned cmp (), ge (), le () This is because reversing a collection doesn't care about values of items but sorting it depends on these values. \u2013 ElmoVanKielmo Feb 19 '18 at 15:12 it seems to me the only important difference is that reversed() looks for a reversed () method in the container its reversing whereas sorted() doesn't look for a sorted () method. \u2013 gregrf Feb 19 '18 at 15:17 This is the technical difference https://stackoverflow.com/questions/48868228/is-there-a-magic-method-for-sorted-in-python","title":"\u30de\u30b8\u30c3\u30af\u30e1\u30bd\u30c3\u30c9\uff08\u7279\u6b8a\u30e1\u30bd\u30c3\u30c9\uff09"},{"location":"python_basis/#generetor","text":"\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u53d6\u308a\u306a\u3069\u90e8\u5206\u7684\u306b\u30e1\u30e2\u30ea\u306b\u8f09\u305b\u3066\u3044\u304f\u5834\u5408\u306b\u4f7f\u3046\u3002 generator\u306f\u30e9\u30f3\u30c0\u30e0\u30a2\u30af\u30bb\u30b9\u3067\u304d\u306a\u3044 ex. range(10) https://www.atmarkit.co.jp/ait/articles/1908/20/news024.html https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee 1 2 3 4 5 6 7 8 # generator expression ( x * x for x in numbers ) # generator function # \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u3063\u3066yield\u3092\u8fd4\u3059\u3002 # next\u3067\u5024\u3092\u53d6\u5f97\u3067\u304d\u308b\u3002 def gen_func (): for x in numbers : yield x * x","title":"generetor \u5f0f"},{"location":"python_basis/#_10","text":"https://note.nkmk.me/python-error-message/ https://note.nkmk.me/python-try-except-else-finally/ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 try : print ( a / b ) #\u4f8b\u5916\u3092\u610f\u56f3\u7684\u306b\u767a\u751f\u3055\u305b\u308b raise ZeroDivisionError except ZeroDivisionError as e : print ( 'catch ZeroDivisionError:' , e ) except ValueError as e : print ( '\u6570\u5b57\u4ee5\u5916\u304c\u5165\u529b\u3055\u308c\u307e\u3057\u305f\u3002\u6570\u5b57\u306e\u307f\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044' ) print ( 'catch ValueError:' , e ) else : \u4f8b\u5916\u304c\u8d77\u304d\u305f\u3068\u304d\u306f\u5b9f\u884c\u3057\u306a\u3044\u30b3\u30fc\u30c9 finally : \u5e38\u306b\u5b9f\u884c\u3059\u308b\u30b3\u30fc\u30c9 finally \u306f\u30ad\u30e3\u30c3\u30c1\u3055\u308c\u306a\u304f\u3066\u3082\u30a8\u30e9\u30fc\u306e\u524d\u306b\u5b9f\u884c\u3055\u308c\u308b raise \u306f\u30a8\u30e9\u30fc\u3092\u767a\u751f\u3055\u305b\u308b\u3002 \u4f8b\u5916\u306e\u81ea\u4f5c Exception \u30af\u30e9\u30b9\u3092\u7d99\u627f\u3059\u308b traceback.print_exc() tracebackmodudle","title":"\u30a8\u30e9\u30fc"},{"location":"python_basis/#_11","text":"","title":"\u30c6\u30b9\u30c8"},{"location":"python_basis/#unittest","text":"assert\u30b3\u30fc\u30c9\u3067\u30c6\u30b9\u30c8\u3057\u3066\u3044\u304f\u3002 \u30c6\u30b9\u30c8\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u66f8\u304f\u3002 Test runnner: unittest, self.assertEqual(power(base, exp), 8) python -m unittest test.py\u3092\u4f7f\u3046 \u4f8b\u5916\u30b1\u30fc\u30b9\u306fwith \u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u4f7f\u3063\u3066\u66f8\u304f\u3002 with self.assertRaise(Typeerror) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #https://qiita.com/phorizon20/items/acb929772aaae4f52101 def fizzbuzz ( number ): if number % 15 == 0 : return \"FizzBuzz\" if number % 5 == 0 : return \"Buzz\" if number % 3 == 0 : return \"Fizz\" return number import unittest import fizzbuzz as fb class FizzBuzzTest ( unittest . TestCase ): def setUp ( self ): # \u521d\u671f\u5316\u51e6\u7406 pass def tearDown ( self ): # \u7d42\u4e86\u51e6\u7406 pass def test_normal ( self ): self . assertEqual ( 1 , fb . fizzbuzz ( 1 )) def test_fizz ( self ): self . assertEqual ( \"Fizz\" , fb . fizzbuzz ( 3 )) def test_buzz ( self ): self . assertEqual ( \"Buzz\" , fb . fizzbuzz ( 5 )) def test_fizzbuzz ( self ): self . assertEqual ( \"FizzBuzz\" , fb . fizzbuzz ( 15 ))","title":"unittest"},{"location":"python_basis/#pytest","text":"pytest assert\u3067\u7c21\u6f54\u306b\u66f8\u3051\u308b\u3002 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #https://kazuhira-r.hatenablog.com/entry/2020/03/14/173536 class Calc : def add ( self , x , y ): return x + y def minus ( self , x , y ): return x - y def multiply ( self , x , y ): return x * y def divide ( self , x , y ): return x / y from sample.calc import Calc def test_add (): calc = Calc () assert calc . add ( 1 , 3 ) == 4 def test_minus (): calc = Calc () assert calc . minus ( 5 , 3 ) == 2 def test_multiply (): calc = Calc () assert calc . multiply ( 2 , 3 ) == 6 def test_divide (): calc = Calc () assert calc . divide ( 10 , 2 ) == 5 \u30c6\u30b9\u30c8\u30ab\u30d0\u30ec\u30c3\u30b8 pytest-cov:\u30ab\u30d0\u30fc\u7387\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002 html\u3084xml\u3067\u51fa\u529b\u3067\u304d\u308b\u3002--cov-append","title":"pytest"},{"location":"python_basis/#_12","text":"https://qiita.com/ganyariya/items/fb3f38c2f4a35d1ee2e8 https://qiita.com/knknkn1162/items/17f7f370a2cc27f812ee http://diveintopython3-ja.rdy.jp/special-method-names.html","title":"\u30ea\u30f3\u30af"},{"location":"python_basis/#utils","text":"","title":"utils"},{"location":"python_basis/#_13","text":"1 2 import glob glob . glob ( '/folder/* /*.dcm' )","title":"\u30d5\u30a1\u30a4\u30eb\u53d6\u5f97"},{"location":"python_basis/#_14","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 import pickle with open ( \"data.pkl\" , \"wb\" ) as pkl_handle : pickle . dump ( dictionary_data , pkl_handle ) # LOAD with open ( \"data.pkl\" , \"rb\" ) as pkl_handle : output = pickle . load ( pkl_handle ) import mpu your_data = { 'foo' : 'bar' } mpu . io . write ( 'filename.pickle' , data ) unserialized_data = mpu . io . read ( 'filename.pickle' ) # https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict CSV : Super simple format (read & write) JSON : Nice for writing human-readable data; VERY commonly used (read & write) YAML : YAML is a superset of JSON, but easier to read (read & write, comparison of JSON and YAML) pickle : A Python serialization format (read & write) MessagePack (Python package): More compact representation (read & write) HDF5 (Python package): Nice for matrices (read & write) XML : exists too sigh (read & write)","title":"\u8f9e\u66f8\u306a\u3069\u306e\u4fdd\u5b58"},{"location":"python_basis/#dicom","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def dicom2array ( path , voi_lut = True , fix_monochrome = True ): # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way dicom = pydicom . read_file ( path ) # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to # \"human-friendly\" view if voi_lut : data = apply_voi_lut ( dicom . pixel_array , dicom ) else : data = dicom . pixel_array # depending on this value, X-ray may look inverted - fix that: if fix_monochrome and dicom . PhotometricInterpretation == \"MONOCHROME1\" : data = np . amax ( data ) - data data = data - np . min ( data ) data = data / np . max ( data ) data = ( data * 255 ) . astype ( np . uint8 ) return data def resize ( array , size , keep_ratio = False , resample = Image . LANCZOS ): # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image im = Image . fromarray ( array ) if keep_ratio : im . thumbnail (( size , size ), resample ) else : im = im . resize (( size , size ), resample ) return im def resize_and_save ( file_path ): split = 'train' if 'train' in file_path else 'test' base_dir = f '/kaggle/working/ { split } ' img = dicom2array ( file_path ) h , w = img . shape [: 2 ] # orig hw if aspect_ratio : r = dim / max ( h , w ) # resize image to img_size interp = cv2 . INTER_AREA if r < 1 else cv2 . INTER_LINEAR if r != 1 : # always resize down, only resize up if training with augmentation img = cv2 . resize ( img , ( int ( w * r ), int ( h * r )), interpolation = interp ) else : img = cv2 . resize ( img , ( dim , dim ), cv2 . INTER_AREA ) filename = file_path . split ( '/' )[ - 1 ] . split ( '.' )[ 0 ] cv2 . imwrite ( os . path . join ( base_dir , f ' { filename } .jpg' ), img ) return filename . replace ( 'dcm' , '' ) + '_image' , w , h","title":"dicom\u30d5\u30a1\u30a4\u30eb"},{"location":"python_other/","text":"logger, argparse pathlib glob nvidia\u3068\u304bstylegan\u5468\u308a\u306e\u5b9f\u88c5\u3092\u8aad\u3080 \u30d5\u30a9\u30eb\u30c0\u691c\u7d22 subprocess https://analytics-note.xyz/programming/glob-recursive/ https://qiita.com/ikura1/items/e94d82692a7a101fde53","title":"Python other"},{"location":"pytorch_basis/","text":"Pytorch basis \u00b6 \u30c1\u30fc\u30c8\u30b7\u30fc\u30c8 \u00b6 https://qiita.com/dokkozo/items/e173acded17a142e6d02 Basis \u00b6 flatten() : view(-1) squeeze() : view(*[s for s int t.shape if s != 1])\u3000\u8981\u7d20\u6570\u304c1\u306e\u8ef8\u3092\u524a\u9664\u3059\u308b\u3002\u6b21\u5143\u3092\u6e1b\u3089\u3059\u3002 unsqueeze(i) view( t.shape[:i-1], 1, t.shape[i:])\u3000\u6b21\u5143\u3092\u5897\u3084\u3059 https://stackoverflow.com/questions/57234095/what-is-the-difference-between-flatten-and-view-1-in-pytorch Utils \u00b6 functions \u00b6 Sigmoid\u95a2\u6570\uff08ndarray\uff09 \u00b6 CPU\u4e0a\u3067Sigmoid\u95a2\u6570\u3092\u4f7f\u3046\u3002 1 2 3 4 5 6 def sigmoid ( gamma ): if gamma < 0 : return 1 - 1 / ( 1 + math . exp ( gamma )) return 1 / ( 1 + math . exp ( - gamma )) sigmoid_v = np . vectorize ( sigmoid ) seed\u8a2d\u5b9a \u00b6 1 2 3 4 5 6 7 8 def set_seed ( seed = 0 ): np . random . seed ( seed ) random_state = np . random . RandomState ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) return random_state mixup \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def mixup_data ( inputs , targets , alpha = 1.0 ): if alpha > 0 : lam = np . random . beta ( alpha , alpha ) else : lam = 1 batch_size = inputs . size ()[ 0 ] index = torch . randperm ( batch_size ) mixed_inputs = lam * inputs + ( 1 - lam ) * inputs [ index , :] targets_a , targets_b = targets , targets [ index ] return mixed_inputs , targets_a , targets_b , lam def mixup_criterion ( criterion , outputs , targets_a , targets_b , lam ): return lam * criterion ( outputs , targets_a ) + ( 1 - lam ) * criterion ( outputs , targets_b ) classes \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class AverageMeter (): def __init__ ( self ): self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def reset ( self ): self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def update ( self , val , n = 1 ): self . val = val self . sum += val * n self . count += n self . avg = self . sum / self . count Dataset \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Dataset (): def __init__ ( self , image_paths , targets , transform = None ): self . image_paths = image_paths self . targets = targets self . transform = None def __len__ ( self ): return len ( self . image_paths ) def __getitem__ ( self , item ): targets = self . targets [ item ] image = np . load ( self . image_paths [ item ]) image = image [ np . newaxis , ] if self . transform : image = self . transform ( image ) return torch . tensor ( image , dtype = torch . float ), torch . tensor ( targets , dtype = torch . float ) Model \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 class BasicNN ( nn . Module ): def __init__ ( self ): super () . __init__ () self . model = timm . create_model ( CFG . pretrained_model_name , pretrained = CFG . pretrained , in_chans = CFG . input_channels ) if not CFG . pretrained : self . model . load_state_dict ( torch . load ( CFG . pretrained_path )) self . model . classifier = nn . Linear ( self . model . classifier . in_features , CFG . out_dim ) def forward ( self , inputs ): outputs = self . model ( inputs ) return outputs Trainer \u00b6 \u521d\u671f\u5316 \u00b6 \u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u306e\u4fdd\u5b58\uff06\u8aad\u307f\u8fbc\u307f : torch.save(model.state_dict(), model_path) , model.load_state_dict(torch.load(model_path)) AMP\uff08Automatic Mixed Precision\uff1a\u6df7\u5408\u7cbe\u5ea6\uff09\u3092\u7528\u3044\u305f\u9ad8\u901f\u5316 :\u901a\u5e38\u3001FP32\uff0832\u30d3\u30c3\u30c8\u6d6e\u52d5\u5c0f\u6570\u70b9\uff09\u3067\u8a08\u7b97\u3055\u308c\u307e\u3059\u304c\u3001\u534a\u5206\u306eFP16\uff0816\u30d3\u30c3\u30c8\u6d6e\u52d5\u5c0f\u6570\u70b9\uff09\u3067\u7cbe\u5ea6\u3092\u843d\u3068\u3055\u305a\u306b\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u91cf\u3092\u7bc0\u7d04\u3057\u3001\u8a08\u7b97\u901f\u5ea6\u3082\u5411\u4e0a\u3055\u305b\u308b\u6a5f\u80fd\u3002\u8a08\u7b97\u7cbe\u5ea6\u3092\u843d\u3068\u3057\u3066\u3082\u63a8\u8ad6\u306e\u7cbe\u5ea6\u304c\u843d\u3061\u306b\u304f\u3044 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Trainer (): def __init__ ( self , model , train_dataset , valid_dataset = None , train_batchsize = 16 , valid_batchsize = 16 , valid_targets = None , num_workers = 4 , fp16 = True , multiple_GPU = False , determinstic = True , benchmark = False ): self . device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) self . model = model self . model . to ( self . device ) self . valid_targets = valid_targets self . criterion = nn . BCEWithLogitsLoss () self . optimizer = self . configure_optimizer () self . scheduler_after_step = self . configure_scheduler_after_step () self . scheduler_after_epoch = self . configure_scheduler_after_epoch () torch . backends . cudnn . deterministic = determinstic torch . backends . cudnn . benchmark = benchmark self . fp16 = fp16 self . scaler = torch . cuda . amp . GradScaler () self . current_epoch = 0 if num_workers == - 1 : num_workers = psutil . cpu_count () self . multiple_GPU = multiple_GPU if multiple_GPU and torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) self = nn . DataParallel ( self ) self . train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = train_batchsize , shuffle = True , num_workers = num_workers , drop_last = True , pin_memory = True ) self . valid_loader = torch . utils . data . DataLoader ( dataset = valid_dataset , batch_size = valid_batchsize , shuffle = False , num_workers = num_workers , drop_last = False , pin_memory = True ) wandb\u521d\u671f\u5316 \u00b6 1 2 3 4 5 6 7 8 9 10 11 def _init_wandb ( self , cfg ): hyperparams = { 'batch_size' : cfg . batch_size , 'epochs' : cfg . epochs } wandb . init ( config = hyperparams , project = cfg . project_name , name = cfg . wandb_exp_name , ) wandb . watch ( self . model ) optimizer, schedular, metric\u306e\u8a2d\u5b9a \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def configure_optimizer ( self ): opt = torch . optim . Adam ( self . model . parameters (), lr = CFG . lr ) return opt def configure_scheduler_after_step ( self ): sch = torch . optim . lr_scheduler . OneCycleLR ( optimizer = self . optimizer , epochs = CFG . epochs , steps_per_epoch = 3500 , max_lr = 5.0e-4 , pct_start = 0.1 , anneal_strategy = 'cos' , div_factor = 1.0e+3 , final_div_factor = 1.0e+3 ) return sch def configure_scheduler_after_epoch ( self ): return None def epoch_metrics ( self , outputs , targets ): preds = sigmoid_v ( outputs ) return metrics . roc_auc_score ( targets , preds ) def monitor_metrics ( self , outputs , targets ): preds = outputs . sigmoid () . cpu () . detach () . numpy () targets = targets . cpu () . detach () . numpy () if len ( np . unique ( targets )) > 1 : roc_auc = metrics . roc_auc_score ( targets , preds ) else : roc_auc = 0.5 return roc_auc 1step\u3054\u3068\u306e\u8a13\u7df4\u3001\u691c\u8a3c\u3001\u63a8\u8ad6 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def train_one_step ( self , inputs , targets ): inputs = inputs . to ( self . device , non_blocking = True ) targets = targets . to ( self . device , non_blocking = True ) inputs , targets_a , targets_b , lam = mixup_data ( inputs , targets , alpha = CFG . mixup_alpha ) self . optimizer . zero_grad () with torch . set_grad_enabled ( True ): if self . fp16 : with torch . cuda . amp . autocast ( self . fp16 ): outputs = self . model ( inputs ) outputs = outputs . flatten () #loss = self.criterion(outputs.flatten(), targets) loss = mixup_criterion ( self . criterion , outputs . flatten (), targets_a , targets_b , lam ) metrics = self . monitor_metrics ( outputs , targets ) self . scaler . scale ( loss ) . backward () self . scaler . step ( self . optimizer ) self . scaler . update () else : outputs = self . model ( inputs ) outputs = outputs . flatten () metrics = self . monitor_metrics ( outputs , targets ) loss = self . criterion ( outputs . flatten (), targets ) loss . backward () self . optimizer . step () if self . scheduler_after_step : self . scheduler_after_step . step () return outputs , loss , metrics def validate_one_step ( self , inputs , targets = None ): inputs = inputs . to ( self . device , non_blocking = True ) if targets is not None : targets = targets . to ( self . device , non_blocking = True ) with torch . no_grad (): outputs = self . model ( inputs ) outputs = outputs . flatten () loss = self . criterion ( outputs . flatten (), targets ) metrics = self . monitor_metrics ( outputs , targets ) return outputs , loss , metrics else : outputs = self . model ( inputs ) outputs = outputs . flatten () return outputs , None , None def predict_one_step ( self , inputs ): outputs , _ , _ = self . validate_one_step ( inputs ) return outputs 1epoch\u3054\u3068\u306e\u8a13\u7df4\u3001\u691c\u8a3c\u3001\u63a8\u8ad6 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def train_one_epoch ( self , data_loader ): self . model . train () running_loss , running_metrics = AverageMeter (), AverageMeter () tk0 = tqdm ( data_loader , total = len ( data_loader ), position = 0 , leave = True ) for b_idx , ( inputs , targets ) in enumerate ( tk0 ): _ , loss , metrics = self . train_one_step ( inputs , targets ) running_loss . update ( loss . item (), data_loader . batch_size ) running_metrics . update ( metrics , data_loader . batch_size ) current_lr = self . optimizer . param_groups [ 0 ][ 'lr' ] wandb . log ({ \"train/step\" : b_idx , \"train/loss_step\" : running_loss . avg , \"lr\" : current_lr }) tk0 . set_postfix ( train_loss = running_loss . avg , train_step_metrics = running_metrics . avg , stage = \"train\" , lr = current_lr ) if self . scheduler_after_epoch : self . scheduler_after_epoch . step () tk0 . close () return running_loss . avg def validate_one_epoch ( self , data_loader ): self . model . eval () running_loss , running_metrics = AverageMeter (), AverageMeter () outputs_list = [] tk0 = tqdm ( data_loader , total = len ( data_loader ), position = 0 , leave = True ) for b_idx , ( inputs , targets ) in enumerate ( tk0 ): outputs_one_batch , loss , metrics = self . validate_one_step ( inputs , targets ) outputs_list . append ( outputs_one_batch . cpu () . detach () . numpy ()) running_loss . update ( loss . item (), data_loader . batch_size ) running_metrics . update ( metrics , data_loader . batch_size ) tk0 . set_postfix ( valid_loss = running_loss . avg , validate_step_metrics = running_metrics . avg , stage = \"validation\" ) wandb . log ({ \"valid/step\" : b_idx , \"valid/metric_step\" : running_metrics . avg , \"valid/loss\" : running_loss . avg , }) outputs_arr = np . concatenate ( outputs_list ) valid_metric_val = self . epoch_metrics ( outputs_arr , self . valid_targets ) tk0 . close () return valid_metric_val , running_loss . avg def predict ( self , dataset , batch_size = 16 , num_workers = 8 , ): self . model . eval () self . test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = num_workers , drop_last = False , pin_memory = True ) outputs_list = [] tk0 = tqdm ( self . test_loader , total = len ( self . test_loader ), position = 0 , leave = True ) for b_idx , ( inputs , targets ) in enumerate ( tk0 ): outputs_one_batch = self . predict_one_step ( inputs ) outputs_list . append ( outputs_one_batch . cpu () . detach () . numpy ()) tk0 . set_postfix ( stage = \"inference\" ) tk0 . close () outputs_arr = np . concatenate ( outputs_list ) return outputs_arr \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def save ( self , model_path ): model_state_dict = self . model . state_dict () if self . optimizer is not None : opt_state_dict = self . optimizer . state_dict () else : opt_state_dict = None if self . scheduler_after_step is not None : sch_state_dict_after_step = self . scheduler_after_step . state_dict () else : sch_state_dict_after_step = None if self . scheduler_after_epoch is not None : sch_state_dict_after_epoch = self . scheduler_after_epoch . state_dict () else : sch_state_dict_after_epoch = None model_dict = {} model_dict [ \"state_dict\" ] = model_state_dict model_dict [ \"optimizer\" ] = opt_state_dict model_dict [ \"scheduler_after_step\" ] = sch_state_dict_after_step model_dict [ \"scheduler_after_epoch\" ] = sch_state_dict_after_epoch model_dict [ \"epoch\" ] = self . current_epoch model_dict [ \"fp16\" ] = self . fp16 model_dict [ \"multiple_GPU\" ] = self . multiple_GPU torch . save ( model_dict , model_path ) def load ( self , model_path ): self . device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) if next ( self . model . parameters ()) . device != self . device : self . model . to ( self . device ) model_dict = torch . load ( model_path , map_location = torch . device ( self . device )) self . model . load_state_dict ( model_dict [ \"state_dict\" ]) \u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u3068\u691c\u8a3c\uff08fit\u95a2\u6570\uff09 \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def fit ( self , cfg , epochs = 5 , checkpoint_save_path = './' , mode = 'max' , patience = 10 , delta = 0.001 , ): set_seed ( CFG . seed ) self . _init_wandb ( cfg ) path_directory = Path ( checkpoint_save_path ) if mode == 'max' : current_best_valid_metrics = - float ( 'inf' ) else : current_best_valid_metrics = float ( 'inf' ) early_stopping_counter = 0 for epoch in range ( epochs ): self . current_epoch = epoch train_loss = self . train_one_epoch ( self . train_loader ) if valid_dataset : valid_metrics , valid_loss = self . validate_one_epoch ( self . valid_loader ) # Early Stopping and save at the check points. if mode == 'max' : if valid_metrics < current_best_valid_metrics + delta : early_stopping_counter += 1 print ( f 'EarlyStopping counter: { early_stopping_counter } out of { patience } ' ) if early_stopping_counter >= patience : break else : print ( f \"Validation score improved ( { current_best_valid_metrics } --> { valid_metrics } ). Saving the check point!\" ) current_best_valid_metrics = valid_metrics self . save ( checkpoint_save_path + f \" { cfg . pretrained_model_name } _epoch { epoch } .cpt\" ) else : if valid_metrics > current_best_valid_metrics - delta : early_stopping_counter += 1 print ( f 'EarlyStopping counter: { early_stopping_counter } out of { patience } ' ) if early_stopping_counter >= patience : break else : print ( f \"Validation score improved ( { current_best_valid_metrics } --> { valid_metrics } ). Saving the check point!\" ) current_best_valid_metrics = valid_metrics self . save ( checkpoint_save_path + f \" { cfg . pretrained_model_name } _epoch { epoch } .cpt\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) print ( f 'epoch: { epoch } , validate_epoch_metrics : { valid_metrics } ' ) wandb . log ({ \"epoch\" : epoch , \"train/loss\" : train_loss , \"valid/loss\" : valid_loss , \"valid/metric\" : valid_metrics , }) wandb . finish () torch . cuda . empty_cache () gc . collect ()","title":"pytorch templates"},{"location":"pytorch_basis/#pytorch-basis","text":"","title":"Pytorch basis"},{"location":"pytorch_basis/#_1","text":"https://qiita.com/dokkozo/items/e173acded17a142e6d02","title":"\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8"},{"location":"pytorch_basis/#basis","text":"flatten() : view(-1) squeeze() : view(*[s for s int t.shape if s != 1])\u3000\u8981\u7d20\u6570\u304c1\u306e\u8ef8\u3092\u524a\u9664\u3059\u308b\u3002\u6b21\u5143\u3092\u6e1b\u3089\u3059\u3002 unsqueeze(i) view( t.shape[:i-1], 1, t.shape[i:])\u3000\u6b21\u5143\u3092\u5897\u3084\u3059 https://stackoverflow.com/questions/57234095/what-is-the-difference-between-flatten-and-view-1-in-pytorch","title":"Basis"},{"location":"pytorch_basis/#utils","text":"","title":"Utils"},{"location":"pytorch_basis/#functions","text":"","title":"functions"},{"location":"pytorch_basis/#sigmoidndarray","text":"CPU\u4e0a\u3067Sigmoid\u95a2\u6570\u3092\u4f7f\u3046\u3002 1 2 3 4 5 6 def sigmoid ( gamma ): if gamma < 0 : return 1 - 1 / ( 1 + math . exp ( gamma )) return 1 / ( 1 + math . exp ( - gamma )) sigmoid_v = np . vectorize ( sigmoid )","title":"Sigmoid\u95a2\u6570\uff08ndarray\uff09"},{"location":"pytorch_basis/#seed","text":"1 2 3 4 5 6 7 8 def set_seed ( seed = 0 ): np . random . seed ( seed ) random_state = np . random . RandomState ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) os . environ [ 'PYTHONHASHSEED' ] = str ( seed ) return random_state","title":"seed\u8a2d\u5b9a"},{"location":"pytorch_basis/#mixup","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 def mixup_data ( inputs , targets , alpha = 1.0 ): if alpha > 0 : lam = np . random . beta ( alpha , alpha ) else : lam = 1 batch_size = inputs . size ()[ 0 ] index = torch . randperm ( batch_size ) mixed_inputs = lam * inputs + ( 1 - lam ) * inputs [ index , :] targets_a , targets_b = targets , targets [ index ] return mixed_inputs , targets_a , targets_b , lam def mixup_criterion ( criterion , outputs , targets_a , targets_b , lam ): return lam * criterion ( outputs , targets_a ) + ( 1 - lam ) * criterion ( outputs , targets_b )","title":"mixup"},{"location":"pytorch_basis/#classes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class AverageMeter (): def __init__ ( self ): self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def reset ( self ): self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def update ( self , val , n = 1 ): self . val = val self . sum += val * n self . count += n self . avg = self . sum / self . count","title":"classes"},{"location":"pytorch_basis/#dataset","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Dataset (): def __init__ ( self , image_paths , targets , transform = None ): self . image_paths = image_paths self . targets = targets self . transform = None def __len__ ( self ): return len ( self . image_paths ) def __getitem__ ( self , item ): targets = self . targets [ item ] image = np . load ( self . image_paths [ item ]) image = image [ np . newaxis , ] if self . transform : image = self . transform ( image ) return torch . tensor ( image , dtype = torch . float ), torch . tensor ( targets , dtype = torch . float )","title":"Dataset"},{"location":"pytorch_basis/#model","text":"1 2 3 4 5 6 7 8 9 10 11 12 class BasicNN ( nn . Module ): def __init__ ( self ): super () . __init__ () self . model = timm . create_model ( CFG . pretrained_model_name , pretrained = CFG . pretrained , in_chans = CFG . input_channels ) if not CFG . pretrained : self . model . load_state_dict ( torch . load ( CFG . pretrained_path )) self . model . classifier = nn . Linear ( self . model . classifier . in_features , CFG . out_dim ) def forward ( self , inputs ): outputs = self . model ( inputs ) return outputs","title":"Model"},{"location":"pytorch_basis/#trainer","text":"","title":"Trainer"},{"location":"pytorch_basis/#_2","text":"\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u306e\u4fdd\u5b58\uff06\u8aad\u307f\u8fbc\u307f : torch.save(model.state_dict(), model_path) , model.load_state_dict(torch.load(model_path)) AMP\uff08Automatic Mixed Precision\uff1a\u6df7\u5408\u7cbe\u5ea6\uff09\u3092\u7528\u3044\u305f\u9ad8\u901f\u5316 :\u901a\u5e38\u3001FP32\uff0832\u30d3\u30c3\u30c8\u6d6e\u52d5\u5c0f\u6570\u70b9\uff09\u3067\u8a08\u7b97\u3055\u308c\u307e\u3059\u304c\u3001\u534a\u5206\u306eFP16\uff0816\u30d3\u30c3\u30c8\u6d6e\u52d5\u5c0f\u6570\u70b9\uff09\u3067\u7cbe\u5ea6\u3092\u843d\u3068\u3055\u305a\u306b\u30e1\u30e2\u30ea\u306e\u4f7f\u7528\u91cf\u3092\u7bc0\u7d04\u3057\u3001\u8a08\u7b97\u901f\u5ea6\u3082\u5411\u4e0a\u3055\u305b\u308b\u6a5f\u80fd\u3002\u8a08\u7b97\u7cbe\u5ea6\u3092\u843d\u3068\u3057\u3066\u3082\u63a8\u8ad6\u306e\u7cbe\u5ea6\u304c\u843d\u3061\u306b\u304f\u3044 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Trainer (): def __init__ ( self , model , train_dataset , valid_dataset = None , train_batchsize = 16 , valid_batchsize = 16 , valid_targets = None , num_workers = 4 , fp16 = True , multiple_GPU = False , determinstic = True , benchmark = False ): self . device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) self . model = model self . model . to ( self . device ) self . valid_targets = valid_targets self . criterion = nn . BCEWithLogitsLoss () self . optimizer = self . configure_optimizer () self . scheduler_after_step = self . configure_scheduler_after_step () self . scheduler_after_epoch = self . configure_scheduler_after_epoch () torch . backends . cudnn . deterministic = determinstic torch . backends . cudnn . benchmark = benchmark self . fp16 = fp16 self . scaler = torch . cuda . amp . GradScaler () self . current_epoch = 0 if num_workers == - 1 : num_workers = psutil . cpu_count () self . multiple_GPU = multiple_GPU if multiple_GPU and torch . cuda . device_count () > 1 : print ( \"Let's use\" , torch . cuda . device_count (), \"GPUs!\" ) self = nn . DataParallel ( self ) self . train_loader = torch . utils . data . DataLoader ( dataset = train_dataset , batch_size = train_batchsize , shuffle = True , num_workers = num_workers , drop_last = True , pin_memory = True ) self . valid_loader = torch . utils . data . DataLoader ( dataset = valid_dataset , batch_size = valid_batchsize , shuffle = False , num_workers = num_workers , drop_last = False , pin_memory = True )","title":"\u521d\u671f\u5316"},{"location":"pytorch_basis/#wandb","text":"1 2 3 4 5 6 7 8 9 10 11 def _init_wandb ( self , cfg ): hyperparams = { 'batch_size' : cfg . batch_size , 'epochs' : cfg . epochs } wandb . init ( config = hyperparams , project = cfg . project_name , name = cfg . wandb_exp_name , ) wandb . watch ( self . model )","title":"wandb\u521d\u671f\u5316"},{"location":"pytorch_basis/#optimizer-schedular-metric","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def configure_optimizer ( self ): opt = torch . optim . Adam ( self . model . parameters (), lr = CFG . lr ) return opt def configure_scheduler_after_step ( self ): sch = torch . optim . lr_scheduler . OneCycleLR ( optimizer = self . optimizer , epochs = CFG . epochs , steps_per_epoch = 3500 , max_lr = 5.0e-4 , pct_start = 0.1 , anneal_strategy = 'cos' , div_factor = 1.0e+3 , final_div_factor = 1.0e+3 ) return sch def configure_scheduler_after_epoch ( self ): return None def epoch_metrics ( self , outputs , targets ): preds = sigmoid_v ( outputs ) return metrics . roc_auc_score ( targets , preds ) def monitor_metrics ( self , outputs , targets ): preds = outputs . sigmoid () . cpu () . detach () . numpy () targets = targets . cpu () . detach () . numpy () if len ( np . unique ( targets )) > 1 : roc_auc = metrics . roc_auc_score ( targets , preds ) else : roc_auc = 0.5 return roc_auc","title":"optimizer, schedular, metric\u306e\u8a2d\u5b9a"},{"location":"pytorch_basis/#1step","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def train_one_step ( self , inputs , targets ): inputs = inputs . to ( self . device , non_blocking = True ) targets = targets . to ( self . device , non_blocking = True ) inputs , targets_a , targets_b , lam = mixup_data ( inputs , targets , alpha = CFG . mixup_alpha ) self . optimizer . zero_grad () with torch . set_grad_enabled ( True ): if self . fp16 : with torch . cuda . amp . autocast ( self . fp16 ): outputs = self . model ( inputs ) outputs = outputs . flatten () #loss = self.criterion(outputs.flatten(), targets) loss = mixup_criterion ( self . criterion , outputs . flatten (), targets_a , targets_b , lam ) metrics = self . monitor_metrics ( outputs , targets ) self . scaler . scale ( loss ) . backward () self . scaler . step ( self . optimizer ) self . scaler . update () else : outputs = self . model ( inputs ) outputs = outputs . flatten () metrics = self . monitor_metrics ( outputs , targets ) loss = self . criterion ( outputs . flatten (), targets ) loss . backward () self . optimizer . step () if self . scheduler_after_step : self . scheduler_after_step . step () return outputs , loss , metrics def validate_one_step ( self , inputs , targets = None ): inputs = inputs . to ( self . device , non_blocking = True ) if targets is not None : targets = targets . to ( self . device , non_blocking = True ) with torch . no_grad (): outputs = self . model ( inputs ) outputs = outputs . flatten () loss = self . criterion ( outputs . flatten (), targets ) metrics = self . monitor_metrics ( outputs , targets ) return outputs , loss , metrics else : outputs = self . model ( inputs ) outputs = outputs . flatten () return outputs , None , None def predict_one_step ( self , inputs ): outputs , _ , _ = self . validate_one_step ( inputs ) return outputs","title":"1step\u3054\u3068\u306e\u8a13\u7df4\u3001\u691c\u8a3c\u3001\u63a8\u8ad6"},{"location":"pytorch_basis/#1epoch","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def train_one_epoch ( self , data_loader ): self . model . train () running_loss , running_metrics = AverageMeter (), AverageMeter () tk0 = tqdm ( data_loader , total = len ( data_loader ), position = 0 , leave = True ) for b_idx , ( inputs , targets ) in enumerate ( tk0 ): _ , loss , metrics = self . train_one_step ( inputs , targets ) running_loss . update ( loss . item (), data_loader . batch_size ) running_metrics . update ( metrics , data_loader . batch_size ) current_lr = self . optimizer . param_groups [ 0 ][ 'lr' ] wandb . log ({ \"train/step\" : b_idx , \"train/loss_step\" : running_loss . avg , \"lr\" : current_lr }) tk0 . set_postfix ( train_loss = running_loss . avg , train_step_metrics = running_metrics . avg , stage = \"train\" , lr = current_lr ) if self . scheduler_after_epoch : self . scheduler_after_epoch . step () tk0 . close () return running_loss . avg def validate_one_epoch ( self , data_loader ): self . model . eval () running_loss , running_metrics = AverageMeter (), AverageMeter () outputs_list = [] tk0 = tqdm ( data_loader , total = len ( data_loader ), position = 0 , leave = True ) for b_idx , ( inputs , targets ) in enumerate ( tk0 ): outputs_one_batch , loss , metrics = self . validate_one_step ( inputs , targets ) outputs_list . append ( outputs_one_batch . cpu () . detach () . numpy ()) running_loss . update ( loss . item (), data_loader . batch_size ) running_metrics . update ( metrics , data_loader . batch_size ) tk0 . set_postfix ( valid_loss = running_loss . avg , validate_step_metrics = running_metrics . avg , stage = \"validation\" ) wandb . log ({ \"valid/step\" : b_idx , \"valid/metric_step\" : running_metrics . avg , \"valid/loss\" : running_loss . avg , }) outputs_arr = np . concatenate ( outputs_list ) valid_metric_val = self . epoch_metrics ( outputs_arr , self . valid_targets ) tk0 . close () return valid_metric_val , running_loss . avg def predict ( self , dataset , batch_size = 16 , num_workers = 8 , ): self . model . eval () self . test_loader = torch . utils . data . DataLoader ( dataset = test_dataset , batch_size = batch_size , shuffle = False , num_workers = num_workers , drop_last = False , pin_memory = True ) outputs_list = [] tk0 = tqdm ( self . test_loader , total = len ( self . test_loader ), position = 0 , leave = True ) for b_idx , ( inputs , targets ) in enumerate ( tk0 ): outputs_one_batch = self . predict_one_step ( inputs ) outputs_list . append ( outputs_one_batch . cpu () . detach () . numpy ()) tk0 . set_postfix ( stage = \"inference\" ) tk0 . close () outputs_arr = np . concatenate ( outputs_list ) return outputs_arr","title":"1epoch\u3054\u3068\u306e\u8a13\u7df4\u3001\u691c\u8a3c\u3001\u63a8\u8ad6"},{"location":"pytorch_basis/#_3","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def save ( self , model_path ): model_state_dict = self . model . state_dict () if self . optimizer is not None : opt_state_dict = self . optimizer . state_dict () else : opt_state_dict = None if self . scheduler_after_step is not None : sch_state_dict_after_step = self . scheduler_after_step . state_dict () else : sch_state_dict_after_step = None if self . scheduler_after_epoch is not None : sch_state_dict_after_epoch = self . scheduler_after_epoch . state_dict () else : sch_state_dict_after_epoch = None model_dict = {} model_dict [ \"state_dict\" ] = model_state_dict model_dict [ \"optimizer\" ] = opt_state_dict model_dict [ \"scheduler_after_step\" ] = sch_state_dict_after_step model_dict [ \"scheduler_after_epoch\" ] = sch_state_dict_after_epoch model_dict [ \"epoch\" ] = self . current_epoch model_dict [ \"fp16\" ] = self . fp16 model_dict [ \"multiple_GPU\" ] = self . multiple_GPU torch . save ( model_dict , model_path ) def load ( self , model_path ): self . device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) if next ( self . model . parameters ()) . device != self . device : self . model . to ( self . device ) model_dict = torch . load ( model_path , map_location = torch . device ( self . device )) self . model . load_state_dict ( model_dict [ \"state_dict\" ])","title":"\u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f"},{"location":"pytorch_basis/#fit","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def fit ( self , cfg , epochs = 5 , checkpoint_save_path = './' , mode = 'max' , patience = 10 , delta = 0.001 , ): set_seed ( CFG . seed ) self . _init_wandb ( cfg ) path_directory = Path ( checkpoint_save_path ) if mode == 'max' : current_best_valid_metrics = - float ( 'inf' ) else : current_best_valid_metrics = float ( 'inf' ) early_stopping_counter = 0 for epoch in range ( epochs ): self . current_epoch = epoch train_loss = self . train_one_epoch ( self . train_loader ) if valid_dataset : valid_metrics , valid_loss = self . validate_one_epoch ( self . valid_loader ) # Early Stopping and save at the check points. if mode == 'max' : if valid_metrics < current_best_valid_metrics + delta : early_stopping_counter += 1 print ( f 'EarlyStopping counter: { early_stopping_counter } out of { patience } ' ) if early_stopping_counter >= patience : break else : print ( f \"Validation score improved ( { current_best_valid_metrics } --> { valid_metrics } ). Saving the check point!\" ) current_best_valid_metrics = valid_metrics self . save ( checkpoint_save_path + f \" { cfg . pretrained_model_name } _epoch { epoch } .cpt\" ) else : if valid_metrics > current_best_valid_metrics - delta : early_stopping_counter += 1 print ( f 'EarlyStopping counter: { early_stopping_counter } out of { patience } ' ) if early_stopping_counter >= patience : break else : print ( f \"Validation score improved ( { current_best_valid_metrics } --> { valid_metrics } ). Saving the check point!\" ) current_best_valid_metrics = valid_metrics self . save ( checkpoint_save_path + f \" { cfg . pretrained_model_name } _epoch { epoch } .cpt\" ) #writer.add_scalar(\"Loss/train\", 1.0, epoch) print ( f 'epoch: { epoch } , validate_epoch_metrics : { valid_metrics } ' ) wandb . log ({ \"epoch\" : epoch , \"train/loss\" : train_loss , \"valid/loss\" : valid_loss , \"valid/metric\" : valid_metrics , }) wandb . finish () torch . cuda . empty_cache () gc . collect ()","title":"\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u3068\u691c\u8a3c\uff08fit\u95a2\u6570\uff09"},{"location":"server_setup/","text":"","title":"Server setup"},{"location":"shellscript/","text":"shell script option","title":"Shellscript"},{"location":"slack_api/","text":"https://qiita.com/tomson784/items/406281bef7a5b2eb3cd8 https://nigimitama.hatenablog.jp/entry/2020/02/10/050000 https://qiita.com/bee2/items/9803f6b44527989496d0 https://zenn.dev/dhirooka/articles/f82744d2475b68","title":"slack API"},{"location":"ssh/","text":"\u30ea\u30e2\u30fc\u30c8\u3067Jupyterlab\u3092\u958b\u304f \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 ssh -L <host port>:localhost:<remote port> user@remote docker run -it --rm -p <host port>:<remote port> --name <container-name> -v: $PWD :/work -w /work <image-name> /bin/bash jupyter lab --ip 0 .0.0.0 --port <container port> --allow-root #\u540c\u6642\u306b docker run -it --rm -p 9999 :9999 --name kaggle-docker \\ -v $PWD :/work \\ -v /dataset:/dataset:ro \\ -w /work kaggle/python-gpu-build \\ jupyter lab --ip 0 .0.0.0 --port 9999 --allow-root https://uxmilk.jp/50946 1 2 3 ssh scp","title":"SSH"},{"location":"ssh/#jupyterlab","text":"1 2 3 4 5 6 7 8 9 10 11 12 ssh -L <host port>:localhost:<remote port> user@remote docker run -it --rm -p <host port>:<remote port> --name <container-name> -v: $PWD :/work -w /work <image-name> /bin/bash jupyter lab --ip 0 .0.0.0 --port <container port> --allow-root #\u540c\u6642\u306b docker run -it --rm -p 9999 :9999 --name kaggle-docker \\ -v $PWD :/work \\ -v /dataset:/dataset:ro \\ -w /work kaggle/python-gpu-build \\ jupyter lab --ip 0 .0.0.0 --port 9999 --allow-root https://uxmilk.jp/50946 1 2 3 ssh scp","title":"\u30ea\u30e2\u30fc\u30c8\u3067Jupyterlab\u3092\u958b\u304f"},{"location":"templates/","text":"TH \u5de6\u5bc4\u305b TH \u4e2d\u592e\u5bc4\u305b TH \u53f3\u5bc4\u305b TD TD TD TD TD TD Note \u3053\u308c\u306f\u30ce\u30fc\u30c8\u3067\u3059\u3002 Tip \u30d2\u30f3\u30c8\u3067\u3059\u3002 Warning \u3053\u308c\u306f\u8b66\u544a\u3067\u3059\u3002 Danger \u3053\u308c\u306f\u5371\u967a\u3067\u3059\u3002 Success \u3053\u308c\u306f\u6210\u529f\u3067\u3059\u3002 Failure \u3053\u308c\u306f\u5931\u6557\u3067\u3059\u3002 Bug \u3053\u308c\u306f\u30d0\u30b0\u3067\u3059\u3002 Summary \u3053\u308c\u306f\u6982\u8981\u3067\u3059\u3002 Mkdocs \u3068\u306f\u9759\u7684\u30b5\u30a4\u30c8\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3067\u3059\u3002 \u30b3\u30f3\u30c6\u30f3\u30c4\u306f\u57fa\u672c\u7684\u306b markdown 1 \u5f62\u5f0f\u3067\u8a18\u8ff0\u3057\u305f\u30bd\u30fc\u30b9\u30d5\u30a1\u30a4\u30eb\u306b\u306a\u308a\u307e\u3059\u3002 \u5b9a\u7fa9\u8a9e \u3053\u3053\u306b\u8aac\u660e\u3092\u66f8\u304d\u307e\u3059 \u6587\u66f8\u3092\u8a18\u8ff0\u3059\u308b\u305f\u3081\u306e\u8efd\u91cf\u30de\u30fc\u30af\u30a2\u30c3\u30d7\u8a00\u8a9e\u306e\u3072\u3068\u3064 \u21a9","title":"Templates"},{"location":"tmux/","text":"","title":"tmux"}]}