
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <meta name="author" content="Yu Saito">
      
      
        <link rel="canonical" href="https://www.yusaito.com/mynote/pytorch_basis/">
      
      <link rel="icon" href="../favicon.ico">
      <meta name="generator" content="mkdocs-1.2.1, mkdocs-material-7.1.7">
    
    

    
      
        <title>Pytorch basis - YS's note</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.ca7ac06f.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.f1a3b89f.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
      <link rel="stylesheet" href="https://unpkg.com/mermaid@8.0.0/dist/mermaid.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#kwargs-key1-1-key2-2-key3-3" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="YS&#39;s note" class="md-header__button md-logo" aria-label="YS's note" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            YS's note
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Pytorch basis
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../albumentations/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../linux_command/" class="md-tabs__link">
        Utils
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../python_basis/" class="md-tabs__link">
        Python
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../xgboost.md" class="md-tabs__link">
        ML
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="YS&#39;s note" class="md-nav__button md-logo" aria-label="YS's note" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    YS's note
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../albumentations/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" data-md-state="indeterminate" type="checkbox" id="__nav_2" checked>
      
      <label class="md-nav__link" for="__nav_2">
        Utils
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Utils" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Utils
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../linux_command/" class="md-nav__link">
        Linux
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../git/" class="md-nav__link">
        Git
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../docker/" class="md-nav__link">
        Docker
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../labrad/" class="md-nav__link">
        LabRAD
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" data-md-state="indeterminate" type="checkbox" id="__nav_3" checked>
      
      <label class="md-nav__link" for="__nav_3">
        Python
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Python" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../python_basis/" class="md-nav__link">
        Basis
      </a>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" data-md-state="indeterminate" type="checkbox" id="__nav_3_2" checked>
      
      <label class="md-nav__link" for="__nav_3_2">
        AL-DS
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="AL-DS" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          AL-DS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        DFS
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" data-md-state="indeterminate" type="checkbox" id="__nav_4" checked>
      
      <label class="md-nav__link" for="__nav_4">
        ML
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="ML" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          ML
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../xgboost.md" class="md-nav__link">
        Boosting
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../albumentations/" class="md-nav__link">
        albumentations
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../templates/" class="md-nav__link">
        pytorch templates
      </a>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_4" data-md-state="indeterminate" type="checkbox" id="__nav_4_4" checked>
      
      <label class="md-nav__link" for="__nav_4_4">
        Gereative models
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Gereative models" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          Gereative models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../albumentations/" class="md-nav__link">
        GAN basis
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../dcgan/" class="md-nav__link">
        other
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
    
                
                
                <p>def func_kwargs(**kwargs):<br />
    print('kwargs: ', kwargs)<br />
    print('type: ', type(kwargs))</p>
<p>func_kwargs(key1=1, key2=2, key3=3)</p>
<h1 id="kwargs-key1-1-key2-2-key3-3">kwargs:  {'key1': 1, 'key2': 2, 'key3': 3}<a class="headerlink" href="#kwargs-key1-1-key2-2-key3-3" title="Permanent link">¶</a></h1>
<h1 id="type">type:  <class 'dict'><a class="headerlink" href="#type" title="Permanent link">¶</a></h1>
<p>def func_kwargs_positional(arg1, arg2, **kwargs):<br />
    print('arg1: ', arg1)<br />
    print('arg2: ', arg2)<br />
    print('kwargs: ', kwargs)</p>
<p>func_kwargs_positional(0, 1, key1=1)</p>
<h1 id="arg1-0">arg1:  0<a class="headerlink" href="#arg1-0" title="Permanent link">¶</a></h1>
<h1 id="arg2-1">arg2:  1<a class="headerlink" href="#arg2-1" title="Permanent link">¶</a></h1>
<h1 id="kwargs-key1-1">kwargs:  {'key1': 1}<a class="headerlink" href="#kwargs-key1-1" title="Permanent link">¶</a></h1>
<h3 id="summary">Summaryの出し方<a class="headerlink" href="#summary" title="Permanent link">¶</a></h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">torchsummary</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg16</span><span class="p">()</span>
<span class="n">summary</span><span class="p">(</span><span class="n">vgg</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</code></pre></div>
</td></tr></table>
<p>これは実は，CrossEntropyLossはcallでforwardを呼ぶようになっており，つまり，</p>
<p>loss = criterion(outputs, labels)<br />
loss = criterion.forward(outputs, labels)<br />
この二つは同じことをしています．<br />
なのでloss = criterion(outputs, labels)がforwardになっています．</p>
<p>x = torch.autograd.Variable(torch.Tensor([3,4]), requires_grad=True)</p>
<h1 id="requires_gradtruevariable">requires_grad=Trueで，このVariableは微分するぞと伝える<a class="headerlink" href="#requires_gradtruevariable" title="Permanent link">¶</a></h1>
<p>print("x.grad : ", x.grad)</p>
<h1 id="none">None<a class="headerlink" href="#none" title="Permanent link">¶</a></h1>
<h1 id="_1">この時点ではまだ何も入っていない．<a class="headerlink" href="#_1" title="Permanent link">¶</a></h1>
<h1 id="_2">適当に目的関数を作る．<a class="headerlink" href="#_2" title="Permanent link">¶</a></h1>
<p>y = x[0]<em><em>2 + 5</em>x[1]  + x[0]</em>x[1]</p>
<h1 id="x0-2x0-x1">x[0]の導関数 : 2*x[0] + x[1]<a class="headerlink" href="#x0-2x0-x1" title="Permanent link">¶</a></h1>
<h1 id="x0-23-4-10">x[0]の微分係数 : 2*3 + 4 = 10<a class="headerlink" href="#x0-23-4-10" title="Permanent link">¶</a></h1>
<h1 id="x1-5-x0">x[1]の導関数 : 5 + x[0]<a class="headerlink" href="#x1-5-x0" title="Permanent link">¶</a></h1>
<h1 id="x1-5-3-8">x[1]の微分係数 : 5 + 3 = 8<a class="headerlink" href="#x1-5-3-8" title="Permanent link">¶</a></h1>
<p>y.backward()</p>
<h1 id="torchautogradbackwardy">torch.autograd.backward(y)　でも良い．<a class="headerlink" href="#torchautogradbackwardy" title="Permanent link">¶</a></h1>
<p>print("x.grad : ", x.grad)</p>
<h1 id="10">10<a class="headerlink" href="#10" title="Permanent link">¶</a></h1>
<h1 id="8">8<a class="headerlink" href="#8" title="Permanent link">¶</a></h1>
<h1 id="zero_grad">.zero_grad()の代わり<a class="headerlink" href="#zero_grad" title="Permanent link">¶</a></h1>
<p>x.grad = None</p>
<p>for inputs, labels in dataloaders[phase]:<br />
    inputs = inputs.to(device)<br />
    labels = labels.to(device)</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code># zero the parameter gradients
optimizer.zero_grad() #勾配の初期化

# forward
# track history if only in train
with torch.set_grad_enabled(phase == &#39;train&#39;):
    outputs = model(inputs) #ネットワーク出力層の（バッチ数, 次元(ex クラス数)）が出力される
    print(f&#39;outputs: {outputs}&#39;)
    _, preds = torch.max(outputs, 1) #最大となるindexを返す
    print(f&#39;preds: {preds}, labels: {labels}&#39;)
    loss = criterion(outputs, labels) #損失値を出す
    print(f&#39;loss: {loss}&#39;)

    # backward + optimize only if in training phase
    if phase == &#39;train&#39;:
        loss.backward()　#導関数の結果が累積
        optimizer.step()　#parameterの更新
</code></pre></div>
</td></tr></table>
<blockquote>
<blockquote>
<blockquote>
<p>date_info = {'year': "2020", 'month': "01", 'day': "01"}<br />
filename = "{year}-{month}-{day}.txt".format(**date_info)<br />
filename<br />
'2020-01-01.txt'</p>
</blockquote>
</blockquote>
</blockquote>
<p>scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)<br />
for epoch in range(0, 100): #ここは以下省略<br />
    scheduler.step() </p>
<p><img alt="" src="2021-06-18-18-56-51.png" /></p>
<p>os.cpu_count()<br />
psutill.cpu_count</p>
<p>AMP<br />
<a href="https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587">https://qiita.com/sugulu_Ogawa_ISID/items/62f5f7adee083d96a587</a></p>
<p>**dataは辞書を受け取る</p>
<p>selfで自分自身つまりforwardが呼ばれる</p>
<p>callback関数は非同期っぽいもの</p>
<p>損失関数<br />
<a href="https://yoshinashigoto-blog.herokuapp.com/detail/27/">https://yoshinashigoto-blog.herokuapp.com/detail/27/</a></p>
<p>from torchvision import models<br />
model = models.mnasnet0_5()<br />
torch.save(model.to('cpu').state_dict(), 'model.pth')</p>
<p>from torchvision import models<br />
model = models.mnasnet0_5()<br />
model.load_state_dict(torch.load('model.pth'))</p>
<h1 id="_3">学習途中の状態<a class="headerlink" href="#_3" title="Permanent link">¶</a></h1>
<p>epoch = 10</p>
<h1 id="_4">学習途中の状態を保存する。<a class="headerlink" href="#_4" title="Permanent link">¶</a></h1>
<p>torch.save(<br />
    {<br />
        "epoch": epoch,<br />
        "model_state_dict": model.state_dict(),<br />
        "optimizer_state_dict": optimizer.state_dict(),<br />
    },<br />
    "model.tar",<br />
)</p>
<h1 id="_5">学習途中の状態を読み込む。<a class="headerlink" href="#_5" title="Permanent link">¶</a></h1>
<p>checkpoint = torch.load("model.tar")<br />
model.load_state_dict(checkpoint["model_state_dict"])<br />
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])<br />
epoch = checkpoint["epoch"]</p>
<p>01.　損失関数とは</p>
<p>まず損失関数とは、ニューラルネットワークの予測がうまく行ったのかどうか判断するために使用する関数です。</p>
<p>この関数を使用して、予測と答えの誤差を求めます。</p>
<p>その誤差が最小になれば予測はより正確なものだったという評価がなされます。</p>
<p>損失関数には下で触れるだけの種類があり、目的によって使い分けます。</p>
<p>このような関数を用いて数学的なアプローチをすることで機械学習の予測の正確性を高めていきます。</p>
<p>以下では数学的な要素に踏み込みすぎない程度に、プログラミングでの活用方法をアウトプットしていきます。</p>
<p>ちなみにエントロピーとは不規則性の程度を表す量をいいます。</p>
<p>その通りといった感じですね。</p>
<p>_02.　バイナリ交差エントロピー損失</p>
<p>バイナリ交差エントロピー損失はデータのクラスが2クラスの場合に使用します。</p>
<p>2クラスというのはデータの種類が2つであることを意味します。</p>
<p>バイナリ交差エントロピー損失は一種の距離を表すような指標で、ニューラルネットワークの出力と正解との間にどの程度の差があるのかを示す尺度です。</p>
<p>n個のデータがあったとしてバイナリ交差エントロピー損失L(y,t)はデータiに対するクラス1の予測確率yiと正解jクラスtiを表します。</p>
<p>クラス1の予測値yiはニューラルネットワークの出力層から出力された値をシグモイド関数で変換した確率値を表しています。</p>
<p>出力層からの出力値をロジットといいます。</p>
<p>ロジットとはあるあるクラスの確率pとそうでない確率1-pnの比に対数をとった値です。</p>
<p>先に出てきたシグモイド関数はロジット関数の逆関数です。</p>
<p>そのためシグモイド関数にロジットを入力することでクラスの確率pを求めることができます。</p>
<p>要は出力値を0から1の範囲に抑えつつ扱いやすい確率の形に変換できる公式といった感じです。</p>
<p>バイナリ交差エントロピー損失の関数はnn.BCELoss()です。</p>
<p>シグモイド関数はnn.Sigmoid()です。</p>
<p>なお、nn.BCELossはtorch.float32型をデータ型として使用しなければなりません。</p>
<p>そのため正解クラスのデータ型は本来intですがfloatに変換する必要があります。</p>
<p>import torch<br />
from torch import nn<br />
m = nn.Sigmoid()<br />
y = torch.rand(3)<br />
t = torch.empty(3, dtype=torch.float32).random_(2)<br />
criterion = nn.BCELoss()<br />
loss = criterion(m(y), t)</p>
<p>print("y: {}".format(y))<br />
print("m(y): {}".format(m(y)))<br />
print("t: {}".format(t))<br />
print("loss: {:.4f}".format(loss))</p>
<h2 id="_6">実行結果<a class="headerlink" href="#_6" title="Permanent link">¶</a></h2>
<blockquote>
<blockquote>
<blockquote>
<p>y: tensor([0.2744, 0.9147, 0.3309])<br />
m(y): tensor([0.5682, 0.7140, 0.5820])<br />
t: tensor([0., 1., 0.])<br />
loss: 0.6830</p>
</blockquote>
</blockquote>
</blockquote>
<p>lossがバイナリ交差エントロピー損失です。</p>
<p>_03.　ロジット付きバイナリ交差エントロピー損失</p>
<p>ロジット付きバイナリ交差エントロピー損失はバイナリ交差エントロピー損失に最初からシグモイド関数が加えられたものです。</p>
<p>すなわち出力値をそのまま与えればバイナリ交差エントロピー損失が得られます。</p>
<p>n個のデータがあったとして、ロジット付きバイナリ交差エントロピー損失はデータiに対するロジットyiと正解のクラスtiをL(y, t)として表すことができます。</p>
<p>ロジット付きバイナリ交差エントロピー損失の関数はnn.BCEWithLogitsLoss()です。</p>
<p>長いですね。</p>
<p>import torch<br />
from torch import nn<br />
y = torch.rand(3)<br />
t = torch.empty(3, dype=torch.float32).random_(2)<br />
criterion = nn.BCEWithLogitsLoss()<br />
loss = criterion(y, t)</p>
<p>print("y: {}".format(y))<br />
print("t: {}".format(t))<br />
print("loss: {:.4f}".format(loss))</p>
<h2 id="_7">実行結果<a class="headerlink" href="#_7" title="Permanent link">¶</a></h2>
<p>y: tensor([0.9709, 0.8976, 0.3228])<br />
t: tensor([0., 1., 0.])<br />
loss: 0.8338</p>
<p>lossがロジット付きバイナリ交差エントロピー損失です。</p>
<p>.format()では指定した変数を{}の中に代入してそれを出力しています。</p>
<p>_04.　ソフトマックス交差エントロピー損失</p>
<p>ソフトマックス交差エントロピー損失もバイナリ交差エントロピー損失と同じように、ニューラルネットワークの出力と正解クラスがどのくらい離れているかを評価する尺度です。</p>
<p>特に2クラス以上の多クラスに分類されている場合に用いられます。</p>
<p>2クラスの分類ではシグモイド関数を使用しましたが、2クラス以上ではソフトマックス交差エントロピー損失を使用します。</p>
<p>ソフトマックスエントロピー損失はn個のデータがあったとしてデータiに対するクラスkのロジットyiと正解クラスtiのデータを使用してL(y, t)で表すことが可能です。</p>
<p>ソフトマックス交差エントロピー損失はnn.CrossEntropyLossです。</p>
<p>import torch<br />
from torch import nn<br />
y = torch.rand(3, 5)<br />
t = torch.empty(3, dtype=torch.int64).random_(5)<br />
criterion = nn.CrossEntropyLoss()<br />
loss = criterion(y, t)</p>
<p>print("y:{}".format(y))<br />
print("t:{}".format(t))<br />
print("loss: {:4f}".format(loss))</p>
<h2 id="_8">実行結果<a class="headerlink" href="#_8" title="Permanent link">¶</a></h2>
<p>y: tensor([[0.7775, 0.7587, 0.9474, 0.5149, 0.7741],<br />
        [0.5059, 0.4802, 0.9846, 0.6292, 0.0167],<br />
        [0.4339, 0.6873, 0.4253, 0.7067, 0.5678]])<br />
t: tensor([1, 4, 1])<br />
loss: 1.757074</p>
<p>データ数は3つで各クラスに出力します。</p>
<p>クラス数は5つです。</p>
<p>lossがソフトマックス交差エントロピー損失を表しています。</p>
<p>torch.cuda.amp.autocast():の正しいindent</p>
<p>to('cpu').numpy()<br />
cpu().detach().numpy()<br />
違い</p>
<h3 id="ver1">ver1<a class="headerlink" href="#ver1" title="Permanent link">¶</a></h3>
<p>def set_seed(seed = 0):<br />
    np.random.seed(seed)<br />
    random_state = np.random.RandomState(seed)<br />
    random.seed(seed)<br />
    torch.manual_seed(seed)<br />
    torch.cuda.manual_seed(seed)<br />
    torch.backends.cudnn.deterministic = True<br />
    torch.backends.cudnn.benchmark = False<br />
    os.environ['PYTHONHASHSEED'] = str(seed)<br />
    return random_state</p>
<p>class CFG:<br />
    project_name = 'sample2'<br />
    model_name = 'resnet18'<br />
    note = '2nd'<br />
    batch_size= 4<br />
    n_fold= 4<br />
    num_workers =4<br />
    image_size =224<br />
    epochs = 25<br />
    seed = 42<br />
    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']<br />
    T_max = 6 # CosineAnnealingLR<br />
    #T_0=6 # CosineAnnealingWarmRestarts<br />
    lr=1e-4<br />
    min_lr=1e-6<br />
    exp_name = f'{model_name}<em>{note}</em>{batch_size}Batch'<br />
print(CFG.model_name)</p>
<p>from tqdm import tqdm<br />
class BasicNN(nn.Module):<br />
    def <strong>init</strong>(self, cfg):<br />
        super().<strong>init</strong>()<br />
        self.cfg = cfg<br />
        self.model = models.resnet18(pretrained=True)<br />
        self.model.fc = nn.Linear(self.model.fc.in_features, 2)</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>    self.model = self.model.to(&#39;cuda&#39;)

    self.current_epoch = 0
    self.fp16 = True
    self.train_loader = None
    self.valid_loader = None
    self.scaler = True
    self.criterion = None
    self.optimizer = None
    self.scheduler = None
    self.metrics = None
    self.num_workers = 1

def _init_model(
    self,
    train_dataset,
    valid_dataset,
    train_batchsize,
    valid_batchsize,
    fp16,
):
    self.num_workers = min(4, psutil.cpu_count())
    if self.train_loader is None:
        self.train_loader = torch.utils.data.DataLoader(
            dataset = train_dataset, 
            batch_size = train_batchsize,
            shuffle=True, 
            num_workers= self.num_workers
        )
    if self.valid_loader is None:
        self.valid_loader = torch.utils.data.DataLoader(
            dataset = valid_dataset, 
            batch_size=valid_batchsize,
            shuffle=False, 
            num_workers = self.num_workers
        )

    self.fp16 = fp16
    if self.fp16: self.scaler = torch.cuda.amp.GradScaler()
    if not self.criterion: self.criterion = self.loss()
    if not self.optimizer: self.optimizer = self.fetch_optimizer()
    if not self.scheduler: self.scheduler = self.fetch_scheduler()

def _init_wandb(self):
    hyperparams = {
        &#39;model_name&#39; : self.cfg.model_name,
        &#39;batch_size&#39; : self.cfg.batch_size,
        &#39;n_fold&#39; : self.cfg.n_fold,
        &#39;num_workers&#39; : self.cfg.num_workers,
        &#39;image_size&#39; : self.cfg.image_size,
        &#39;epochs&#39; : self.cfg.epochs
    }
    wandb.init(
        config = hyperparams,
        project= self.cfg.project_name,
        name=self.cfg.exp_name,
    )
    wandb.watch(self)

def loss(self):
    loss = nn.CrossEntropyLoss()
    return loss

def fetch_optimizer(self):
    #opt = torch.optim.Adam(self.parameters(), lr=5e-4)
    opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)
    return opt

def fetch_scheduler(self):
    # sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    #     self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1
    # )
    sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1)
    #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False,
    #step_size_up=1400,step_size_down=1400, mode=&quot;triangular2&quot;)
    return sch

def monitor_metrics(self, *args, **kwargs):
    self.metrics = None
    return

def forward(self, x):
    return self.model(x)

def model_fn(self, inputs, labels):
    inputs = inputs.to(&#39;cuda&#39;)
    labels = labels.to(&#39;cuda&#39;)
    return self(inputs)

def train_one_batch(self, inputs, labels):
    inputs = inputs.to(&#39;cuda&#39;)
    labels = labels.to(&#39;cuda&#39;)
    self.optimizer.zero_grad()
    with torch.set_grad_enabled(True):
        if self.fp16:
            with torch.cuda.amp.autocast():              
                outputs = self(inputs)
                _, preds = torch.max(outputs, 1)
                loss = self.criterion(outputs, labels)
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            outputs = self(inputs)
            _, preds = torch.max(outputs, 1)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()
    return loss, preds, labels

def train_one_epoch(self):
    self.train()
    running_loss = 0.0
    running_corrects = 0
    for inputs, labels in self.train_loader:
        loss, preds, labels = self.train_one_batch(inputs, labels)
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)
    self.scheduler.step() #スケジューラかepoch単位かbatch単位かに注意
    one_epoch_loss = running_loss / dataset_sizes[&#39;train&#39;]
    one_epoch_acc = running_corrects.double() / dataset_sizes[&#39;train&#39;]
    return one_epoch_loss, one_epoch_acc

def validate_one_batch(self, inputs, labels):
    inputs = inputs.to(&#39;cuda&#39;)
    labels = labels.to(&#39;cuda&#39;)
    with torch.no_grad():
        outputs = self(inputs)
        _, preds = torch.max(outputs, 1)
        loss = self.criterion(outputs, labels)
    return loss, preds, labels

def validate_one_epoch(self):
    self.eval()
    running_loss = 0.0
    running_corrects = 0
    for inputs, labels in self.valid_loader:
        loss, preds, labels = self.validate_one_batch(inputs, labels)
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)
    one_epoch_loss = running_loss / dataset_sizes[&#39;val&#39;]
    one_epoch_acc = running_corrects.double()/ dataset_sizes[&#39;val&#39;]
    return one_epoch_loss, one_epoch_acc

def predict_one_batch(self, inputs, labels):
    inputs = inputs.to(&#39;cuda&#39;)
    labels = labels.to(&#39;cuda&#39;)
    with torch.no_grad():
        outputs = self(inputs)
        _, preds_one_batch = torch.max(outputs, 1)
    return preds_one_batch

def predict(
    self,
    dataset,
    batch_size,
):
    self.eval()
    self.num_workers = min(4, psutil.cpu_count())
    self.test_loader =  torch.utils.data.DataLoader(
        dataset = test_dataset, 
        batch_size = batch_size,
        shuffle=True, 
        num_workers= self.num_workers
    )
    preds_list = []
    for inputs, labels in self.test_loader:
        preds_one_batch = self.predict_one_batch(inputs, labels)
        preds_list.append(preds_one_batch.to(&#39;cpu&#39;).numpy())
    preds_arr = np.concatenate(preds_list)
    return preds_arr


def save(self, model_path):
    model_state_dict = self.state_dict()
    if self.optimizer is not None:
        opt_state_dict = self.optimizer.state_dict()
    else:
        opt_state_dict = None
    if self.scheduler is not None:
        sch_state_dict = self.scheduler.state_dict()
    else:
        sch_state_dict = None
    model_dict = {}
    model_dict[&quot;state_dict&quot;] = model_state_dict
    model_dict[&quot;optimizer&quot;] = opt_state_dict
    model_dict[&quot;scheduler&quot;] = sch_state_dict
    model_dict[&quot;epoch&quot;] = self.current_epoch
    model_dict[&quot;fp16&quot;] = self.fp16
    torch.save(model_dict, model_path)

def load(self, model_path, device=&quot;cuda&quot;):
    self.device = device
    if next(self.parameters()).device != self.device:
        self.to(self.device)
    model_dict = torch.load(model_path, map_location=torch.device(device))
    self.load_state_dict(model_dict[&quot;state_dict&quot;])

def fit(
    self,
    train_dataset,
    valid_dataset= None,
    epochs = 10,
    train_batchsize = 16,
    valid_batchsize = 16,
    fp16 = True
):
    set_seed(CFG.seed)
    self._init_model(
        train_dataset = train_dataset,
        valid_dataset = valid_dataset,
        train_batchsize = train_batchsize,
        valid_batchsize = valid_batchsize,
        fp16 = fp16
    )
    self._init_wandb()

    tk0 = tqdm(range(epochs), position = 0, leave = True)
    for epoch in enumerate(tk0, 1):
        train_loss, train_acc = self.train_one_epoch()
        if valid_dataset: 
            valid_loss, valid_acc = self.validate_one_epoch()
        #writer.add_scalar(&quot;Loss/train&quot;, 1.0, epoch)
        wandb.log({
            &#39;epoch&#39; : epoch,
            &quot;train_acc&quot; : train_acc,
            &quot;valid_acc&quot; : valid_acc,
            &quot;loss&quot;: train_loss, 
            })
        tk0.set_postfix(train_acc = train_acc.item(), valid_acc = valid_acc.item())
    tk0.close()
    wandb.finish()
</code></pre></div>
</td></tr></table>
<h3 id="ver3">ver3<a class="headerlink" href="#ver3" title="Permanent link">¶</a></h3>
<p>class CFG:<br />
    project_name = 'SETI_test2'<br />
    model_name = 'efficientnetv2_rw_s'<br />
    note = '2nd'<br />
    batch_size= 32<br />
    n_fold= 4<br />
    num_workers =4<br />
    image_size =224<br />
    epochs = 5<br />
    seed = 42<br />
    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']<br />
    T_max = 6 # CosineAnnealingLR<br />
    #T_0=6 # CosineAnnealingWarmRestarts<br />
    lr=1e-4<br />
    min_lr=1e-6<br />
    exp_name = f'{model_name}<em>{note}</em>{batch_size}Batch'<br />
print(CFG.model_name)</p>
<p>def set_seed(seed = 0):<br />
    np.random.seed(seed)<br />
    random_state = np.random.RandomState(seed)<br />
    random.seed(seed)<br />
    torch.manual_seed(seed)<br />
    torch.cuda.manual_seed(seed)<br />
    torch.backends.cudnn.deterministic = True<br />
    torch.backends.cudnn.benchmark = False<br />
    os.environ['PYTHONHASHSEED'] = str(seed)<br />
    return random_state</p>
<p>class AverageMeter:<br />
    """<br />
    Computes and stores the average and current value<br />
    """<br />
    def <strong>init</strong>(self):<br />
        self.val = 0<br />
        self.avg = 0<br />
        self.sum = 0<br />
        self.count = 0</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0

def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count
</code></pre></div>
</td></tr></table>
<p>from tqdm import tqdm<br />
class BasicNN(nn.Module):<br />
    def <strong>init</strong>(self, model_name, pretrained_path):<br />
        super().<strong>init</strong>()</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>    self.model = timm.create_model(model_name, pretrained = False, in_chans=3)
    self.model.load_state_dict(torch.load(pretrained_path))
    self.model.classifier = nn.Linear(self.model.classifier.in_features, 1)
    self.conv1 = nn.Conv2d(1, 3, 
                           kernel_size=3, 
                           stride=1, 
                           padding=3, 
                           bias=False)

    self.valid_targets = None
    self.current_epoch = 0
    self.device = None
    self.fp16 = True
    self.train_loader = None
    self.valid_loader = None
    self.scaler = True
    self.criterion = None
    self.optimizer = None
    self.scheduler_after_step = None
    self.scheduler_after_epoch = None
    self.metrics = None
    self.multiple_GPU = False
    self.num_workers = 1

def _init_model(
    self,
    train_dataset,
    valid_dataset,
    train_batchsize,
    valid_batchsize,
    valid_targets,
    fp16,
):

    if self.device is None:
        self.device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    if self.multiple_GPU and torch.cuda.device_count() &gt; 1:
        print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)
        self = nn.DataParallel(self)
    self.to(self.device)

    self.num_workers = min(4, psutil.cpu_count())
    if self.train_loader is None:
        self.train_loader = torch.utils.data.DataLoader(
            dataset = train_dataset, 
            batch_size = train_batchsize,
            shuffle=True, 
            num_workers= self.num_workers
        )
    if self.valid_loader is None:
        self.valid_loader = torch.utils.data.DataLoader(
            dataset = valid_dataset, 
            batch_size=valid_batchsize,
            shuffle=False, 
            num_workers = self.num_workers
        )
    if self.valid_targets is None:
        self.valid_targets = valid_targets

    self.fp16 = fp16
    self.train_metric_val = None
    self.valid_metric_val = None
    if self.fp16: self.scaler = torch.cuda.amp.GradScaler()
    if not self.criterion: self.criterion = self.configure_criterion()
    if not self.optimizer: self.optimizer = self.configure_optimizer()
    if not self.scheduler_after_step: 
        self.scheduler_after_step = self.configure_scheduler_after_step()
    if not self.scheduler_after_epoch: 
        self.scheduler_after_epoch = self.configure_scheduler_after_epoch()

def _init_wandb(self, cfg):
    hyperparams = {
        &#39;model_name&#39; : cfg.model_name,
        &#39;batch_size&#39; : cfg.batch_size,
        &#39;n_fold&#39; : cfg.n_fold,
        &#39;num_workers&#39; : cfg.num_workers,
        &#39;image_size&#39; : cfg.image_size,
        &#39;epochs&#39; : cfg.epochs
    }
    wandb.init(
        config = hyperparams,
        project= cfg.project_name,
        name=cfg.exp_name,
    )
    wandb.watch(self)

def configure_criterion(self):
    criterion =  nn.BCEWithLogitsLoss()
    return criterion

def configure_optimizer(self):
    opt = torch.optim.Adam(self.parameters(), lr=5e-4)
    #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)
    return opt

def configure_scheduler_after_step(self):
    sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1
    )
    #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1)
    #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False,
    #step_size_up=1400,step_size_down=1400, mode=&quot;triangular2&quot;)
    return sch

def configure_scheduler_after_epoch(self):
    return None

def epoch_metrics(self, outputs, targets):
    return metrics.roc_auc_score(targets, outputs)

def forward(self, x, targets = None):
    x = self.conv1(x)
    outputs = self.model(x)

    if targets is not None:
        loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))
        return outputs, loss
    return outputs, None

def train_one_batch(self, inputs, labels):
    inputs = inputs.to(self.device)
    labels = labels.to(self.device)
    self.optimizer.zero_grad()
    with torch.set_grad_enabled(True):
        if self.fp16:
            with torch.cuda.amp.autocast():              
                outputs, loss = self(inputs, labels)
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            outputs, loss = self(inputs)
            loss.backward()
            self.optimizer.step()
        if self.scheduler_after_step:
            self.scheduler_after_step.step()
    return outputs, loss

def train_one_epoch(self, data_loader):
    self.train()
    running_loss = AverageMeter()
    tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True)
    for batch_idx, (inputs, labels) in enumerate(tk0):
        d1 = datetime.datetime.now()
        preds_one_batch, loss = self.train_one_batch(inputs, labels)
        running_loss.update(loss.item(), data_loader.batch_size)

        # wandb.log({
        #     &quot;train_loss&quot;: running_loss.avg, 
        #     })

        d2 = datetime.datetime.now()
        tk0.set_postfix(train_loss=running_loss.avg, stage=&quot;train&quot;, one_step_time = d2-d1)
    if self.scheduler_after_epoch:
        self.scheduler_after_epoch.step()
    tk0.close()
    return running_loss.avg

def validate_one_step(self, inputs, labels):
    inputs = inputs.to(&#39;cuda&#39;)
    labels = labels.to(&#39;cuda&#39;)
    with torch.no_grad():
        outputs, loss = self(inputs, labels)
    return outputs, loss

def validate_one_epoch(self, data_loader):
    self.eval()
    running_loss = AverageMeter()
    preds_list = []
    tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True)
    for batch_idx, (inputs, labels) in enumerate(tk0):
        preds_one_batch, loss = self.validate_one_step(inputs, labels)
        preds_list.append(preds_one_batch.cpu().detach().numpy())
        running_loss.update(loss.item(), data_loader.batch_size)
        tk0.set_postfix(valid_loss = running_loss.avg,  metrics = self.valid_metric_val, stage=&quot;validation&quot;)
        wandb.log({
            &quot;validate_loss&quot;: running_loss.avg, 
            })
    preds_arr = np.concatenate(preds_list)

    self.valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets)
    tk0.close()
    return self.valid_metric_val, running_loss.avg

def predict_one_step(self, inputs, labels):
    inputs = inputs.to(self.device)
    labels = labels.to(self.device)
    with torch.no_grad():
        outputs, _ = self(inputs, labels)
    return outputs

def predict(
    self,
    dataset,
    batch_size,
):
    self.eval()
    self.num_workers = min(4, psutil.cpu_count())
    self.test_loader =  torch.utils.data.DataLoader(
        dataset = test_dataset, 
        batch_size = batch_size,
        shuffle = False, 
        num_workers= self.num_workers
    )
    preds_list = []
    tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True)
    for batch_idx, (inputs, labels) in enumerate(tk0):
        preds_one_batch = self.predict_one_step(inputs, labels)
        preds_list.append(preds_one_batch.cpu().detach().numpy())
        tk0.set_postfix(stage=&quot;inference&quot;)
    tk0.close()
    preds_arr = np.concatenate(preds_list)
    return preds_arr

def save(self, model_path):
    model_state_dict = self.state_dict()
    if self.optimizer is not None:
        opt_state_dict = self.optimizer.state_dict()
    else:
        opt_state_dict = None
    if self.scheduler_after_step is not None:
        sch_state_dict_after_step = self.scheduler_after_step.state_dict()
    else:
        sch_state_dict_after_step = None
    if self.scheduler_after_epoch is not None:
        sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict()
    else:
        sch_state_dict_after_epoch = None
    model_dict = {}
    model_dict[&quot;state_dict&quot;] = model_state_dict
    model_dict[&quot;optimizer&quot;] = opt_state_dict
    model_dict[&quot;scheduler_after_step&quot;] = sch_state_dict_after_step
    model_dict[&quot;scheduler_after_epoch&quot;] = sch_state_dict_after_epoch
    model_dict[&quot;epoch&quot;] = self.current_epoch
    model_dict[&quot;fp16&quot;] = self.fp16
    model_dict[&quot;multiple_GPU&quot;] = self.multiple_GPU
    torch.save(model_dict, model_path)

def load(self, model_path):
    self.device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    if next(self.parameters()).device != self.device:
        self.to(self.device)
    model_dict = torch.load(model_path, map_location=torch.device(device))
    self.load_state_dict(model_dict[&quot;state_dict&quot;])

def fit(
    self,
    cfg,
    train_dataset,
    valid_dataset= None,
    valid_targets = None,
    epochs = 10,
    train_batchsize = 16,
    valid_batchsize = 16,
    fp16 = True,
    checkpoint_save_path = &#39;&#39;,
    mode = &#39;max&#39;,
    patience = 5,
    delta = 0.001
):
    set_seed(CFG.seed)
    self._init_model(
        train_dataset = train_dataset,
        valid_dataset = valid_dataset,
        train_batchsize = train_batchsize,
        valid_batchsize = valid_batchsize,
        valid_targets = valid_targets,
        fp16 = fp16
    )
    # self._init_wandb(cfg)

    if mode == &#39;max&#39;:
        current_best_valid_score = -float(&#39;inf&#39;)
    else:
        current_best_valid_score = float(&#39;inf&#39;)
    early_stopping_counter = 0

    for epoch in range(epochs):
        train_loss = self.train_one_epoch(self.train_loader)
        if valid_dataset:
            valid_score, valid_loss = self.validate_one_epoch(self.valid_loader)
            # Early Stopping.
            if mode == &#39;max&#39;:
                if valid_score &lt; current_best_valid_score + delta:
                    early_stopping_counter += 1
                    print(f&#39;EarlyStopping counter: {early_stopping_counter} out of {patience}&#39;)
                    if early_stopping_counter &gt;= patience: break
                else:
                    print(f&quot;Validation score improved ({current_best_valid_score} --&gt; {valid_score}). Saving the check point!&quot;)
                    current_best_valid_score = valid_score
                    self.save(checkpoint_save_path + f&quot;{cfg.model_name}_epoch{epoch}.pth&quot; )
            else:
                if valid_score &gt; current_best_valid_score - delta:
                    early_stopping_counter += 1
                    print(f&#39;EarlyStopping counter: {early_stopping_counter} out of {patience}&#39;)
                    if early_stopping_counter &gt;= patience: break
                else:
                    print(f&quot;Validation score improved ({current_best_valid_score} --&gt; {valid_score}). Saving the check point!&quot;)
                    current_best_valid_score = valid_score
                    self.save(checkpoint_save_path + f&quot;{cfg.model_name}_epoch{epoch}.pth&quot; )


        #writer.add_scalar(&quot;Loss/train&quot;, 1.0, epoch)
        # wandb.log({
        #     &quot;epoch&quot; : epoch,
        #     &quot;epch_train_loss&quot; : train_loss,
        #     &quot;epoch_valid_loss&quot; : valid_loss,
        #     &quot;epoch_valid_score&quot; : valid_score,
        #     })
    wandb.finish()
</code></pre></div>
</td></tr></table>
<h3 id="ver4">ver4<a class="headerlink" href="#ver4" title="Permanent link">¶</a></h3>
<p>!pip install wandb<br />
import os<br />
import sys<br />
import random<br />
from tqdm import tqdm<br />
import datetime<br />
import psutil</p>
<p>import pandas as pd<br />
import numpy as np<br />
from sklearn import metrics<br />
from sklearn.model_selection import StratifiedKFold<br />
import torch<br />
import torch.nn as nn<br />
import torchvision</p>
<p>import cv2<br />
from PIL import Image<br />
import albumentations as A</p>
<p>import wandb<br />
import warnings</p>
<p>warnings.filterwarnings("ignore")</p>
<p>class ClassificationDataset():<br />
    def <strong>init</strong>(self, image_paths, targets, transform = None): <br />
        self.image_paths = image_paths<br />
        self.targets = targets<br />
        self.transform = None</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def __len__(self):
    return len(self.image_paths)

def __getitem__(self, item): 
    targets = self.targets[item]
    #image1 = np.load(self.image_paths[item]).astype(float)
    image1 = np.load(self.image_paths[item])[::2].astype(np.float32)
    image = np.vstack(image1).transpose((1, 0))

    image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True))
    image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True))

    image = image.astype(np.float32)[np.newaxis, ]

    # image = np.load(self.image_paths[item]).astype(np.float32)
    # image = np.vstack(image).transpose((1, 0))
    # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC)
    # image = image[np.newaxis, :, :]

    if self.transform:
        image = self.transform(image=image)[&quot;image&quot;]

    return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float)
</code></pre></div>
</td></tr></table>
<p>def set_seed(seed = 0):<br />
    np.random.seed(seed)<br />
    random_state = np.random.RandomState(seed)<br />
    random.seed(seed)<br />
    torch.manual_seed(seed)<br />
    torch.cuda.manual_seed(seed)<br />
    torch.backends.cudnn.deterministic = True<br />
    torch.backends.cudnn.benchmark = False<br />
    os.environ['PYTHONHASHSEED'] = str(seed)<br />
    return random_state</p>
<p>class AverageMeter:<br />
    """<br />
    Computes and stores the average and current value<br />
    """<br />
    def <strong>init</strong>(self):<br />
        self.val = 0<br />
        self.avg = 0<br />
        self.sum = 0<br />
        self.count = 0</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0

def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count
</code></pre></div>
</td></tr></table>
<p>class CFG:<br />
    project_name = 'SETI_test2'<br />
    pretrained_model_name = 'efficientnet_b0'<br />
    pretrained = True<br />
    prettained_path = '../input/timm_weight/efficientnet_b0_ra-3dd342df.pth'<br />
    input_channels = 3<br />
    out_dim = 1<br />
    wandb_note = ''<br />
    colab_or_kaggle = 'colab'<br />
    wandb_exp_name = f'{pretrained_model_name}<em>{colab_or_kaggle}</em>{wandb_note}'<br />
    batch_size= 32<br />
    epochs = 5<br />
    num_of_fold = 5<br />
    seed = 42<br />
    patience = 3<br />
    delta = 0.002<br />
    num_workers = 8<br />
    fp16 = True<br />
    checkpoint_path = ''<br />
    patience_mode = 'max'<br />
    patience = 3<br />
    delta = 0.002<br />
    mixup_alpha = 1.0</p>
<p>train_aug = A.Compose(<br />
    [<br />
        A.Resize(p = 1, height = 512, width = 512),<br />
        #A.Transpose(p=0.5),<br />
        A.HorizontalFlip(p=0.5),<br />
        A.VerticalFlip(p=0.5),<br />
        A.ShiftScaleRotate(p=0.5, <br />
                           scale_limit=0.02,<br />
                           rotate_limit=10, <br />
                           border_mode = cv2.BORDER_REPLICATE),<br />
        A.MotionBlur(p=0.5),<br />
        # Horizontal, Verical, shiftscale rotate, one of (very small Blur, gaussian blur, median blur, motionblur), (別枠gassian noise）, contrast, <br />
    ]<br />
)<br />
df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')<br />
df['img_path'] = df['id'].apply(<br />
    lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy'<br />
)<br />
X = df.img_path.values<br />
Y = df.target.values<br />
skf = StratifiedKFold(n_splits = CFG.num_of_fold)</p>
<p>class BasicNN(nn.Module):<br />
    def <strong>init</strong>(self):<br />
        super().<strong>init</strong>()</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>    self.model = timm.create_model(CFG.pretrained_model_name, 
                                   pretrained = CFG.pretrained, 
                                   in_chans = CFG.input_channels)
    if not CFG.pretrained: self.model.load_state_dict(torch.load(CFG.pretrained_path))
    self.model.classifier = nn.Linear(self.model.classifier.in_features, CFG.out_dim)
    #self.fc = ppe.nn.LazyLinear(None, CFG.out_dim)
    self.conv1 = nn.Conv2d(1, 3, 
                           kernel_size=3, 
                           stride=1, 
                           padding=3, 
                           bias=False)

    self.valid_targets = None
    self.current_epoch = 0
    self.device = None
    self.fp16 = True
    self.train_loader = None
    self.valid_loader = None
    self.scaler = True
    self.criterion = None
    self.optimizer = None
    self.scheduler_after_step = None
    self.scheduler_after_epoch = None
    self.metrics = None
    self.multiple_GPU = False

def _init_model(
    self,
    train_dataset,
    valid_dataset,
    train_batchsize,
    valid_batchsize,
    valid_targets,
    num_workers,
    fp16,
    multiple_GPU,
):

    if self.device is None:
        self.device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    if num_workers == -1:
        num_workers = psutil.cpu_count()
    self.multiple_GPU = multiple_GPU
    if multiple_GPU and torch.cuda.device_count() &gt; 1:
        print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)
        self = nn.DataParallel(self)
    self.to(self.device)

    if self.train_loader is None:
        self.train_loader = torch.utils.data.DataLoader(
            dataset = train_dataset, 
            batch_size = train_batchsize,
            shuffle=True, 
            num_workers= num_workers,
            drop_last = True,
            pin_memory = True
        )
    if self.valid_loader is None:
        self.valid_loader = torch.utils.data.DataLoader(
            dataset = valid_dataset, 
            batch_size=valid_batchsize,
            shuffle=False, 
            num_workers = num_workers,
            drop_last = False,
            pin_memory = True
        )
    if self.valid_targets is None:
        self.valid_targets = valid_targets

    self.fp16 = fp16
    if self.fp16: self.scaler = torch.cuda.amp.GradScaler()
    if not self.criterion: self.criterion = self.configure_criterion()
    if not self.optimizer: self.optimizer = self.configure_optimizer()
    if not self.scheduler_after_step: 
        self.scheduler_after_step = self.configure_scheduler_after_step()
    if not self.scheduler_after_epoch: 
        self.scheduler_after_epoch = self.configure_scheduler_after_epoch()

def _init_wandb(self, cfg):
    hyperparams = {
        &#39;batch_size&#39; : cfg.batch_size,
        &#39;epochs&#39; : cfg.epochs
    }
    wandb.init(
        config = hyperparams,
        project= cfg.project_name,
        name=cfg.wandb_exp_name,
    )
    wandb.watch(self)

def configure_criterion(self):
    criterion =  nn.BCEWithLogitsLoss()
    return criterion

def mixup_data(self, inputs, targets, alpha=1.0):
    if alpha &gt; 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = inputs.size()[0]
    index = torch.randperm(batch_size)
    mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :]
    targets_a, targets_b = targets, targets[index]

    return mixed_inputs, targets_a, targets_b, lam

def mixup_criterion(self, criterion, outputs, targets_a, targets_b, lam):
    return lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)

def configure_optimizer(self):
    opt = torch.optim.Adam(self.parameters(), lr=5e-4)
    #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)
    return opt

def configure_scheduler_after_step(self):
    sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1
    )
    #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1)
    #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False,
    #step_size_up=1400,step_size_down=1400, mode=&quot;triangular2&quot;)
    return sch

def configure_scheduler_after_epoch(self):
    return None

def epoch_metrics(self, outputs, targets):
    return metrics.roc_auc_score(targets, outputs)

def forward(self, image, targets):
    image, targets_a, targets_b, lam = self.mixup_data(image, 
                                                  targets,
                                                  alpha= CFG.mixup_alpha)
    image = self.conv1(image)
    outputs = self.model(image)

    if targets is not None:
        #loss = self.criterion(outputs, targets.view(-1, 1))
        loss = self.mixup_criterion(self.criterion, 
                                outputs, targets_a.view(-1, 1), 
                                targets_b.view(-1, 1), 
                                lam)
        return outputs, loss
    return outputs, None

def train_one_step(self, inputs, targets):
    inputs = inputs.to(self.device, non_blocking=True)
    targets = targets.to(self.device, non_blocking=True)
    self.optimizer.zero_grad()
    with torch.set_grad_enabled(True):
        if self.fp16:
            with torch.cuda.amp.autocast():    
                outputs, loss = self(inputs, targets)
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            outputs, loss = self(inputs, targets)
            loss.backward()
            self.optimizer.step()
        if self.scheduler_after_step:
            self.scheduler_after_step.step()
    return outputs, loss

def validate_one_step(self, inputs, targets):
    inputs = inputs.to(self.device, non_blocking=True)
    targets = targets.to(self.device, non_blocking=True)
    with torch.no_grad():
        outputs, loss = self(inputs, targets)
    return outputs, loss

def predict_one_step(self, inputs, targets):
    outputs, _ = validate_one_step(inputs, targets)
    return outputs

def train_one_epoch(self, data_loader):
    self.train()
    running_loss = AverageMeter()
    tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True)
    for batch_idx, (inputs, targets) in enumerate(tk0):
        preds_one_batch, loss = self.train_one_step(inputs, targets)
        running_loss.update(loss.item(), data_loader.batch_size)
        current_lr = self.optimizer.param_groups[0][&#39;lr&#39;] 
        wandb.log({
            &quot;train_step&quot; : batch_idx,
            &quot;train_loss&quot;: running_loss.avg,
            &quot;lr&quot;: current_lr 
            })
        tk0.set_postfix(train_loss=running_loss.avg, stage=&quot;train&quot;, lr = current_lr)
    if self.scheduler_after_epoch:
        self.scheduler_after_epoch.step()
    tk0.close()
    return running_loss.avg

def validate_one_epoch(self, data_loader):
    self.eval()
    running_loss = AverageMeter()
    preds_list = []
    tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True)
    for batch_idx, (inputs, targets) in enumerate(tk0):
        preds_one_batch, loss = self.validate_one_step(inputs, targets)
        preds_list.append(preds_one_batch.cpu().detach().numpy())
        running_loss.update(loss.item(), data_loader.batch_size)
        tk0.set_postfix(valid_loss = running_loss.avg,  stage=&quot;validation&quot;)
        wandb.log({
            &quot;validate_step&quot; : batch_idx,
            &quot;validate_loss&quot;: running_loss.avg, 
            })
    preds_arr = np.concatenate(preds_list) 
    valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets)
    tk0.close()
    return valid_metric_val, running_loss.avg

def predict(
    self,
    dataset,
    batch_size = 16,
    num_workers = 8,
):
    self.eval()
    self.test_loader =  torch.utils.data.DataLoader(
        dataset = test_dataset, 
        batch_size = batch_size,
        shuffle = False, 
        num_workers= num_workers,
        drop_last = False,
        pin_memory = True
    )
    preds_list = []
    tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True)
    for batch_idx, (inputs, targets) in enumerate(tk0):
        preds_one_batch = self.predict_one_step(inputs, targets)
        preds_list.append(preds_one_batch.cpu().detach().numpy())
        tk0.set_postfix(stage=&quot;inference&quot;)
    tk0.close()
    preds_arr = np.concatenate(preds_list)
    return preds_arr

def save(self, model_path):
    model_state_dict = self.state_dict()
    if self.optimizer is not None:
        opt_state_dict = self.optimizer.state_dict()
    else:
        opt_state_dict = None
    if self.scheduler_after_step is not None:
        sch_state_dict_after_step = self.scheduler_after_step.state_dict()
    else:
        sch_state_dict_after_step = None
    if self.scheduler_after_epoch is not None:
        sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict()
    else:
        sch_state_dict_after_epoch = None
    model_dict = {}
    model_dict[&quot;state_dict&quot;] = model_state_dict
    model_dict[&quot;optimizer&quot;] = opt_state_dict
    model_dict[&quot;scheduler_after_step&quot;] = sch_state_dict_after_step
    model_dict[&quot;scheduler_after_epoch&quot;] = sch_state_dict_after_epoch
    model_dict[&quot;epoch&quot;] = self.current_epoch
    model_dict[&quot;fp16&quot;] = self.fp16
    model_dict[&quot;multiple_GPU&quot;] = self.multiple_GPU
    torch.save(model_dict, model_path)

def load(self, model_path):
    self.device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    if next(self.parameters()).device != self.device:
        self.to(self.device)
    model_dict = torch.load(model_path, map_location=torch.device(device))
    self.load_state_dict(model_dict[&quot;state_dict&quot;])

def fit(
    self,
    cfg,
    train_dataset,
    valid_dataset= None,
    valid_targets = None,
    epochs = 10,
    train_batchsize = 16,
    valid_batchsize = 16,
    num_workers = 8,
    fp16 = True,
    multiple_GPU = False,
    checkpoint_save_path = &#39;&#39;,
    mode = &#39;max&#39;,
    patience = 5,
    delta = 0.001,
):
    set_seed(CFG.seed)
    self._init_model(
        train_dataset = train_dataset,
        valid_dataset = valid_dataset,
        train_batchsize = train_batchsize,
        valid_batchsize = valid_batchsize,
        valid_targets = valid_targets,
        num_workers = num_workers,
        fp16 = fp16,
        multiple_GPU = multiple_GPU
    )
    self._init_wandb(cfg)

    torch.backends.cudnn.benchmark = True

    if mode == &#39;max&#39;:
        current_best_valid_score = -float(&#39;inf&#39;)
    else:
        current_best_valid_score = float(&#39;inf&#39;)
    early_stopping_counter = 0

    for epoch in range(epochs):
        train_loss = self.train_one_epoch(self.train_loader)
        if valid_dataset:
            valid_score, valid_loss = self.validate_one_epoch(self.valid_loader)
            # Early Stopping and save at the check points.
            if mode == &#39;max&#39;:
                if valid_score &lt; current_best_valid_score + delta:
                    early_stopping_counter += 1
                    print(f&#39;EarlyStopping counter: {early_stopping_counter} out of {patience}&#39;)
                    if early_stopping_counter &gt;= patience: break
                else:
                    print(f&quot;Validation score improved ({current_best_valid_score} --&gt; {valid_score}). Saving the check point!&quot;)
                    current_best_valid_score = valid_score
                    self.save(CFG.checkpoint_save_path + f&quot;{cfg.pretrained_model_name}_epoch{epoch}.cpt&quot; )
            else:
                if valid_score &gt; current_best_valid_score - delta:
                    early_stopping_counter += 1
                    print(f&#39;EarlyStopping counter: {early_stopping_counter} out of {patience}&#39;)
                    if early_stopping_counter &gt;= patience: break
                else:
                    print(f&quot;Validation score improved ({current_best_valid_score} --&gt; {valid_score}). Saving the check point!&quot;)
                    current_best_valid_score = valid_score
                    self.save(checkpoint_save_path + f&quot;{cfg.pretrained_model_name}_epoch{epoch}.cpt&quot; )

        #writer.add_scalar(&quot;Loss/train&quot;, 1.0, epoch)
        print(f&#39;epoch: {epoch}, epoch_valid_score : {valid_score}&#39;)
        wandb.log({
            &quot;epoch&quot; : epoch,
            &quot;epch_train_loss&quot; : train_loss,
            &quot;epoch_valid_loss&quot; : valid_loss,
            &quot;epoch_valid_score&quot; : valid_score,
            })
    wandb.finish()
</code></pre></div>
</td></tr></table>
<p>for fold_cnt, (train_index, test_index) in enumerate(skf.split(X, Y), 1):<br />
    train_images, valid_images = X[train_index], X[test_index]<br />
    train_targets, valid_targets = Y[train_index], Y[test_index]</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>train_dataset = ClassificationDataset(
    image_paths=train_images, 
    targets=train_targets, 
    transform = None
)
valid_dataset = ClassificationDataset(
    image_paths=valid_images, 
    targets=valid_targets, 
    transform = None
)
model = BasicNN()

model.fit(
    cfg = CFG,
    train_dataset = train_dataset,
    valid_dataset = valid_dataset,
    valid_targets = valid_targets,
    epochs = CFG.epochs,
    train_batchsize = CFG.batch_size,
    valid_batchsize = CFG.batch_size,
    num_workers = CFG.num_workers,
    fp16 = CFG.fp16,
    checkpoint_save_path = CFG.checkpoint_path,
    mode = CFG.patience_mode,
    patience = CFG.patience,
    delta = CFG.delta
)
</code></pre></div>
</td></tr></table>
                
              
    
    
        
    

              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../python_basis/" class="md-footer__link md-footer__link--prev" aria-label="Previous: None" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              None
            </div>
          </div>
        </a>
      
      
        
        <a href="../templates/" class="md-footer__link md-footer__link--next" aria-label="Next: None" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              None
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/yseeker" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.expand"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.b0710199.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.76f349be.min.js"></script>
      
        <script src="../javascripts/extra.js"></script>
      
        <script src="https://unpkg.com/mermaid@8.0.0/dist/mermaid.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>